{
  "version": 1,
  "status": "error",
  "eval": {
    "task": "task",
    "task_version": 0,
    "task_id": "oUwzDJfRSmEDYg6eyY9Fxn",
    "run_id": "QdD8mxcoUfamRtgo2ZggM6",
    "created": "2024-05-22T15:02:22",
    "dataset": {},
    "model": "openai/gpt-4",
    "task_attribs": {},
    "task_args": {},
    "model_args": {},
    "config": {},
    "revision": {
      "type": "git",
      "origin": "https://github.com/abigailhaddad/LMGradingRubric.git",
      "commit": "f5b4a66"
    },
    "packages": {
      "inspect_ai": "0.3.4.dev1+g6cfb4fe"
    }
  },
  "plan": {
    "name": "plan",
    "steps": [
      {
        "solver": "system_message",
        "params": {
          "message": "You are an AI assistant helping with fact comparison."
        }
      }
    ],
    "config": {}
  },
  "stats": {
    "started_at": "2024-05-22T15:02:22",
    "completed_at": "2024-05-22T15:02:27",
    "model_usage": {
      "openai/gpt-4": {
        "input_tokens": 1228,
        "output_tokens": 144,
        "total_tokens": 1372
      }
    }
  },
  "error": {
    "message": "1 validation error for EvalSample\nscore\n  Input should be a valid dictionary or instance of Score [type=model_type, input_value=Score(value={'groundednes...is blue\", metadata=None), input_type=Score]\n    For further information visit https://errors.pydantic.dev/2.6/v/model_type",
    "traceback": "Traceback (most recent call last):\n\n  File \"C:\\Users\\abiga\\OneDrive\\Documents\\PythonScripts\\LLMGradingRubric\\env\\Lib\\site-packages\\inspect_ai\\_eval\\task.py\", line 306, in run\n    self._log_output(\n\n  File \"C:\\Users\\abiga\\OneDrive\\Documents\\PythonScripts\\LLMGradingRubric\\env\\Lib\\site-packages\\inspect_ai\\_eval\\task.py\", line 509, in _log_output\n    logger.log_sample(epoch, samples[i], states[i], scores[i])\n\n  File \"C:\\Users\\abiga\\OneDrive\\Documents\\PythonScripts\\LLMGradingRubric\\env\\Lib\\site-packages\\inspect_ai\\_eval\\log.py\", line 102, in log_sample\n    EvalSample(\n\n  File \"C:\\Users\\abiga\\OneDrive\\Documents\\PythonScripts\\LLMGradingRubric\\env\\Lib\\site-packages\\pydantic\\main.py\", line 171, in __init__\n    self.__pydantic_validator__.validate_python(data, self_instance=self)\n\npydantic_core._pydantic_core.ValidationError: 1 validation error for EvalSample\nscore\n  Input should be a valid dictionary or instance of Score [type=model_type, input_value=Score(value={'groundednes...is blue\", metadata=None), input_type=Score]\n    For further information visit https://errors.pydantic.dev/2.6/v/model_type\n",
    "traceback_ansi": "\u001b[31m+-\u001b[0m\u001b[31m------------------------------\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m-------------------------------\u001b[0m\u001b[31m-+\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33mC:\\Users\\abiga\\OneDrive\\Documents\\PythonScripts\\LLMGradingRubric\\env\\Lib\\site-packages\\inspect_a\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33mi\\_eval\\task.py\u001b[0m:\u001b[94m306\u001b[0m in \u001b[92mrun\u001b[0m                                                                       \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m                                                                                                  \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33mC:\\Users\\abiga\\OneDrive\\Documents\\PythonScripts\\LLMGradingRubric\\env\\Lib\\site-packages\\inspect_a\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33mi\\_eval\\task.py\u001b[0m:\u001b[94m509\u001b[0m in \u001b[92m_log_output\u001b[0m                                                               \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m                                                                                                  \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33mC:\\Users\\abiga\\OneDrive\\Documents\\PythonScripts\\LLMGradingRubric\\env\\Lib\\site-packages\\inspect_a\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33mi\\_eval\\log.py\u001b[0m:\u001b[94m102\u001b[0m in \u001b[92mlog_sample\u001b[0m                                                                 \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m                                                                                                  \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33mC:\\Users\\abiga\\OneDrive\\Documents\\PythonScripts\\LLMGradingRubric\\env\\Lib\\site-packages\\pydantic\\\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33mmain.py\u001b[0m:\u001b[94m171\u001b[0m in \u001b[92m__init__\u001b[0m                                                                          \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m                                                                                                  \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m   \u001b[2m 168 \u001b[0m\u001b[33m        \"\"\"\u001b[0m                                                                               \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m   \u001b[2m 169 \u001b[0m        \u001b[2m# `__tracebackhide__` tells pytest and some other tools to omit this function fr\u001b[0m  \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m   \u001b[2m 170 \u001b[0m        __tracebackhide__ = \u001b[94mTrue\u001b[0m                                                          \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[31m‚ù± \u001b[0m 171         \u001b[96mself\u001b[0m.__pydantic_validator__.validate_python(data, self_instance=\u001b[96mself\u001b[0m)             \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m   \u001b[2m 172 \u001b[0m                                                                                          \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m   \u001b[2m 173 \u001b[0m    \u001b[2m# The following line sets a flag that we use to determine when `__init__` gets overr\u001b[0m  \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m   \u001b[2m 174 \u001b[0m    \u001b[92m__init__\u001b[0m.__pydantic_base_init__ = \u001b[94mTrue\u001b[0m  \u001b[2m# pyright: ignore[reportFunctionMemberAccess\u001b[0m  \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m                                                                                                  \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m+-\u001b[0m\u001b[33m------------------------------------------\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m------------------------------------------\u001b[0m\u001b[33m-+\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m data = \u001b[1m{\u001b[0m                                                                                     \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m            \u001b[33m'id'\u001b[0m: \u001b[33m'sample_1'\u001b[0m,                                                                 \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m            \u001b[33m'epoch'\u001b[0m: \u001b[94m1\u001b[0m,                                                                       \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m            \u001b[33m'input'\u001b[0m: \u001b[33m'The fox is a dog.'\u001b[0m,                                                     \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m            \u001b[33m'choices'\u001b[0m: \u001b[94mNone\u001b[0m,                                                                  \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m            \u001b[33m'target'\u001b[0m: \u001b[33m'The fox is brown.'\u001b[0m,                                                    \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m            \u001b[33m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,                                                                   \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m            \u001b[33m'messages'\u001b[0m: \u001b[1m[\u001b[0m                                                                     \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m                \u001b[1;35mChatMessageSystem\u001b[0m\u001b[1m(\u001b[0m                                                            \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m                    \u001b[33mcontent\u001b[0m=\u001b[33m'You are an AI assistant helping with fact comparison.'\u001b[0m,          \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m                    \u001b[33msource\u001b[0m=\u001b[94mNone\u001b[0m,                                                              \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m                    \u001b[33mrole\u001b[0m=\u001b[33m'system'\u001b[0m,                                                            \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m                    \u001b[33mtool\u001b[0m=\u001b[94mNone\u001b[0m                                                                 \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m                \u001b[1m)\u001b[0m,                                                                            \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m                \u001b[1;35mChatMessageUser\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[33m'The fox is a dog.'\u001b[0m, \u001b[33msource\u001b[0m=\u001b[33m'input'\u001b[0m, \u001b[33mrole\u001b[0m=\u001b[33m'user'\u001b[0m\u001b[1m)\u001b[0m     \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m            \u001b[1m]\u001b[0m,                                                                                \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m            \u001b[33m'output'\u001b[0m: \u001b[1;35mModelOutput\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmodel\u001b[0m=\u001b[33m'openai/gpt-4'\u001b[0m, \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m, \u001b[33musage\u001b[0m=\u001b[94mNone\u001b[0m, \u001b[33merror\u001b[0m=\u001b[94mNone\u001b[0m\u001b[1m)\u001b[0m,  \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m            \u001b[33m'score'\u001b[0m: \u001b[1;35mScore\u001b[0m\u001b[1m(\u001b[0m                                                                   \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m                \u001b[33mvalue\u001b[0m=\u001b[1m{\u001b[0m\u001b[33m'groundedness'\u001b[0m: \u001b[33m'0.0'\u001b[0m, \u001b[33m'thoroughness'\u001b[0m: \u001b[33m'0'\u001b[0m\u001b[1m}\u001b[0m,                           \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m                \u001b[33manswer\u001b[0m=\u001b[94mNone\u001b[0m,                                                                  \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m                \u001b[33mexplanation\u001b[0m=\u001b[33m\"\u001b[0m\u001b[1;33m{\u001b[0m\u001b[33m'context_list': 'It seems you forgot to include the text you \u001b[0m   \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m        \u001b[33mwant parsed into i\"\u001b[0m+\u001b[94m308\u001b[0m,                                                              \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m                \u001b[33mmetadata\u001b[0m=\u001b[94mNone\u001b[0m                                                                 \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m            \u001b[1m)\u001b[0m                                                                                 \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m        \u001b[1m}\u001b[0m                                                                                     \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m|\u001b[0m self = \u001b[1;35mEvalSample\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                                                                          \u001b[33m|\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m|\u001b[0m \u001b[33m+----------------------------------------------------------------------------------------------+\u001b[0m \u001b[31m|\u001b[0m\n\u001b[31m+--------------------------------------------------------------------------------------------------+\u001b[0m\n\u001b[1;91mValidationError: \u001b[0m\u001b[1;36m1\u001b[0m validation error for EvalSample\nscore\n  Input should be a valid dictionary or instance of Score \u001b[1m[\u001b[0m\u001b[33mtype\u001b[0m=\u001b[35mmodel_type\u001b[0m, \n\u001b[33minput_value\u001b[0m=\u001b[1;35mScore\u001b[0m\u001b[1m(\u001b[0m\u001b[33mvalue\u001b[0m=\u001b[1m{\u001b[0m'groundednes\u001b[33m...\u001b[0mis blue\", \u001b[33mmetadata\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m, \u001b[33minput_type\u001b[0m=\u001b[35mScore\u001b[0m\u001b[1m]\u001b[0m\n    For further information visit \u001b[4;94mhttps://errors.pydantic.dev/2.6/v/model_type\u001b[0m\n"
  },
  "logging": []
}