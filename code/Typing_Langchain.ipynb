{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a851d3d-cb9b-448c-ae3b-21c524601308",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5119e1e4-68fd-4176-9b72-ff30c8f3c38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (0.1.19)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.6.6)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.0.38)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.1.52)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.1.56)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain) (2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (8.1.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipywidgets) (8.21.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipywidgets) (5.14.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: decorator in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (0.1.6)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.46 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-openai) (0.1.52)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.24.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-openai) (1.28.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-openai) (0.5.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (0.1.56)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (2.6.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (8.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (0.26.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain-openai) (3.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.46->langchain-openai) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langchain-openai) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langchain-openai) (2.16.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.24.0->langchain-openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain\n",
    "%pip install ipywidgets\n",
    "%pip install langchain-openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35219496-2769-4767-831c-f85ac271b390",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbb8fd73-0951-4687-9e4f-3c2433666dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Dict, Protocol, cast, runtime_checkable\n",
    "\n",
    "from langchain_core.callbacks import AsyncCallbackManagerForLLMRun, CallbackManagerForLLMRun\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, BaseMessage, FunctionMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.messages import ToolCall as LCToolCall\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult\n",
    "from pydantic.v1 import Field\n",
    "from typing_extensions import override\n",
    "\n",
    "from inspect_ai.model import (\n",
    "    ChatMessage,\n",
    "    ChatMessageAssistant,\n",
    "    ChatMessageSystem,\n",
    "    ChatMessageTool,\n",
    "    ChatMessageUser,\n",
    "    Content,\n",
    "    ContentImage,\n",
    "    ContentText,\n",
    "    GenerateConfig,\n",
    "    ModelName,\n",
    "    ModelOutput,\n",
    "    ToolCall,\n",
    "    ToolChoice,\n",
    "    ToolInfo,\n",
    "    ToolParam,\n",
    "    get_model,\n",
    ")\n",
    "from inspect_ai.solver import Generate, Solver, TaskState\n",
    "\n",
    "@runtime_checkable\n",
    "class LangChainAgent(Protocol):\n",
    "    async def __call__(\n",
    "        self, llm: BaseChatModel, input: dict[str, Any]\n",
    "    ) -> str | list[str | dict[str, Any]]:\n",
    "        ...\n",
    "\n",
    "\n",
    "def langchain_solver(agent: LangChainAgent) -> Solver:\n",
    "    async def solve(state: TaskState, generate: Generate) -> TaskState:\n",
    "        # create the inspect model api bridge\n",
    "        llm = InspectChatModel()\n",
    "\n",
    "        # call the agent\n",
    "        await agent(\n",
    "            llm=llm,\n",
    "            input=dict(\n",
    "                input=state.user_prompt.text,\n",
    "                chat_history=as_langchain_chat_history(state.messages[1:]),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # collect output from llm interface\n",
    "        state.messages = llm.messages\n",
    "        state.output = llm.output\n",
    "\n",
    "        # return state\n",
    "        return state\n",
    "\n",
    "    return solve\n",
    "\n",
    "\n",
    "class InspectChatModel(BaseChatModel):\n",
    "    # track messages and model output so we can update\n",
    "    # the inspect task state when we are complete\n",
    "    messages: list[ChatMessage] = Field(default=[], exclude=True)\n",
    "    output: ModelOutput = Field(default=ModelOutput(), exclude=True)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return f\"Inspect ({ModelName(get_model()).api})\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"model_name\": str(ModelName(get_model()).name),\n",
    "        }\n",
    "\n",
    "    @override\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: list[BaseMessage],\n",
    "        stop: list[str] | None = None,\n",
    "        run_manager: CallbackManagerForLLMRun | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        # inspect uses async exclusively\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @override\n",
    "    async def _agenerate(\n",
    "        self,\n",
    "        messages: list[BaseMessage],\n",
    "        stop: list[str] | None = None,\n",
    "        run_manager: AsyncCallbackManagerForLLMRun | None = None,\n",
    "        **kwargs: dict[str, Any],\n",
    "    ) -> ChatResult:\n",
    "        # extract tools from kwargs\n",
    "        tools: list[ToolInfo] = []\n",
    "        tool_choice: ToolChoice | None = None\n",
    "        lc_tools = cast(list[dict[str, Any]] | None, kwargs.get(\"tools\", None))\n",
    "        if lc_tools:\n",
    "            tools = [\n",
    "                ToolInfo(\n",
    "                    name=tool[\"function\"][\"name\"],\n",
    "                    description=tool[\"function\"][\"description\"],\n",
    "                    params=as_inspect_tool_params(tool[\"function\"][\"parameters\"]),\n",
    "                )\n",
    "                for tool in lc_tools\n",
    "            ]\n",
    "            tool_choice = \"auto\"\n",
    "\n",
    "        # generate\n",
    "        input = [as_inspect_message(message) for message in messages]\n",
    "        result = await get_model().generate(\n",
    "            input=input,\n",
    "            tools=tools,\n",
    "            tool_choice=tool_choice,\n",
    "            config=GenerateConfig(stop_seqs=stop),\n",
    "        )\n",
    "\n",
    "        # track last messages / model output\n",
    "        self.messages = input\n",
    "        self.messages.append(result.choices[0].message)\n",
    "        self.output = result\n",
    "\n",
    "        # extract choices\n",
    "        generations = [\n",
    "            ChatGeneration(message=as_langchain_message(choice.message))\n",
    "            for choice in result.choices\n",
    "        ]\n",
    "\n",
    "        # return\n",
    "        return ChatResult(generations=generations)\n",
    "\n",
    "\n",
    "def as_inspect_message(message: BaseMessage) -> ChatMessage:\n",
    "    if isinstance(message, SystemMessage):\n",
    "        return ChatMessageSystem(content=as_inspect_content(message.content))\n",
    "    elif isinstance(message, HumanMessage):\n",
    "        return ChatMessageUser(content=as_inspect_content(message.content))\n",
    "    elif isinstance(message, AIMessage):\n",
    "        return ChatMessageAssistant(\n",
    "            content=as_inspect_content(message.content),\n",
    "            tool_calls=(\n",
    "                [\n",
    "                    ToolCall(\n",
    "                        type=\"function\",\n",
    "                        function=call[\"name\"],\n",
    "                        id=call[\"id\"] or call[\"name\"],\n",
    "                        arguments=call[\"args\"],\n",
    "                    )\n",
    "                    for call in message.tool_calls\n",
    "                ]\n",
    "                if message.tool_calls and len(message.tool_calls) > 0\n",
    "                else None\n",
    "            ),\n",
    "        )\n",
    "    elif isinstance(message, ToolMessage):\n",
    "        return ChatMessageTool(\n",
    "            content=as_inspect_content(message.content),\n",
    "            tool_call_id=message.tool_call_id,\n",
    "        )\n",
    "    elif isinstance(message, FunctionMessage):\n",
    "        return ChatMessageTool(\n",
    "            content=as_inspect_content(message.content), tool_call_id=message.name\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected message type: {type(message)}\")\n",
    "\n",
    "\n",
    "def as_langchain_message(message: ChatMessage) -> BaseMessage:\n",
    "    if isinstance(message, ChatMessageSystem):\n",
    "        return SystemMessage(content=as_langchain_content(message.content))\n",
    "    elif isinstance(message, ChatMessageUser):\n",
    "        return HumanMessage(content=as_langchain_content(message.content))\n",
    "    elif isinstance(message, ChatMessageAssistant):\n",
    "        additional_kwargs: dict[str, Any] = {}\n",
    "        if message.tool_calls and len(message.tool_calls) > 0:\n",
    "            additional_kwargs[\"tool_calls\"] = [\n",
    "                dict(\n",
    "                    id=call.id, name=call.function, arguments=json.dumps(call.arguments)\n",
    "                )\n",
    "                for call in message.tool_calls\n",
    "            ]\n",
    "\n",
    "        return AIMessage(\n",
    "            content=as_langchain_content(message.content),\n",
    "            tool_calls=(\n",
    "                [\n",
    "                    LCToolCall(id=call.id, name=call.function, args=call.arguments)\n",
    "                    for call in message.tool_calls\n",
    "                ]\n",
    "                if message.tool_calls\n",
    "                else []\n",
    "            ),\n",
    "            additional_kwargs=additional_kwargs,\n",
    "        )\n",
    "    elif isinstance(message, ChatMessageTool):\n",
    "        return ToolMessage(\n",
    "            content=as_langchain_content(message.content),\n",
    "            tool_call_id=message.tool_call_id or \"\",\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected message type: {type(message)}\")\n",
    "\n",
    "\n",
    "def as_langchain_chat_history(messages: list[ChatMessage]) -> list[dict[str, Any]]:\n",
    "    return [dict(role=message.role, content=message.text) for message in messages]\n",
    "\n",
    "\n",
    "def as_inspect_content(\n",
    "    content: str | list[str | dict[str, Any]],\n",
    ") -> str | list[Content]:\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "    else:\n",
    "        return [\n",
    "            (\n",
    "                ContentText(text=c)\n",
    "                if isinstance(c, str)\n",
    "                else (\n",
    "                    ContentText(text=c[\"text\"])\n",
    "                    if c[\"type\"] == \"text\"\n",
    "                    else ContentImage(image=c[\"image\"])\n",
    "                )\n",
    "            )\n",
    "            for c in content\n",
    "        ]\n",
    "\n",
    "\n",
    "def as_inspect_tool_params(parameters: dict[str, Any]) -> list[ToolParam]:\n",
    "    params: list[ToolParam] = []\n",
    "    for key, param in parameters[\"properties\"].items():\n",
    "        params.append(\n",
    "            ToolParam(\n",
    "                name=key,\n",
    "                type=param[\"type\"],\n",
    "                description=param.get(\"description\", param.get(\"title\")),\n",
    "                optional=key not in parameters[\"required\"],\n",
    "            )\n",
    "        )\n",
    "    return params\n",
    "\n",
    "\n",
    "def as_langchain_content(\n",
    "    content: str | list[Content],\n",
    ") -> str | list[str | dict[str, Any]]:\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "    else:\n",
    "        return [c if isinstance(c, str) else c.model_dump() for c in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d108b5b-d0ad-4b5f-8b2d-308364e05dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from inspect_ai import eval, Task, task\n",
    "from inspect_ai.model import get_model\n",
    "from inspect_ai.solver import TaskState, generate, system_message\n",
    "from inspect_ai.scorer import Score, Scorer, Target, metric, scorer\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "from inspect_ai.dataset import Sample\n",
    "\n",
    "\n",
    "class FactComparator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.parser = PydanticOutputParser(pydantic_object=ComparisonResult)\n",
    "\n",
    "    async def __call__(self, context, answer):\n",
    "        return await self.process_data(context, answer)\n",
    "\n",
    "    async def process_data(self, context, answer):\n",
    "        context_list = (await self.model._agenerate([HumanMessage(content=self._parse_prompt().format(text=context))])).generations[0].text\n",
    "        answer_list = (await self.model._agenerate([HumanMessage(content=self._parse_prompt().format(text=answer))])).generations[0].text\n",
    "\n",
    "        comparison_result = self.parser.parse((await self.model._agenerate([HumanMessage(content=self._compare_prompt().format(context_list=context_list, answer_list=answer_list))])).generations[0].text)\n",
    "\n",
    "        return {\n",
    "            \"context_list\": context_list,\n",
    "            \"answer_list\": answer_list,\n",
    "            \"comparison_result\": comparison_result,\n",
    "        }\n",
    "\n",
    "    def calculate_metrics(self, comparison_result):\n",
    "        facts_in_both_count = len(comparison_result.facts_in_both)\n",
    "        facts_only_in_answer_count = len(comparison_result.facts_only_in_answer)\n",
    "        facts_only_in_context_count = len(comparison_result.facts_only_in_context)\n",
    "\n",
    "        total_answer_facts = facts_in_both_count + facts_only_in_answer_count\n",
    "        total_context_facts = facts_in_both_count + facts_only_in_context_count\n",
    "\n",
    "        groundedness = facts_in_both_count / total_answer_facts * 100 if total_answer_facts > 0 else 0\n",
    "        thoroughness = facts_in_both_count / total_context_facts * 100 if total_context_facts > 0 else 0\n",
    "\n",
    "        return {\n",
    "            \"groundedness\": groundedness,\n",
    "            \"thoroughness\": thoroughness,\n",
    "        }\n",
    "    @staticmethod\n",
    "    def _parse_prompt():\n",
    "        return PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=\"\"\"\n",
    "            Here is a text that may contain one or more facts:\n",
    "\n",
    "            <text>\n",
    "            {text}\n",
    "            </text>\n",
    "\n",
    "            Please parse this text into a list of individual facts. If a sentence contains multiple facts, break it up into separate sentences as needed so that each sentence contains only one fact.\n",
    "\n",
    "            If any of the facts contain pronouns and the pronoun reference is clear, replace the pronoun with the noun it refers to. If the pronoun reference is ambiguous, leave the pronoun as is.\n",
    "\n",
    "        Return the final list of parsed and pronoun-replaced facts inside <facts> tags, with each fact on its own line. Do not include any additional commentary or explanation, including about pronoun changes, number of facts, or truth value of the facts.\n",
    "        \"\"\",\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _compare_prompt():\n",
    "        return PromptTemplate(\n",
    "            input_variables=[\"context_list\", \"answer_list\"],\n",
    "            template=\"\"\"\n",
    "            You will be comparing facts between a context and an answer to determine which facts are shared and which are unique to each.\n",
    "\n",
    "            Here is the context:\n",
    "\n",
    "            <context>\n",
    "            {context_list}\n",
    "            </context>\n",
    "\n",
    "            And here is the answer: \n",
    "\n",
    "            <answer>\n",
    "            {answer_list}\n",
    "            </answer>\n",
    "\n",
    "            Carefully analyze the facts presented in the context and answer, focusing on the semantic meaning rather than the exact wording.\n",
    "\n",
    "            Then, output a dictionary with the following keys and corresponding lists of facts as values:\n",
    "\n",
    "            1. \"facts_in_both\": A list of facts that are present in both the context and the answer\n",
    "\n",
    "            2. \"facts_only_in_answer\": A list of facts that are only present in the answer \n",
    "\n",
    "            3. \"facts_only_in_context\": A list of facts that are only present in the context\n",
    "\n",
    "            Remember, the facts do not need to be worded identically to be considered the same. Focus on whether the core meaning is shared or unique.  A fact in the context may be expressed in different terms in the answer, or multiple facts in one may combine to express a single fact in the other.\n",
    "\n",
    "            Provide your results in this format:\n",
    "\n",
    "            {{\n",
    "                \"facts_in_both\": [\n",
    "                    \"Fact 1 present in both\",\n",
    "                    \"Fact 2 present in both\"\n",
    "                ],\n",
    "                \"facts_only_in_answer\": [\n",
    "                    \"Fact 1 only in answer\",\n",
    "                    \"Fact 2 only in answer\"  \n",
    "                ],\n",
    "                \"facts_only_in_context\": [\n",
    "                    \"Fact 1 only in context\",\n",
    "                    \"Fact 2 only in context\"\n",
    "                ]\n",
    "            }}\n",
    "            \"\"\",\n",
    "        )\n",
    "\n",
    "\n",
    "class ComparisonResult(BaseModel):\n",
    "    facts_in_both: list[str] = Field(default_factory=list, description=\"List of facts present in both context and answer\")\n",
    "    facts_only_in_answer: list[str] = Field(default_factory=list, description=\"List of facts only present in the answer\")\n",
    "    facts_only_in_context: list[str] = Field(default_factory=list, description=\"List of facts only present in the context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa72dbc-fd5a-462c-8308-9238c5cef0ad",
   "metadata": {},
   "source": [
    "## Run on First Pair of Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2751f4e-3992-496c-aad0-c0604a712257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: INSPECT_EVAL_MODEL=openai/gpt-4\n",
      "env: INSPECT_MODEL_NAME=openai/gpt-4\n",
      "\n",
      "Context list:\n",
      "<facts>\n",
      "The fox is brown.\n",
      "The fox runs quickly.\n",
      "The fox's best friend is Sally.\n",
      "Sally is a cat.\n",
      "</facts>\n",
      "\n",
      "Answer list:\n",
      "<facts>\n",
      "The fox is tan.\n",
      "The fox runs fast.\n",
      "The fox's best friend is a cat.\n",
      "The cat's name is Sally.\n",
      "</facts>\n",
      "\n",
      "Comparison result:\n",
      "facts_in_both=['The fox runs quickly.', \"The fox's best friend is Sally.\", 'Sally is a cat.'] facts_only_in_answer=['The fox is tan.'] facts_only_in_context=['The fox is brown.']\n",
      "\n",
      "Metrics:\n",
      "Groundedness: 75.00%\n",
      "Thoroughness: 75.00%\n"
     ]
    }
   ],
   "source": [
    "%env INSPECT_EVAL_MODEL=openai/gpt-4\n",
    "%env INSPECT_MODEL_NAME=openai/gpt-4\n",
    "\n",
    "# Create an instance of InspectChatModel with the specified model\n",
    "inspect_model = InspectChatModel()\n",
    "\n",
    "# Create an instance of FactComparator with the InspectChatModel\n",
    "comparator = FactComparator(inspect_model)\n",
    "\n",
    "\n",
    "context = \"The fox is brown. It runs quickly. The fox's best friend is Sally, which is a cat.\"\n",
    "answer = \"The fox is tan. It runs fast. Its best friend is a cat. She's named Sally.\"\n",
    "\n",
    "# Run the asynchronous process_data method\n",
    "result = await comparator(context, answer)\n",
    "\n",
    "metrics = comparator.calculate_metrics(result[\"comparison_result\"])\n",
    "\n",
    "\n",
    "print(\"\\nContext list:\")\n",
    "print(result[\"context_list\"])\n",
    "\n",
    "print(\"\\nAnswer list:\")\n",
    "print(result[\"answer_list\"])\n",
    "\n",
    "print(\"\\nComparison result:\")\n",
    "print(result[\"comparison_result\"])\n",
    "\n",
    "print(\"\\nMetrics:\")\n",
    "print(f\"Groundedness: {metrics['groundedness']:.2f}%\")\n",
    "print(f\"Thoroughness: {metrics['thoroughness']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ca606e4-b187-4560-970b-fd100cfdde00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The fox runs quickly.', \"The fox's best friend is Sally.\", 'Sally is a cat.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96965d12-1461-407f-8fe7-96f08bb9812c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83dff6fda4ea4b7992ef286621774f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">value={'groundedness': 0.0, 'thoroughness': 0.0} answer=None explanation=\"{'context_list': '&lt;facts&gt;\\\\nI do not know\n",
       "the answer to this.\\\\nI am sorry.\\\\nI cannot help you.\\\\n&lt;/facts&gt;', 'answer_list': '&lt;facts&gt;\\\\nThe cat is \n",
       "red.\\\\n&lt;/facts&gt;', 'comparison_result': ComparisonResult(facts_in_both=[], facts_only_in_answer=['The cat is red.'],\n",
       "facts_only_in_context=['I do not know the answer to this.', 'I am sorry.', 'I cannot help you.'])}\\nModel Output: I\n",
       "don't know the answer to this sorry, I can't help you.\" metadata=None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "value={'groundedness': 0.0, 'thoroughness': 0.0} answer=None explanation=\"{'context_list': '<facts>\\\\nI do not know\n",
       "the answer to this.\\\\nI am sorry.\\\\nI cannot help you.\\\\n</facts>', 'answer_list': '<facts>\\\\nThe cat is \n",
       "red.\\\\n</facts>', 'comparison_result': ComparisonResult(facts_in_both=[], facts_only_in_answer=['The cat is red.'],\n",
       "facts_only_in_context=['I do not know the answer to this.', 'I am sorry.', 'I cannot help you.'])}\\nModel Output: I\n",
       "don't know the answer to this sorry, I can't help you.\" metadata=None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">value={'groundedness': 50.0, 'thoroughness': 33.33333333333333} answer=None explanation=\"{'context_list': \n",
       "'&lt;facts&gt;\\\\nBuffalo buffalo that other Buffalo buffalo bully also bully Buffalo buffalo.\\\\nBuffalo buffalo are \n",
       "bullied by other Buffalo buffalo.\\\\n&lt;/facts&gt;', 'answer_list': '&lt;facts&gt;\\\\nBuffalo buffalo.\\\\n&lt;/facts&gt;', \n",
       "'comparison_result': ComparisonResult(facts_in_both=['Buffalo buffalo'], facts_only_in_answer=['None'], \n",
       "facts_only_in_context=['Buffalo buffalo that other Buffalo buffalo bully also bully Buffalo buffalo.', 'Buffalo \n",
       "buffalo are bullied by other Buffalo buffalo.'])}\\nModel Output: Buffalo buffalo Buffalo buffalo buffalo buffalo \n",
       "Buffalo buffalo.\" metadata=None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "value={'groundedness': 50.0, 'thoroughness': 33.33333333333333} answer=None explanation=\"{'context_list': \n",
       "'<facts>\\\\nBuffalo buffalo that other Buffalo buffalo bully also bully Buffalo buffalo.\\\\nBuffalo buffalo are \n",
       "bullied by other Buffalo buffalo.\\\\n</facts>', 'answer_list': '<facts>\\\\nBuffalo buffalo.\\\\n</facts>', \n",
       "'comparison_result': ComparisonResult(facts_in_both=['Buffalo buffalo'], facts_only_in_answer=['None'], \n",
       "facts_only_in_context=['Buffalo buffalo that other Buffalo buffalo bully also bully Buffalo buffalo.', 'Buffalo \n",
       "buffalo are bullied by other Buffalo buffalo.'])}\\nModel Output: Buffalo buffalo Buffalo buffalo buffalo buffalo \n",
       "Buffalo buffalo.\" metadata=None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class FactComparatorScorer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.fact_comparator = FactComparator(model)\n",
    "\n",
    "    async def __call__(self, state: TaskState, target: Sample):\n",
    "        context = state.input\n",
    "        target_text = target.target\n",
    "\n",
    "        result = await self.fact_comparator.process_data(context, target_text)\n",
    "        metrics = self.fact_comparator.calculate_metrics(result[\"comparison_result\"])\n",
    "\n",
    "        scorer_value = {\n",
    "            \"groundedness\": metrics[\"groundedness\"],\n",
    "            \"thoroughness\": metrics[\"thoroughness\"],\n",
    "        }\n",
    "\n",
    "        explanation = str(result) + f\"\\nModel Output: {context}\"\n",
    "\n",
    "        return Score(\n",
    "            value=scorer_value,\n",
    "            explanation=explanation,\n",
    "        )\n",
    "        \n",
    "@metric\n",
    "def thoroughness():\n",
    "  def metric(scores: list[Score]) -> float:\n",
    "    total = 0.0\n",
    "    for item in scores:\n",
    "      metadata = item.metadata\n",
    "      if metadata is not None:\n",
    "          total += float(metadata[\"thoroughness\"])\n",
    "    return total / float(len(scores))\n",
    "  return metric\n",
    "\n",
    "@metric\n",
    "def groundedness():\n",
    "  def metric(scores: list[Score]) -> float:\n",
    "    total = 0.0\n",
    "    for item in scores:\n",
    "        metadata = item.metadata\n",
    "        if metadata is not None:\n",
    "            total += float(metadata[\"groundedness\"])\n",
    "    return total / float(len(scores))\n",
    "  return metric\n",
    "\n",
    "    \n",
    "@scorer(metrics=[groundedness(), thoroughness()])\n",
    "def fact_comparator_scorer(model) -> Scorer:\n",
    "  \n",
    "  async def score(state: TaskState, target: Target) -> Score:\n",
    "\n",
    "    # Create an instance of the scorer\n",
    "    model = InspectChatModel()\n",
    "    fact_comparator_scorer = FactComparatorScorer(model)\n",
    "\n",
    "    # Call the scorer\n",
    "    score = await fact_comparator_scorer(state, target)\n",
    "    print(score)\n",
    "\n",
    "    # Ignore the actual processing and return a dummy value\n",
    "    grounded_score = score.value['groundedness']\n",
    "    thorough_score = score.value['thoroughness']\n",
    "    explanation = score.explanation\n",
    "\n",
    "    answer = state.output.completion\n",
    "\n",
    "    return Score(\n",
    "        value=f\"grounddness:{grounded_score} :thoroughness{thorough_score}\", # make a better string?\n",
    "        answer=answer,\n",
    "        explanation= \"nothing\",\n",
    "        metadata = {\n",
    "           \"thoroughness\": thorough_score,\n",
    "           \"groundedness\": grounded_score,\n",
    "            \"stuff\": explanation\n",
    "        }\n",
    "    )\n",
    "\n",
    "  return score\n",
    "\n",
    "# Define your samples\n",
    "samples = [\n",
    "    Sample(\n",
    "        input=\"Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo.\",\n",
    "        target=\"Buffalo buffalo.\",\n",
    "        id=\"sample_1\"\n",
    "    ),\n",
    "    Sample(\n",
    "        input=\"I don't know the answer to this sorry, I can't help you.\",\n",
    "        target=\"The cat is red.\",\n",
    "        id=\"sample_2\"\n",
    "    ),\n",
    "    # Add more samples as needed\n",
    "]\n",
    "\n",
    "# Define your task\n",
    "SYSTEM_MESSAGE = \"You are an AI assistant helping with fact comparison.\"\n",
    "\n",
    "@task\n",
    "def my_eval():\n",
    "  return Task(\n",
    "      dataset=samples,\n",
    "      plan=[\n",
    "          system_message(SYSTEM_MESSAGE),\n",
    "          generate()\n",
    "      ],\n",
    "      scorer=fact_comparator_scorer(model=get_model()),\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    eval(my_eval(), model=\"openai/gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56b7a79-7ef5-4c61-9967-fa3b46844193",
   "metadata": {},
   "source": [
    "Need to fix\n",
    "\n",
    "1. Do generation of actual responses from model\n",
    "2. Better guarantee of formatting holding\n",
    "3. Pronouns\n",
    "4. Better accuracy for overlap\n",
    "5. Get running in vscode\n",
    "6. Test cases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64390128-ba9e-477e-91c0-1d4c0a5d4c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Virtual Env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
