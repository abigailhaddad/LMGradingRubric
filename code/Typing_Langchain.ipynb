{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a851d3d-cb9b-448c-ae3b-21c524601308",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5119e1e4-68fd-4176-9b72-ff30c8f3c38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: langchain in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (0.1.19)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.6.6)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.0.38)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.1.52)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.1.56)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain) (2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (8.1.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipywidgets) (8.21.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipywidgets) (5.14.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: decorator in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (0.1.6)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.46 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-openai) (0.1.52)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.24.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-openai) (1.28.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-openai) (0.5.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (0.1.56)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (2.6.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (8.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (0.26.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain-openai) (3.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.46->langchain-openai) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langchain-openai) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langchain-openai) (2.16.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.24.0->langchain-openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain\n",
    "%pip install ipywidgets\n",
    "%pip install langchain-openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35219496-2769-4767-831c-f85ac271b390",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbb8fd73-0951-4687-9e4f-3c2433666dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Dict, Protocol, cast, runtime_checkable\n",
    "\n",
    "from langchain_core.callbacks import (\n",
    "    AsyncCallbackManagerForLLMRun,\n",
    "    CallbackManagerForLLMRun,\n",
    ")\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "from langchain_core.messages import ToolCall as LCToolCall\n",
    "from langchain_core.outputs import (\n",
    "    ChatGeneration,\n",
    "    ChatResult,\n",
    ")\n",
    "from pydantic.v1 import Field\n",
    "from typing_extensions import override\n",
    "\n",
    "from inspect_ai.model import (\n",
    "    ChatMessage,\n",
    "    ChatMessageAssistant,\n",
    "    ChatMessageSystem,\n",
    "    ChatMessageTool,\n",
    "    ChatMessageUser,\n",
    "    Content,\n",
    "    ContentImage,\n",
    "    ContentText,\n",
    "    GenerateConfig,\n",
    "    ModelName,\n",
    "    ModelOutput,\n",
    "    ToolCall,\n",
    "    ToolChoice,\n",
    "    ToolInfo,\n",
    "    ToolParam,\n",
    "    get_model,\n",
    ")\n",
    "from inspect_ai.solver import Generate, Solver, TaskState\n",
    "\n",
    "@runtime_checkable\n",
    "class LangChainAgent(Protocol):\n",
    "    async def __call__(\n",
    "        self, llm: BaseChatModel, input: dict[str, Any]\n",
    "    ) -> str | list[str | dict[str, Any]]:\n",
    "        ...\n",
    "\n",
    "\n",
    "def langchain_solver(agent: LangChainAgent) -> Solver:\n",
    "    async def solve(state: TaskState, generate: Generate) -> TaskState:\n",
    "        # create the inspect model api bridge\n",
    "        llm = InspectChatModel()\n",
    "\n",
    "        # call the agent\n",
    "        await agent(\n",
    "            llm=llm,\n",
    "            input=dict(\n",
    "                input=state.user_prompt.text,\n",
    "                chat_history=as_langchain_chat_history(state.messages[1:]),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # collect output from llm interface\n",
    "        state.messages = llm.messages\n",
    "        state.output = llm.output\n",
    "\n",
    "        # return state\n",
    "        return state\n",
    "\n",
    "    return solve\n",
    "\n",
    "\n",
    "class InspectChatModel(BaseChatModel):\n",
    "    # track messages and model output so we can update\n",
    "    # the inspect task state when we are complete\n",
    "    messages: list[ChatMessage] = Field(default=[], exclude=True)\n",
    "    output: ModelOutput = Field(default=ModelOutput(), exclude=True)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return f\"Inspect ({ModelName(get_model()).api})\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"model_name\": str(ModelName(get_model()).name),\n",
    "        }\n",
    "\n",
    "    @override\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: list[BaseMessage],\n",
    "        stop: list[str] | None = None,\n",
    "        run_manager: CallbackManagerForLLMRun | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        # inspect uses async exclusively\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @override\n",
    "    async def _agenerate(\n",
    "        self,\n",
    "        messages: list[BaseMessage],\n",
    "        stop: list[str] | None = None,\n",
    "        run_manager: AsyncCallbackManagerForLLMRun | None = None,\n",
    "        **kwargs: dict[str, Any],\n",
    "    ) -> ChatResult:\n",
    "        # extract tools from kwargs\n",
    "        tools: list[ToolInfo] = []\n",
    "        tool_choice: ToolChoice | None = None\n",
    "        lc_tools = cast(list[dict[str, Any]] | None, kwargs.get(\"tools\", None))\n",
    "        if lc_tools:\n",
    "            tools = [\n",
    "                ToolInfo(\n",
    "                    name=tool[\"function\"][\"name\"],\n",
    "                    description=tool[\"function\"][\"description\"],\n",
    "                    params=as_inspect_tool_params(tool[\"function\"][\"parameters\"]),\n",
    "                )\n",
    "                for tool in lc_tools\n",
    "            ]\n",
    "            tool_choice = \"auto\"\n",
    "\n",
    "        # generate\n",
    "        input = [as_inspect_message(message) for message in messages]\n",
    "        result = await get_model().generate(\n",
    "            input=input,\n",
    "            tools=tools,\n",
    "            tool_choice=tool_choice,\n",
    "            config=GenerateConfig(stop_seqs=stop),\n",
    "        )\n",
    "\n",
    "        # track last messages / model output\n",
    "        self.messages = input\n",
    "        self.messages.append(result.choices[0].message)\n",
    "        self.output = result\n",
    "\n",
    "        # extract choices\n",
    "        generations = [\n",
    "            ChatGeneration(message=as_langchain_message(choice.message))\n",
    "            for choice in result.choices\n",
    "        ]\n",
    "\n",
    "        # return\n",
    "        return ChatResult(generations=generations)\n",
    "\n",
    "\n",
    "def as_inspect_message(message: BaseMessage) -> ChatMessage:\n",
    "    if isinstance(message, SystemMessage):\n",
    "        return ChatMessageSystem(content=as_inspect_content(message.content))\n",
    "    elif isinstance(message, HumanMessage):\n",
    "        return ChatMessageUser(content=as_inspect_content(message.content))\n",
    "    elif isinstance(message, AIMessage):\n",
    "        return ChatMessageAssistant(\n",
    "            content=as_inspect_content(message.content),\n",
    "            tool_calls=(\n",
    "                [\n",
    "                    ToolCall(\n",
    "                        type=\"function\",\n",
    "                        function=call[\"name\"],\n",
    "                        id=call[\"id\"] or call[\"name\"],\n",
    "                        arguments=call[\"args\"],\n",
    "                    )\n",
    "                    for call in message.tool_calls\n",
    "                ]\n",
    "                if message.tool_calls and len(message.tool_calls) > 0\n",
    "                else None\n",
    "            ),\n",
    "        )\n",
    "    elif isinstance(message, ToolMessage):\n",
    "        return ChatMessageTool(\n",
    "            content=as_inspect_content(message.content),\n",
    "            tool_call_id=message.tool_call_id,\n",
    "        )\n",
    "    elif isinstance(message, FunctionMessage):\n",
    "        return ChatMessageTool(\n",
    "            content=as_inspect_content(message.content), tool_call_id=message.name\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected message type: {type(message)}\")\n",
    "\n",
    "\n",
    "def as_langchain_message(message: ChatMessage) -> BaseMessage:\n",
    "    if isinstance(message, ChatMessageSystem):\n",
    "        return SystemMessage(content=as_langchain_content(message.content))\n",
    "    elif isinstance(message, ChatMessageUser):\n",
    "        return HumanMessage(content=as_langchain_content(message.content))\n",
    "    elif isinstance(message, ChatMessageAssistant):\n",
    "        additional_kwargs: dict[str, Any] = {}\n",
    "        if message.tool_calls and len(message.tool_calls) > 0:\n",
    "            additional_kwargs[\"tool_calls\"] = [\n",
    "                dict(\n",
    "                    id=call.id, name=call.function, arguments=json.dumps(call.arguments)\n",
    "                )\n",
    "                for call in message.tool_calls\n",
    "            ]\n",
    "\n",
    "        return AIMessage(\n",
    "            content=as_langchain_content(message.content),\n",
    "            tool_calls=(\n",
    "                [\n",
    "                    LCToolCall(id=call.id, name=call.function, args=call.arguments)\n",
    "                    for call in message.tool_calls\n",
    "                ]\n",
    "                if message.tool_calls\n",
    "                else []\n",
    "            ),\n",
    "            additional_kwargs=additional_kwargs,\n",
    "        )\n",
    "    elif isinstance(message, ChatMessageTool):\n",
    "        return ToolMessage(\n",
    "            content=as_langchain_content(message.content),\n",
    "            tool_call_id=message.tool_call_id or \"\",\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected message type: {type(message)}\")\n",
    "\n",
    "\n",
    "def as_langchain_chat_history(messages: list[ChatMessage]) -> list[dict[str, Any]]:\n",
    "    return [dict(role=message.role, content=message.text) for message in messages]\n",
    "\n",
    "\n",
    "def as_inspect_content(\n",
    "    content: str | list[str | dict[str, Any]],\n",
    ") -> str | list[Content]:\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "    else:\n",
    "        return [\n",
    "            (\n",
    "                ContentText(text=c)\n",
    "                if isinstance(c, str)\n",
    "                else (\n",
    "                    ContentText(text=c[\"text\"])\n",
    "                    if c[\"type\"] == \"text\"\n",
    "                    else ContentImage(image=c[\"image\"])\n",
    "                )\n",
    "            )\n",
    "            for c in content\n",
    "        ]\n",
    "\n",
    "\n",
    "def as_inspect_tool_params(parameters: dict[str, Any]) -> list[ToolParam]:\n",
    "    params: list[ToolParam] = []\n",
    "    for key, param in parameters[\"properties\"].items():\n",
    "        params.append(\n",
    "            ToolParam(\n",
    "                name=key,\n",
    "                type=param[\"type\"],\n",
    "                description=param.get(\"description\", param.get(\"title\")),\n",
    "                optional=key not in parameters[\"required\"],\n",
    "            )\n",
    "        )\n",
    "    return params\n",
    "\n",
    "\n",
    "def as_langchain_content(\n",
    "    content: str | list[Content],\n",
    ") -> str | list[str | dict[str, Any]]:\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "    else:\n",
    "        return [c if isinstance(c, str) else c.model_dump() for c in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d108b5b-d0ad-4b5f-8b2d-308364e05dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "import pandas as pd\n",
    "\n",
    "class FactComparator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.parser = PydanticOutputParser(pydantic_object=ComparisonResult)\n",
    "\n",
    "    async def __call__(self, context, answer):\n",
    "        return await self.process_data(context, answer)\n",
    "\n",
    "    async def process_data(self, context, answer):\n",
    "        context_replace_pronouns = (await self.model._agenerate([HumanMessage(content=self._pronoun_prompt().format(text=context))])).generations[0].text\n",
    "        answer_replace_pronouns = (await self.model._agenerate([HumanMessage(content=self._pronoun_prompt().format(text=answer))])).generations[0].text\n",
    "\n",
    "        context_list = (await self.model._agenerate([HumanMessage(content=self._parse_prompt().format(text=context_replace_pronouns))])).generations[0].text\n",
    "        answer_list = (await self.model._agenerate([HumanMessage(content=self._parse_prompt().format(text=answer_replace_pronouns))])).generations[0].text\n",
    "\n",
    "        comparison_result = self.parser.parse((await self.model._agenerate([HumanMessage(content=self._compare_prompt().format(context_list=context_list, answer_list=answer_list))])).generations[0].text)\n",
    "\n",
    "        return {\n",
    "            \"context_replace_pronouns\": context_replace_pronouns,\n",
    "            \"answer_replace_pronouns\": answer_replace_pronouns,\n",
    "            \"context_list\": context_list,\n",
    "            \"answer_list\": answer_list,\n",
    "            \"comparison_result\": comparison_result,\n",
    "        }\n",
    "\n",
    "    def calculate_metrics(self, comparison_result):\n",
    "        facts_in_both_count = len(comparison_result.facts_in_both)\n",
    "        facts_only_in_answer_count = len(comparison_result.facts_only_in_answer)\n",
    "        facts_only_in_context_count = len(comparison_result.facts_only_in_context)\n",
    "\n",
    "        total_answer_facts = facts_in_both_count + facts_only_in_answer_count\n",
    "        total_context_facts = facts_in_both_count + facts_only_in_context_count\n",
    "\n",
    "        groundedness = facts_in_both_count / total_answer_facts * 100 if total_answer_facts > 0 else 0\n",
    "        thoroughness = facts_in_both_count / total_context_facts * 100 if total_context_facts > 0 else 0\n",
    "\n",
    "        return {\n",
    "            \"groundedness\": groundedness,\n",
    "            \"thoroughness\": thoroughness,\n",
    "        }\n",
    "\n",
    "    def process_data_list(self, data_list):\n",
    "        results = []\n",
    "        for data in data_list:\n",
    "            context = data['context']\n",
    "            answer = data['answer']\n",
    "    \n",
    "            try:\n",
    "                result = asyncio.run(self.process_data(context, answer))\n",
    "                metrics = self.calculate_metrics(result[\"comparison_result\"])\n",
    "\n",
    "                result_data = {\n",
    "                    'context': context,\n",
    "                    'answer': answer,\n",
    "                    'context_replace_pronouns': result[\"context_replace_pronouns\"],\n",
    "                    'answer_replace_pronouns': result[\"answer_replace_pronouns\"],\n",
    "                    'context_list': result[\"context_list\"],\n",
    "                    'answer_list': result[\"answer_list\"],\n",
    "                    'facts_in_both': ', '.join(result[\"comparison_result\"].facts_in_both),\n",
    "                    'facts_only_in_answer': ', '.join(result[\"comparison_result\"].facts_only_in_answer),\n",
    "                    'facts_only_in_context': ', '.join(result[\"comparison_result\"].facts_only_in_context),\n",
    "                    'groundedness': metrics['groundedness'],\n",
    "                    'thoroughness': metrics['thoroughness']\n",
    "                }\n",
    "                results.append(result_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing data item: {data}\")\n",
    "                print(f\"Exception: {e}\")\n",
    "                continue\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    @staticmethod\n",
    "    def _pronoun_prompt():\n",
    "        return PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=\"\"\"\n",
    "            Your task is to replace all the pronouns in the following text with the nouns they refer to:\n",
    "\n",
    "            <text>\n",
    "            {text}\n",
    "            </text>\n",
    "\n",
    "        Please follow these steps to replace the pronouns in the text with the nouns they refer to:\n",
    "\n",
    "        1. Read through the text carefully and identify all the pronouns (words like \"he\", \"she\", \"it\", \"they\", \"them\", etc.). If there are no pronouns, simply return the original text unchanged.\n",
    "\n",
    "        2. For each pronoun you find, look back in the text to determine which specific noun or name that pronoun is referring to. The referent noun will generally be the most recent noun or name mentioned before the pronoun that matches the pronoun in number (singular/plural) and gender.\n",
    "\n",
    "        3. If a pronoun is part of text inside quotation marks, do not replace that pronoun, as it is part of a direct quote.\n",
    "\n",
    "        4. Replace each pronoun with the most recent matching noun or name that it refers to. \n",
    "\n",
    "        5. If it is unclear which noun a pronoun is referring to, or if no matching referent can be found, do not replace that pronoun. Leave it as is.\n",
    "\n",
    "        6. Continue this process, scanning through the text and replacing appropriate pronouns with their referent nouns, until all pronouns with clear referents have been replaced.\n",
    "\n",
    "        7. Return the modified text with pronouns replaced. Do not include any of your analysis or thought process, or a statement that no pronouns were found, ONLY return the text itself with pronouns replaced (or the original text if no changes were made).\n",
    "            \"\"\",\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_prompt():\n",
    "        return PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=\"\"\"\n",
    "            Please parse the following text into a list of individual facts:\n",
    "\n",
    "            <text>\n",
    "            {text}\n",
    "            </text>\n",
    "\n",
    "            Read the text carefully. Your task is to break it down into the key facts it contains. Parse out each individual fact into a separate sentence, even if that means splitting up or rewording the original sentences. The goal is to have a clear, concise list of the core facts contained in the text.\n",
    "\n",
    "            Output the parsed facts in a numbered list, with each fact written as a complete sentence on its own line. Use <facts> tags to demarcate the start and end of the list.\n",
    "            \"\"\",\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _compare_prompt():\n",
    "        return PromptTemplate(\n",
    "            input_variables=[\"context_list\", \"answer_list\"],\n",
    "            template=\"\"\"\n",
    "            You will be comparing facts between a context and an answer to determine which facts are shared and which are unique to each.\n",
    "\n",
    "            Here is the context:\n",
    "\n",
    "            <context>\n",
    "            {context_list}\n",
    "            </context>\n",
    "\n",
    "            And here is the answer: \n",
    "\n",
    "            <answer>\n",
    "            {answer_list}\n",
    "            </answer>\n",
    "\n",
    "            Carefully analyze the facts presented in the context and answer, focusing on the semantic meaning rather than the exact wording.\n",
    "\n",
    "            Then, output a dictionary with the following keys and corresponding lists of facts as values:\n",
    "\n",
    "            1. \"facts_in_both\": A list of facts that are present in both the context and the answer\n",
    "\n",
    "            2. \"facts_only_in_answer\": A list of facts that are only present in the answer \n",
    "\n",
    "            3. \"facts_only_in_context\": A list of facts that are only present in the context\n",
    "\n",
    "            Remember, the facts do not need to be worded identically to be considered the same. Focus on whether the core meaning is shared or unique.\n",
    "\n",
    "            Provide your results in this format:\n",
    "\n",
    "            {{\n",
    "                \"facts_in_both\": [\n",
    "                    \"Fact 1 present in both\",\n",
    "                    \"Fact 2 present in both\"\n",
    "                ],\n",
    "                \"facts_only_in_answer\": [\n",
    "                    \"Fact 1 only in answer\",\n",
    "                    \"Fact 2 only in answer\"  \n",
    "                ],\n",
    "                \"facts_only_in_context\": [\n",
    "                    \"Fact 1 only in context\",\n",
    "                    \"Fact 2 only in context\"\n",
    "                ]\n",
    "            }}\n",
    "            \"\"\",\n",
    "        )\n",
    "\n",
    "\n",
    "class ComparisonResult(BaseModel):\n",
    "    facts_in_both: list[str] = Field(default_factory=list, description=\"List of facts present in both context and answer\")\n",
    "    facts_only_in_answer: list[str] = Field(default_factory=list, description=\"List of facts only present in the answer\")\n",
    "    facts_only_in_context: list[str] = Field(default_factory=list, description=\"List of facts only present in the context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa72dbc-fd5a-462c-8308-9238c5cef0ad",
   "metadata": {},
   "source": [
    "## Run on First Pair of Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2751f4e-3992-496c-aad0-c0604a712257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: INSPECT_EVAL_MODEL=openai/gpt-4\n",
      "env: INSPECT_MODEL_NAME=openai/gpt-4\n",
      "Context with replaced pronouns:\n",
      "The quick brown fox jumps over the rock because the quick brown fox is happy. The quick brown fox was born in 2005. The hedgehog was born in 2010, but the hedgehog is even happier than the quick brown fox.\n",
      "Context with replaced pronouns:\n",
      "The quick brown fox jumps over the rock because the quick brown fox is happy. The quick brown fox was born in 2005. The hedgehog was born in 2010, but the hedgehog is even happier than the quick brown fox.\n",
      "\n",
      "Answer with replaced pronouns:\n",
      "The quick brown fox was born in 2005, and the hedgehog in 2010. The quick brown fox is not as happy as the hedgehog.\n",
      "\n",
      "Context list:\n",
      "<facts>\n",
      "1. The quick brown fox jumps over the rock.\n",
      "2. The quick brown fox is happy.\n",
      "3. The quick brown fox was born in 2005.\n",
      "4. The hedgehog was born in 2010.\n",
      "5. The hedgehog is happier than the quick brown fox.\n",
      "</facts>\n",
      "\n",
      "Answer list:\n",
      "<facts>\n",
      "1. The quick brown fox was born in 2005.\n",
      "2. The hedgehog was born in 2010.\n",
      "3. The quick brown fox is not as happy as the hedgehog.\n",
      "</facts>\n",
      "\n",
      "Comparison result:\n",
      "facts_in_both=['The quick brown fox was born in 2005.', 'The hedgehog was born in 2010.', 'The quick brown fox is not as happy as the hedgehog.'] facts_only_in_answer=[] facts_only_in_context=['The quick brown fox jumps over the rock.', 'The quick brown fox is happy.']\n",
      "\n",
      "Metrics:\n",
      "Groundedness: 100.00%\n",
      "Thoroughness: 60.00%\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "%env INSPECT_EVAL_MODEL=openai/gpt-4\n",
    "%env INSPECT_MODEL_NAME=openai/gpt-4\n",
    "\n",
    "# Create an instance of InspectChatModel with the specified model\n",
    "inspect_model = InspectChatModel()\n",
    "\n",
    "# Create an instance of FactComparator with the InspectChatModel\n",
    "comparator = FactComparator(inspect_model)\n",
    "\n",
    "\n",
    "context = \"The quick brown fox jumps over the rock because he's happy. He was born in 2005. The hedgehog was born in 2010, but she's even happier than him.\"\n",
    "answer = \"The quick brown fox was born in 2005, and the hedgehog in 2010. The quick brown fox is not as happy as the hedgehog\"\n",
    "\n",
    "# Run the asynchronous process_data method\n",
    "result = await comparator(context, answer)\n",
    "\n",
    "metrics = comparator.calculate_metrics(result[\"comparison_result\"])\n",
    "\n",
    "print(\"Context with replaced pronouns:\")\n",
    "print(result[\"context_replace_pronouns\"])\n",
    "\n",
    "print(\"Context with replaced pronouns:\")\n",
    "print(result[\"context_replace_pronouns\"])\n",
    "\n",
    "print(\"\\nAnswer with replaced pronouns:\")\n",
    "print(result[\"answer_replace_pronouns\"])\n",
    "\n",
    "print(\"\\nContext list:\")\n",
    "print(result[\"context_list\"])\n",
    "\n",
    "print(\"\\nAnswer list:\")\n",
    "print(result[\"answer_list\"])\n",
    "\n",
    "print(\"\\nComparison result:\")\n",
    "print(result[\"comparison_result\"])\n",
    "\n",
    "print(\"\\nMetrics:\")\n",
    "print(f\"Groundedness: {metrics['groundedness']:.2f}%\")\n",
    "print(f\"Thoroughness: {metrics['thoroughness']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11437091-c8a1-40fe-b55b-d4f06c0941c3",
   "metadata": {},
   "source": [
    "## Run on another pair of statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad11011d-dd24-4c8f-b4e9-e8573b0c7ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: INSPECT_EVAL_MODEL=openai/gpt-4\n",
      "env: INSPECT_MODEL_NAME=openai/gpt-4\n",
      "Context with replaced pronouns:\n",
      "There are no pronouns in the text:\n",
      "\n",
      "\"To boil pasta, first bring a large pot of salted water to a rolling boil over high heat.\"\n",
      "\n",
      "Answer with replaced pronouns:\n",
      "To boil pasta, begin by filling a large pot with water, making sure there's enough to fully submerge the pasta. Bring the water to a rolling boil over high heat, then add salt to enhance the pasta's flavor. Once the water is boiling, carefully add the pasta, stirring gently to prevent the pasta from sticking. Cook the pasta according to the package instructions or until the pasta reaches your desired level of tenderness, usually around 8-12 minutes. To check for doneness, taste a piece of pasta—the pasta should be tender but still slightly firm (al dente).\n",
      "\n",
      "Context list:\n",
      "<facts>\n",
      "1. There are no pronouns in the given text.\n",
      "2. The given text is about boiling pasta.\n",
      "3. The first step to boil pasta is to bring a large pot of salted water to a rolling boil.\n",
      "4. The pot of salted water should be heated over high heat.\n",
      "</facts>\n",
      "\n",
      "Answer list:\n",
      "<facts>\n",
      "1. Pasta is boiled by starting with a large pot filled with water.\n",
      "2. The pot of water must be sufficient to fully submerge the pasta.\n",
      "3. The water is brought to a rolling boil over high heat.\n",
      "4. Salt is added to boiling water to enhance the pasta's flavor.\n",
      "5. The pasta is added to the boiling water carefully.\n",
      "6. The pasta is stirred gently to prevent it from sticking.\n",
      "7. The pasta is cooked according to the package instructions or until it reaches desired level of tenderness.\n",
      "8. The typical cooking time ranges from 8-12 minutes.\n",
      "9. To check pasta's doneness, a piece is tasted.\n",
      "10. The pasta should be tender but still slightly firm, or al dente, when properly cooked.\n",
      "</facts>\n",
      "\n",
      "Comparison result:\n",
      "facts_in_both=['The first step to boil pasta is to bring a large pot of salted water to a rolling boil.', 'The pot of salted water should be heated over high heat.'] facts_only_in_answer=['Pasta is boiled by starting with a large pot filled with water.', 'The pot of water must be sufficient to fully submerge the pasta.', \"Salt is added to boiling water to enhance the pasta's flavor.\", 'The pasta is added to the boiling water carefully.', 'The pasta is stirred gently to prevent it from sticking.', 'The pasta is cooked according to the package instructions or until it reaches desired level of tenderness.', 'The typical cooking time ranges from 8-12 minutes.', \"To check pasta's doneness, a piece is tasted.\", 'The pasta should be tender but still slightly firm, or al dente, when properly cooked.'] facts_only_in_context=['There are no pronouns in the given text.', 'The given text is about boiling pasta.']\n",
      "\n",
      "Metrics:\n",
      "Groundedness: 18.18%\n",
      "Thoroughness: 50.00%\n"
     ]
    }
   ],
   "source": [
    "%env INSPECT_EVAL_MODEL=openai/gpt-4\n",
    "%env INSPECT_MODEL_NAME=openai/gpt-4\n",
    "\n",
    "# Create an instance of InspectChatModel with the specified model\n",
    "inspect_model = InspectChatModel()\n",
    "\n",
    "# Create an instance of FactComparator with the InspectChatModel\n",
    "comparator = FactComparator(inspect_model)\n",
    "\n",
    "context = \"To boil pasta, first bring a large pot of salted water to a rolling boil over high heat..\"\n",
    "answer = \"To boil pasta, begin by filling a large pot with water, making sure there's enough to fully submerge the pasta. Bring the water to a rolling boil over high heat, then add salt to enhance the pasta's flavor. Once the water is boiling, carefully add the pasta, stirring gently to prevent sticking. Cook the pasta according to the package instructions or until it reaches your desired level of tenderness, usually around 8-12 minutes. To check for doneness, taste a piece of pasta—it should be tender but still slightly firm (al dente).\"\n",
    "\n",
    "# Run the asynchronous process_data method\n",
    "result = asyncio.run(comparator.process_data(context, answer))\n",
    "\n",
    "metrics = comparator.calculate_metrics(result[\"comparison_result\"])\n",
    "print(\"Context with replaced pronouns:\")\n",
    "print(result[\"context_replace_pronouns\"])\n",
    "\n",
    "print(\"\\nAnswer with replaced pronouns:\")\n",
    "print(result[\"answer_replace_pronouns\"])\n",
    "\n",
    "print(\"\\nContext list:\")\n",
    "print(result[\"context_list\"])\n",
    "\n",
    "print(\"\\nAnswer list:\")\n",
    "print(result[\"answer_list\"])\n",
    "\n",
    "print(\"\\nComparison result:\")\n",
    "print(result[\"comparison_result\"])\n",
    "\n",
    "print(\"\\nMetrics:\")\n",
    "print(f\"Groundedness: {metrics['groundedness']:.2f}%\")\n",
    "print(f\"Thoroughness: {metrics['thoroughness']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaf37e7-66f4-422b-926d-cc1b93109a89",
   "metadata": {},
   "source": [
    "## Run on a list of dictionaries - return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9514e054-f73a-49e8-b893-61981fe6d802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>context_replace_pronouns</th>\n",
       "      <th>answer_replace_pronouns</th>\n",
       "      <th>context_list</th>\n",
       "      <th>answer_list</th>\n",
       "      <th>facts_in_both</th>\n",
       "      <th>facts_only_in_answer</th>\n",
       "      <th>facts_only_in_context</th>\n",
       "      <th>groundedness</th>\n",
       "      <th>thoroughness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The quick brown fox jumps over the rock becaus...</td>\n",
       "      <td>The quick brown fox was born in 2005, and the ...</td>\n",
       "      <td>&lt;text&gt;\\nThe quick brown fox jumps over the roc...</td>\n",
       "      <td>The quick brown fox was born in 2005, and the ...</td>\n",
       "      <td>&lt;facts&gt;\\n1. The quick brown fox jumps over the...</td>\n",
       "      <td>&lt;facts&gt;\\n1. The quick brown fox was born in 20...</td>\n",
       "      <td>The quick brown fox was born in 2005., The hed...</td>\n",
       "      <td></td>\n",
       "      <td>The quick brown fox jumps over the rock., The ...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The sun is a star at the center of our solar s...</td>\n",
       "      <td>The sun is a star located approximately 93 mil...</td>\n",
       "      <td>The sun is a star at the center of our solar s...</td>\n",
       "      <td>The sun is a star located approximately 93 mil...</td>\n",
       "      <td>&lt;facts&gt;\\n1. The sun is a star.\\n2. The sun is ...</td>\n",
       "      <td>&lt;facts&gt;\\n1. The sun is a star.\\n2. The sun is ...</td>\n",
       "      <td>The sun is a star., The sun is approximately 9...</td>\n",
       "      <td>The sun is not a solid object., The sun is a s...</td>\n",
       "      <td>The sun is located at the center of our solar ...</td>\n",
       "      <td>71.428571</td>\n",
       "      <td>83.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Birds are warm-blooded vertebrates that lay eg...</td>\n",
       "      <td>Birds are a diverse group of animals with feat...</td>\n",
       "      <td>The text does not contain any pronouns, so it ...</td>\n",
       "      <td>The text contains no pronouns, so it remains u...</td>\n",
       "      <td>&lt;facts&gt;\\n1. Birds are warm-blooded vertebrates...</td>\n",
       "      <td>&lt;facts&gt;\\n1. Birds are a diverse group of anima...</td>\n",
       "      <td>Birds are warm-blooded vertebrates., Birds lay...</td>\n",
       "      <td>Birds are a diverse group of animals.</td>\n",
       "      <td>Birds have beaks.</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>88.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Eiffel Tower is a wrought-iron lattice tow...</td>\n",
       "      <td>The Eiffel Tower, found in Paris, France, is a...</td>\n",
       "      <td>The Eiffel Tower is a wrought-iron lattice tow...</td>\n",
       "      <td>The Eiffel Tower, found in Paris, France, is a...</td>\n",
       "      <td>&lt;facts&gt;\\n1. The Eiffel Tower is a wrought-iron...</td>\n",
       "      <td>&lt;facts&gt;\\n1. The Eiffel Tower is located in Par...</td>\n",
       "      <td>The Eiffel Tower is a wrought-iron lattice tow...</td>\n",
       "      <td>Gustave Eiffel's company was responsible for t...</td>\n",
       "      <td></td>\n",
       "      <td>75.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Great Wall of China is a series of fortifi...</td>\n",
       "      <td>The Great Wall of China, a series of walls and...</td>\n",
       "      <td>The Great Wall of China is a series of fortifi...</td>\n",
       "      <td>The Great Wall of China, a series of walls and...</td>\n",
       "      <td>&lt;facts&gt;\\n1. The Great Wall of China is a serie...</td>\n",
       "      <td>&lt;facts&gt;\\n1. The Great Wall of China is a serie...</td>\n",
       "      <td>The Great Wall of China is a series of fortifi...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  The quick brown fox jumps over the rock becaus...   \n",
       "1  The sun is a star at the center of our solar s...   \n",
       "2  Birds are warm-blooded vertebrates that lay eg...   \n",
       "3  The Eiffel Tower is a wrought-iron lattice tow...   \n",
       "4  The Great Wall of China is a series of fortifi...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  The quick brown fox was born in 2005, and the ...   \n",
       "1  The sun is a star located approximately 93 mil...   \n",
       "2  Birds are a diverse group of animals with feat...   \n",
       "3  The Eiffel Tower, found in Paris, France, is a...   \n",
       "4  The Great Wall of China, a series of walls and...   \n",
       "\n",
       "                            context_replace_pronouns  \\\n",
       "0  <text>\\nThe quick brown fox jumps over the roc...   \n",
       "1  The sun is a star at the center of our solar s...   \n",
       "2  The text does not contain any pronouns, so it ...   \n",
       "3  The Eiffel Tower is a wrought-iron lattice tow...   \n",
       "4  The Great Wall of China is a series of fortifi...   \n",
       "\n",
       "                             answer_replace_pronouns  \\\n",
       "0  The quick brown fox was born in 2005, and the ...   \n",
       "1  The sun is a star located approximately 93 mil...   \n",
       "2  The text contains no pronouns, so it remains u...   \n",
       "3  The Eiffel Tower, found in Paris, France, is a...   \n",
       "4  The Great Wall of China, a series of walls and...   \n",
       "\n",
       "                                        context_list  \\\n",
       "0  <facts>\\n1. The quick brown fox jumps over the...   \n",
       "1  <facts>\\n1. The sun is a star.\\n2. The sun is ...   \n",
       "2  <facts>\\n1. Birds are warm-blooded vertebrates...   \n",
       "3  <facts>\\n1. The Eiffel Tower is a wrought-iron...   \n",
       "4  <facts>\\n1. The Great Wall of China is a serie...   \n",
       "\n",
       "                                         answer_list  \\\n",
       "0  <facts>\\n1. The quick brown fox was born in 20...   \n",
       "1  <facts>\\n1. The sun is a star.\\n2. The sun is ...   \n",
       "2  <facts>\\n1. Birds are a diverse group of anima...   \n",
       "3  <facts>\\n1. The Eiffel Tower is located in Par...   \n",
       "4  <facts>\\n1. The Great Wall of China is a serie...   \n",
       "\n",
       "                                       facts_in_both  \\\n",
       "0  The quick brown fox was born in 2005., The hed...   \n",
       "1  The sun is a star., The sun is approximately 9...   \n",
       "2  Birds are warm-blooded vertebrates., Birds lay...   \n",
       "3  The Eiffel Tower is a wrought-iron lattice tow...   \n",
       "4  The Great Wall of China is a series of fortifi...   \n",
       "\n",
       "                                facts_only_in_answer  \\\n",
       "0                                                      \n",
       "1  The sun is not a solid object., The sun is a s...   \n",
       "2              Birds are a diverse group of animals.   \n",
       "3  Gustave Eiffel's company was responsible for t...   \n",
       "4                                                      \n",
       "\n",
       "                               facts_only_in_context  groundedness  \\\n",
       "0  The quick brown fox jumps over the rock., The ...    100.000000   \n",
       "1  The sun is located at the center of our solar ...     71.428571   \n",
       "2                                  Birds have beaks.     88.888889   \n",
       "3                                                        75.000000   \n",
       "4                                                       100.000000   \n",
       "\n",
       "   thoroughness  \n",
       "0     60.000000  \n",
       "1     83.333333  \n",
       "2     88.888889  \n",
       "3    100.000000  \n",
       "4    100.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list = [\n",
    "    {\n",
    "        'context': 'The quick brown fox jumps over the rock because he\\'s happy. He was born in 2005. The hedgehog was born in 2010, but she\\'s even happier than him.',\n",
    "        'answer': 'The quick brown fox was born in 2005, and the hedgehog in 2010. The quick brown fox is not as happy as the hedgehog'\n",
    "    },\n",
    "    {\n",
    "        'context': 'The sun is a star at the center of our solar system. It is about 93 million miles away from Earth. The sun is a hot ball of glowing gases that provides light and warmth to Earth.',\n",
    "        'answer': 'The sun is a star located approximately 93 million miles from Earth. It is the source of light and heat for our planet. The sun is not a solid object, but rather a sphere of hot glowing gases.'\n",
    "    },\n",
    "    {\n",
    "        'context': 'Birds are warm-blooded vertebrates that lay eggs and have feathers, wings, and beaks. There are over 10,000 species of birds worldwide. Some common bird species include sparrows, pigeons, and parrots.',\n",
    "        'answer': 'Birds are a diverse group of animals with feathers and wings. They are warm-blooded egg-laying vertebrates. The number of bird species globally exceeds 10,000. Pigeons, parrots, and sparrows are among the most familiar bird types.'\n",
    "    },\n",
    "    {\n",
    "        'context': 'The Eiffel Tower is a wrought-iron lattice tower located on the Champ de Mars in Paris, France. It was constructed from 1887 to 1889 and stands at a height of 324 meters. The tower is named after Gustave Eiffel, whose company designed and built it.',\n",
    "        'answer': 'The Eiffel Tower, found in Paris, France, is a lattice tower made of wrought iron. Built between 1887 and 1889, it reaches a height of 324 meters. Gustave Eiffel\\'s company was responsible for the tower\\'s design and construction, hence its name.'\n",
    "    },\n",
    "    {\n",
    "        'context': 'The Great Wall of China is a series of fortifications and walls built across the historical northern borders of ancient Chinese states and Imperial China. The most well-known sections were built during the Ming dynasty, which ruled from 1368 to 1644.',\n",
    "        'answer': 'The Great Wall of China, a series of walls and fortifications, was constructed along the northern borders of ancient Chinese states and Imperial China. The Ming dynasty, which lasted from 1368 to 1644, is responsible for the construction of the most famous sections of the wall.'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create an instance of InspectChatModel with the specified model\n",
    "inspect_model = InspectChatModel()\n",
    "\n",
    "# Create an instance of FactComparator with the InspectChatModel\n",
    "comparator = FactComparator(inspect_model)\n",
    "\n",
    "df = comparator.process_data_list(data_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6c2f75d-0436-463e-b2cc-ae51199dea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "Value = Union[\n",
    "    str | int | float | bool,\n",
    "    list[str | int | float | bool],\n",
    "    dict[str, str | int | float | bool],\n",
    "]\n",
    "\n",
    "class Score(BaseModel):\n",
    "    \"\"\"Score generated by a scorer.\n",
    "\n",
    "    Args:\n",
    "       value (Value): Score value.\n",
    "       answer (str | None): Answer extracted from model output (optional).\n",
    "       explanation (str | None): Explanation of score (optional).\n",
    "       metadata (dict[str,Any]): Additional metadata related to the score.\n",
    "    \"\"\"\n",
    "\n",
    "    value: Value\n",
    "    \"\"\"Score value.\"\"\"\n",
    "\n",
    "    answer: str | None = Field(default=None)\n",
    "    \"\"\"Answer extracted from model output (optional)\"\"\"\n",
    "\n",
    "    explanation: str | None = Field(default=None)\n",
    "    \"\"\"Explanation of score (optional).\"\"\"\n",
    "\n",
    "    metadata: dict[str, Any] | None = Field(default=None)\n",
    "    \"\"\"Additional metadata related to the score\"\"\"\n",
    "\n",
    "    @property\n",
    "    def text(self) -> str:\n",
    "        \"\"\"Read the score as text.\"\"\"\n",
    "        return self.as_str()\n",
    "\n",
    "    def as_str(self) -> str:\n",
    "        \"\"\"Read the score as a string.\"\"\"\n",
    "        return str(self._as_scalar())\n",
    "\n",
    "    def as_int(self) -> int:\n",
    "        \"\"\"Read the score as an integer.\"\"\"\n",
    "        return int(self._as_scalar())\n",
    "\n",
    "    def as_float(self) -> float:\n",
    "        \"\"\"Read the score as a float.\"\"\"\n",
    "        return float(self._as_scalar())\n",
    "\n",
    "    def as_bool(self) -> bool:\n",
    "        \"\"\"Read the score as a boolean.\"\"\"\n",
    "        return bool(self._as_scalar())\n",
    "\n",
    "    def _as_scalar(self) -> str | int | float | bool:\n",
    "        if (\n",
    "            isinstance(self.value, str)\n",
    "            or isinstance(self.value, int)\n",
    "            or isinstance(self.value, float)\n",
    "            or isinstance(self.value, bool)\n",
    "        ):\n",
    "            return self.value\n",
    "        else:\n",
    "            raise ValueError(\"This score is not a scalar\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7c1af45-b434-4c84-afc3-630676d9a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from inspect_ai.scorer import scorer, Scorer, Target\n",
    "\n",
    "class ScorerValue(NamedTuple):\n",
    "    groundedness: float\n",
    "    thoroughness: float\n",
    "\n",
    "def groundedness_thoroughness_metric(scores):\n",
    "    groundedness = [score.value.groundedness for score in scores]\n",
    "    thoroughness = [score.value.thoroughness for score in scores]\n",
    "    return {\n",
    "        \"groundedness\": sum(groundedness) / len(groundedness) if groundedness else 0,\n",
    "        \"thoroughness\": sum(thoroughness) / len(thoroughness) if thoroughness else 0,\n",
    "    }\n",
    "\n",
    "@scorer(metrics=[groundedness_thoroughness_metric])\n",
    "class FactComparatorScorer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.fact_comparator = FactComparator(model)\n",
    "\n",
    "    async def __call__(self, answer: str, target: Target):\n",
    "        result = await self.fact_comparator.process_data(target.text, answer)\n",
    "        metrics = self.fact_comparator.calculate_metrics(result[\"comparison_result\"])\n",
    "\n",
    "        return Score(\n",
    "            value=ScorerValue(metrics[\"groundedness\"], metrics[\"thoroughness\"]),\n",
    "            explanation=str(result),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "630aef33-7d8a-4402-b93a-c86088ee8d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100.0', '50.0']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming you have the necessary data\n",
    "answer = \"The fox is brown.\"\n",
    "target_text = \"The fox is brown.\"\n",
    "\n",
    "# Create a Target object\n",
    "target = Target(target_text)\n",
    "\n",
    "# Create an instance of the scorer\n",
    "model = InspectChatModel()\n",
    "fact_comparator_scorer = FactComparatorScorer(model)\n",
    "\n",
    "# Call the scorer\n",
    "score = await fact_comparator_scorer(answer, target)\n",
    "\n",
    "# Access the score value and explanation\n",
    "\n",
    "scorer_value = score.value\n",
    "groundedness = scorer_value[0]\n",
    "thoroughness = scorer_value[0]\n",
    "explanation = score.explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390a7dca-3899-4c58-b5bf-9e4963498c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Virtual Env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
