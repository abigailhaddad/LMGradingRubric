{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a851d3d-cb9b-448c-ae3b-21c524601308",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5119e1e4-68fd-4176-9b72-ff30c8f3c38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (0.1.19)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.6.6)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.0.38)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.1.52)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.1.56)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain) (2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (8.1.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipywidgets) (8.21.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipywidgets) (5.14.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: decorator in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (0.1.6)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.46 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-openai) (0.1.52)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.24.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-openai) (1.28.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-openai) (0.5.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (0.1.56)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (2.6.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (8.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (0.26.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain-openai) (3.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.46->langchain-openai) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langchain-openai) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langchain-openai) (2.16.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.24.0->langchain-openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain\n",
    "%pip install ipywidgets\n",
    "%pip install langchain-openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35219496-2769-4767-831c-f85ac271b390",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbb8fd73-0951-4687-9e4f-3c2433666dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Dict, Protocol, cast, runtime_checkable\n",
    "\n",
    "from langchain_core.callbacks import AsyncCallbackManagerForLLMRun, CallbackManagerForLLMRun\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, BaseMessage, FunctionMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.messages import ToolCall as LCToolCall\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult\n",
    "from pydantic.v1 import Field\n",
    "from typing_extensions import override\n",
    "\n",
    "from inspect_ai.model import (\n",
    "    ChatMessage,\n",
    "    ChatMessageAssistant,\n",
    "    ChatMessageSystem,\n",
    "    ChatMessageTool,\n",
    "    ChatMessageUser,\n",
    "    Content,\n",
    "    ContentImage,\n",
    "    ContentText,\n",
    "    GenerateConfig,\n",
    "    ModelName,\n",
    "    ModelOutput,\n",
    "    ToolCall,\n",
    "    ToolChoice,\n",
    "    ToolInfo,\n",
    "    ToolParam,\n",
    "    get_model,\n",
    ")\n",
    "from inspect_ai.solver import Generate, Solver, TaskState\n",
    "\n",
    "@runtime_checkable\n",
    "class LangChainAgent(Protocol):\n",
    "    async def __call__(\n",
    "        self, llm: BaseChatModel, input: dict[str, Any]\n",
    "    ) -> str | list[str | dict[str, Any]]:\n",
    "        ...\n",
    "\n",
    "\n",
    "def langchain_solver(agent: LangChainAgent) -> Solver:\n",
    "    async def solve(state: TaskState, generate: Generate) -> TaskState:\n",
    "        # create the inspect model api bridge\n",
    "        llm = InspectChatModel()\n",
    "\n",
    "        # call the agent\n",
    "        await agent(\n",
    "            llm=llm,\n",
    "            input=dict(\n",
    "                input=state.user_prompt.text,\n",
    "                chat_history=as_langchain_chat_history(state.messages[1:]),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # collect output from llm interface\n",
    "        state.messages = llm.messages\n",
    "        state.output = llm.output\n",
    "\n",
    "        # return state\n",
    "        return state\n",
    "\n",
    "    return solve\n",
    "\n",
    "\n",
    "class InspectChatModel(BaseChatModel):\n",
    "    # track messages and model output so we can update\n",
    "    # the inspect task state when we are complete\n",
    "    messages: list[ChatMessage] = Field(default=[], exclude=True)\n",
    "    output: ModelOutput = Field(default=ModelOutput(), exclude=True)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return f\"Inspect ({ModelName(get_model()).api})\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"model_name\": str(ModelName(get_model()).name),\n",
    "        }\n",
    "\n",
    "    @override\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: list[BaseMessage],\n",
    "        stop: list[str] | None = None,\n",
    "        run_manager: CallbackManagerForLLMRun | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        # inspect uses async exclusively\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @override\n",
    "    async def _agenerate(\n",
    "        self,\n",
    "        messages: list[BaseMessage],\n",
    "        stop: list[str] | None = None,\n",
    "        run_manager: AsyncCallbackManagerForLLMRun | None = None,\n",
    "        **kwargs: dict[str, Any],\n",
    "    ) -> ChatResult:\n",
    "        # extract tools from kwargs\n",
    "        tools: list[ToolInfo] = []\n",
    "        tool_choice: ToolChoice | None = None\n",
    "        lc_tools = cast(list[dict[str, Any]] | None, kwargs.get(\"tools\", None))\n",
    "        if lc_tools:\n",
    "            tools = [\n",
    "                ToolInfo(\n",
    "                    name=tool[\"function\"][\"name\"],\n",
    "                    description=tool[\"function\"][\"description\"],\n",
    "                    params=as_inspect_tool_params(tool[\"function\"][\"parameters\"]),\n",
    "                )\n",
    "                for tool in lc_tools\n",
    "            ]\n",
    "            tool_choice = \"auto\"\n",
    "\n",
    "        # generate\n",
    "        input = [as_inspect_message(message) for message in messages]\n",
    "        result = await get_model().generate(\n",
    "            input=input,\n",
    "            tools=tools,\n",
    "            tool_choice=tool_choice,\n",
    "            config=GenerateConfig(stop_seqs=stop),\n",
    "        )\n",
    "\n",
    "        # track last messages / model output\n",
    "        self.messages = input\n",
    "        self.messages.append(result.choices[0].message)\n",
    "        self.output = result\n",
    "\n",
    "        # extract choices\n",
    "        generations = [\n",
    "            ChatGeneration(message=as_langchain_message(choice.message))\n",
    "            for choice in result.choices\n",
    "        ]\n",
    "\n",
    "        # return\n",
    "        return ChatResult(generations=generations)\n",
    "\n",
    "\n",
    "def as_inspect_message(message: BaseMessage) -> ChatMessage:\n",
    "    if isinstance(message, SystemMessage):\n",
    "        return ChatMessageSystem(content=as_inspect_content(message.content))\n",
    "    elif isinstance(message, HumanMessage):\n",
    "        return ChatMessageUser(content=as_inspect_content(message.content))\n",
    "    elif isinstance(message, AIMessage):\n",
    "        return ChatMessageAssistant(\n",
    "            content=as_inspect_content(message.content),\n",
    "            tool_calls=(\n",
    "                [\n",
    "                    ToolCall(\n",
    "                        type=\"function\",\n",
    "                        function=call[\"name\"],\n",
    "                        id=call[\"id\"] or call[\"name\"],\n",
    "                        arguments=call[\"args\"],\n",
    "                    )\n",
    "                    for call in message.tool_calls\n",
    "                ]\n",
    "                if message.tool_calls and len(message.tool_calls) > 0\n",
    "                else None\n",
    "            ),\n",
    "        )\n",
    "    elif isinstance(message, ToolMessage):\n",
    "        return ChatMessageTool(\n",
    "            content=as_inspect_content(message.content),\n",
    "            tool_call_id=message.tool_call_id,\n",
    "        )\n",
    "    elif isinstance(message, FunctionMessage):\n",
    "        return ChatMessageTool(\n",
    "            content=as_inspect_content(message.content), tool_call_id=message.name\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected message type: {type(message)}\")\n",
    "\n",
    "\n",
    "def as_langchain_message(message: ChatMessage) -> BaseMessage:\n",
    "    if isinstance(message, ChatMessageSystem):\n",
    "        return SystemMessage(content=as_langchain_content(message.content))\n",
    "    elif isinstance(message, ChatMessageUser):\n",
    "        return HumanMessage(content=as_langchain_content(message.content))\n",
    "    elif isinstance(message, ChatMessageAssistant):\n",
    "        additional_kwargs: dict[str, Any] = {}\n",
    "        if message.tool_calls and len(message.tool_calls) > 0:\n",
    "            additional_kwargs[\"tool_calls\"] = [\n",
    "                dict(\n",
    "                    id=call.id, name=call.function, arguments=json.dumps(call.arguments)\n",
    "                )\n",
    "                for call in message.tool_calls\n",
    "            ]\n",
    "\n",
    "        return AIMessage(\n",
    "            content=as_langchain_content(message.content),\n",
    "            tool_calls=(\n",
    "                [\n",
    "                    LCToolCall(id=call.id, name=call.function, args=call.arguments)\n",
    "                    for call in message.tool_calls\n",
    "                ]\n",
    "                if message.tool_calls\n",
    "                else []\n",
    "            ),\n",
    "            additional_kwargs=additional_kwargs,\n",
    "        )\n",
    "    elif isinstance(message, ChatMessageTool):\n",
    "        return ToolMessage(\n",
    "            content=as_langchain_content(message.content),\n",
    "            tool_call_id=message.tool_call_id or \"\",\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected message type: {type(message)}\")\n",
    "\n",
    "\n",
    "def as_langchain_chat_history(messages: list[ChatMessage]) -> list[dict[str, Any]]:\n",
    "    return [dict(role=message.role, content=message.text) for message in messages]\n",
    "\n",
    "\n",
    "def as_inspect_content(\n",
    "    content: str | list[str | dict[str, Any]],\n",
    ") -> str | list[Content]:\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "    else:\n",
    "        return [\n",
    "            (\n",
    "                ContentText(text=c)\n",
    "                if isinstance(c, str)\n",
    "                else (\n",
    "                    ContentText(text=c[\"text\"])\n",
    "                    if c[\"type\"] == \"text\"\n",
    "                    else ContentImage(image=c[\"image\"])\n",
    "                )\n",
    "            )\n",
    "            for c in content\n",
    "        ]\n",
    "\n",
    "\n",
    "def as_inspect_tool_params(parameters: dict[str, Any]) -> list[ToolParam]:\n",
    "    params: list[ToolParam] = []\n",
    "    for key, param in parameters[\"properties\"].items():\n",
    "        params.append(\n",
    "            ToolParam(\n",
    "                name=key,\n",
    "                type=param[\"type\"],\n",
    "                description=param.get(\"description\", param.get(\"title\")),\n",
    "                optional=key not in parameters[\"required\"],\n",
    "            )\n",
    "        )\n",
    "    return params\n",
    "\n",
    "\n",
    "def as_langchain_content(\n",
    "    content: str | list[Content],\n",
    ") -> str | list[str | dict[str, Any]]:\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "    else:\n",
    "        return [c if isinstance(c, str) else c.model_dump() for c in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d108b5b-d0ad-4b5f-8b2d-308364e05dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from inspect_ai import eval, Task, task\n",
    "from inspect_ai.model import get_model\n",
    "from inspect_ai.solver import TaskState, generate, system_message\n",
    "from inspect_ai.scorer import Score, Scorer, Target, metric, scorer\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "from inspect_ai.dataset import Sample\n",
    "\n",
    "\n",
    "class FactComparator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.parser = PydanticOutputParser(pydantic_object=ComparisonResult)\n",
    "\n",
    "    async def __call__(self, context, answer):\n",
    "        return await self.process_data(context, answer)\n",
    "\n",
    "    async def process_data(self, context, answer):\n",
    "        context_list = (await self.model._agenerate([HumanMessage(content=self._parse_prompt().format(text=context))])).generations[0].text\n",
    "        answer_list = (await self.model._agenerate([HumanMessage(content=self._parse_prompt().format(text=answer))])).generations[0].text\n",
    "\n",
    "        comparison_result = self.parser.parse((await self.model._agenerate([HumanMessage(content=self._compare_prompt().format(context_list=context_list, answer_list=answer_list))])).generations[0].text)\n",
    "\n",
    "        return {\n",
    "            \"context_list\": context_list,\n",
    "            \"answer_list\": answer_list,\n",
    "            \"comparison_result\": comparison_result,\n",
    "        }\n",
    "\n",
    "    def calculate_metrics(self, comparison_result):\n",
    "        facts_in_both_count = len(comparison_result.facts_in_both)\n",
    "        facts_only_in_answer_count = len(comparison_result.facts_only_in_answer)\n",
    "        facts_only_in_context_count = len(comparison_result.facts_only_in_context)\n",
    "\n",
    "        total_answer_facts = facts_in_both_count + facts_only_in_answer_count\n",
    "        total_context_facts = facts_in_both_count + facts_only_in_context_count\n",
    "\n",
    "        groundedness = facts_in_both_count / total_answer_facts * 100 if total_answer_facts > 0 else 0\n",
    "        thoroughness = facts_in_both_count / total_context_facts * 100 if total_context_facts > 0 else 0\n",
    "\n",
    "        return {\n",
    "            \"groundedness\": groundedness,\n",
    "            \"thoroughness\": thoroughness,\n",
    "        }\n",
    "    @staticmethod\n",
    "    def _parse_prompt():\n",
    "        return PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=\"\"\"\n",
    "            Here is a text that may contain one or more facts:\n",
    "\n",
    "            <text>\n",
    "            {text}\n",
    "            </text>\n",
    "\n",
    "            Please parse this text into a list of individual facts. If a sentence contains multiple facts, break it up into separate sentences as needed so that each sentence contains only one fact.\n",
    "\n",
    "            If any of the facts contain pronouns and the pronoun reference is clear, replace the pronoun with the noun it refers to. If the pronoun reference is ambiguous, leave the pronoun as is.\n",
    "\n",
    "        Return the final list of parsed and pronoun-replaced facts inside <facts> tags, with each fact on its own line. Do not include any additional commentary or explanation, including about pronoun changes, number of facts, or truth value of the facts.\n",
    "        \"\"\",\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _compare_prompt():\n",
    "        return PromptTemplate(\n",
    "            input_variables=[\"context_list\", \"answer_list\"],\n",
    "            template=\"\"\"\n",
    "            You will be comparing facts between a context and an answer to determine which facts are shared and which are unique to each.\n",
    "\n",
    "            Here is the context:\n",
    "\n",
    "            <context>\n",
    "            {context_list}\n",
    "            </context>\n",
    "\n",
    "            And here is the answer: \n",
    "\n",
    "            <answer>\n",
    "            {answer_list}\n",
    "            </answer>\n",
    "\n",
    "            Carefully analyze the facts presented in the context and answer, focusing on the semantic meaning rather than the exact wording.\n",
    "\n",
    "            Then, output a dictionary with the following keys and corresponding lists of facts as values:\n",
    "\n",
    "            1. \"facts_in_both\": A list of facts that are present in both the context and the answer\n",
    "\n",
    "            2. \"facts_only_in_answer\": A list of facts that are only present in the answer \n",
    "\n",
    "            3. \"facts_only_in_context\": A list of facts that are only present in the context\n",
    "\n",
    "            Remember, the facts do not need to be worded identically to be considered the same. Focus on whether the core meaning is shared or unique.  A fact in the context may be expressed in different terms in the answer, or multiple facts in one may combine to express a single fact in the other.\n",
    "\n",
    "            Provide your results in this format:\n",
    "\n",
    "            {{\n",
    "                \"facts_in_both\": [\n",
    "                    \"Fact 1 present in both\",\n",
    "                    \"Fact 2 present in both\"\n",
    "                ],\n",
    "                \"facts_only_in_answer\": [\n",
    "                    \"Fact 1 only in answer\",\n",
    "                    \"Fact 2 only in answer\"  \n",
    "                ],\n",
    "                \"facts_only_in_context\": [\n",
    "                    \"Fact 1 only in context\",\n",
    "                    \"Fact 2 only in context\"\n",
    "                ]\n",
    "            }}\n",
    "            \"\"\",\n",
    "        )\n",
    "\n",
    "\n",
    "class ComparisonResult(BaseModel):\n",
    "    facts_in_both: list[str] = Field(default_factory=list, description=\"List of facts present in both context and answer\")\n",
    "    facts_only_in_answer: list[str] = Field(default_factory=list, description=\"List of facts only present in the answer\")\n",
    "    facts_only_in_context: list[str] = Field(default_factory=list, description=\"List of facts only present in the context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa72dbc-fd5a-462c-8308-9238c5cef0ad",
   "metadata": {},
   "source": [
    "## Run on First Pair of Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ca606e4-b187-4560-970b-fd100cfdde00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The fox runs quickly.', \"The fox's best friend is Sally.\", 'Sally is a cat.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2751f4e-3992-496c-aad0-c0604a712257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: INSPECT_EVAL_MODEL=openai/gpt-4\n",
      "env: INSPECT_MODEL_NAME=openai/gpt-4\n",
      "\n",
      "Context list:\n",
      "<facts>\n",
      "The fox is brown.\n",
      "The fox runs quickly.\n",
      "The fox's best friend is Sally.\n",
      "Sally is a cat.\n",
      "</facts>\n",
      "\n",
      "Answer list:\n",
      "<facts>\n",
      "The fox is tan.\n",
      "The fox runs fast.\n",
      "The fox's best friend is a cat.\n",
      "The cat's name is Sally.\n",
      "</facts>\n",
      "\n",
      "Comparison result:\n",
      "facts_in_both=['The fox runs quickly.', \"The fox's best friend is Sally.\", 'Sally is a cat.'] facts_only_in_answer=['The fox is tan.'] facts_only_in_context=['The fox is brown.']\n",
      "\n",
      "Metrics:\n",
      "Groundedness: 75.00%\n",
      "Thoroughness: 75.00%\n"
     ]
    }
   ],
   "source": [
    "%env INSPECT_EVAL_MODEL=openai/gpt-4\n",
    "%env INSPECT_MODEL_NAME=openai/gpt-4\n",
    "\n",
    "# Create an instance of InspectChatModel with the specified model\n",
    "inspect_model = InspectChatModel()\n",
    "\n",
    "# Create an instance of FactComparator with the InspectChatModel\n",
    "comparator = FactComparator(inspect_model)\n",
    "\n",
    "\n",
    "context = \"The fox is brown. It runs quickly. The fox's best friend is Sally, which is a cat.\"\n",
    "answer = \"The fox is tan. It runs fast. Its best friend is a cat. She's named Sally.\"\n",
    "\n",
    "# Run the asynchronous process_data method\n",
    "result = await comparator(context, answer)\n",
    "\n",
    "metrics = comparator.calculate_metrics(result[\"comparison_result\"])\n",
    "\n",
    "\n",
    "print(\"\\nContext list:\")\n",
    "print(result[\"context_list\"])\n",
    "\n",
    "print(\"\\nAnswer list:\")\n",
    "print(result[\"answer_list\"])\n",
    "\n",
    "print(\"\\nComparison result:\")\n",
    "print(result[\"comparison_result\"])\n",
    "\n",
    "print(\"\\nMetrics:\")\n",
    "print(f\"Groundedness: {metrics['groundedness']:.2f}%\")\n",
    "print(f\"Thoroughness: {metrics['thoroughness']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0267bdb7-24dc-4efd-815f-4fd4a0da941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "class ModelComparator:\n",
    "    def __init__(self, model):\n",
    "        self.inspect_model = InspectChatModel()\n",
    "        self.comparator = FactComparator(self.inspect_model)\n",
    "\n",
    "    async def run_and_compare(self, target_statement, input_statement):\n",
    "        try:\n",
    "            result = await self.comparator(target_statement, input_statement)\n",
    "            metrics = self.comparator.calculate_metrics(result[\"comparison_result\"])\n",
    "            groundedness_model = metrics['groundedness']\n",
    "            thoroughness_model = metrics['thoroughness']\n",
    "            context_list = result[\"context_list\"]\n",
    "            answer_list = result[\"answer_list\"]\n",
    "            comparison_result = result[\"comparison_result\"]\n",
    "            model_error = None\n",
    "        except Exception as e:\n",
    "            groundedness_model = None\n",
    "            thoroughness_model = None\n",
    "            context_list = None\n",
    "            answer_list = None\n",
    "            comparison_result = None\n",
    "            model_error = str(e)\n",
    "\n",
    "        return {\n",
    "            'Groundedness (Model)': groundedness_model,\n",
    "            'Thoroughness (Model)': thoroughness_model,\n",
    "            'Context List': context_list,\n",
    "            'Answer List': answer_list,\n",
    "            'Facts in Both': comparison_result.facts_in_both if comparison_result else None,\n",
    "            'Facts Only in Answer': comparison_result.facts_only_in_answer if comparison_result else None,\n",
    "            'Facts Only in Context': comparison_result.facts_only_in_context if comparison_result else None,\n",
    "            'Model Error': model_error\n",
    "        }\n",
    "\n",
    "def compare_metrics(cases: Dict[str, Dict[str, Tuple[str, str, Dict[str, float], str]]]):\n",
    "    data = []\n",
    "    model_comparator = ModelComparator(model='openai/gpt-4')\n",
    "\n",
    "    for case_name, case_data in cases.items():\n",
    "        input_statement = case_data['input']\n",
    "        target_statement = case_data['target']\n",
    "        true_metrics = case_data['true_metrics']\n",
    "        description = case_data['description']\n",
    "\n",
    "        model_results = asyncio.run(model_comparator.run_and_compare(target_statement, input_statement))\n",
    "\n",
    "        groundedness_true = true_metrics['groundedness']\n",
    "        thoroughness_true = true_metrics['thoroughness']\n",
    "\n",
    "        data.append({\n",
    "            'Case': case_name,\n",
    "            'Input Statement': input_statement,\n",
    "            'Target Statement': target_statement,\n",
    "            'Description': description,\n",
    "            'Groundedness (True)': groundedness_true,\n",
    "            'Thoroughness (True)': thoroughness_true,\n",
    "            **model_results\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a69e13a4-b1cc-4d30-9aa1-9fbb39d88c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: INSPECT_EVAL_MODEL=openai/gpt-4\n",
      "env: INSPECT_MODEL_NAME=openai/gpt-4\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "cases = {\n",
    "    'case1': {\n",
    "        'input': 'The Sun is a medium-sized star. It\\'s about 4.6 billion years old.',\n",
    "        'target': 'The sun is approximately 4.6 billion years old. It\\'s a mid-sized star.',\n",
    "        'true_metrics': {'groundedness': 100, 'thoroughness': 100},\n",
    "        'description': 'This is a basic use case with pronouns and mild rephrasing.'\n",
    "    },\n",
    "    'case2': {\n",
    "        'input': 'The Sun, a medium-sized star, is located at the center of our Solar System and is approximately 4.6 billion years old.',\n",
    "        'target': 'The sun is a mid-sized star which has existed for about 4.6 billion years.',\n",
    "        'true_metrics': {'groundedness': 67, 'thoroughness': 100},\n",
    "        'description': 'This is a basic use case with mild rephrasing.'\n",
    "    },\n",
    "    'case3': {\n",
    "        'input': 'Sally is Rachel\\'s cat.',\n",
    "        'target': 'Sally is a cat. Rachel is her owner.',\n",
    "        'true_metrics': {'groundedness': 100, 'thoroughness': 100},  \n",
    "        'description': 'This case involves simple restructuring and clarification.'\n",
    "    },\n",
    "    'case4': {\n",
    "        'input': 'Sally is larger than Stan.',\n",
    "        'target': 'Stan is smaller than Sally.',\n",
    "        'true_metrics': {'groundedness': 100, 'thoroughness': 100}, \n",
    "        'description': 'This case demonstrates a change in comparative perspective.'\n",
    "    },\n",
    "    'case5': {\n",
    "        'input': 'the average temperature today is 20 degrees celsius.',\n",
    "        'target': 'the mean temperature today is 68 degrees fahrenheit.',\n",
    "        'true_metrics': {'groundedness': 100, 'thoroughness': 100},  \n",
    "        'description': 'This case involves unit conversion and synonym use.'\n",
    "    },\n",
    "    'case6': {\n",
    "        'input': 'the average temperature today is 20 degrees celsius.',\n",
    "        'target': 'the average temperature today is 50 degrees celsius.',\n",
    "        'true_metrics': {'groundedness': 0, 'thoroughness': 0},  \n",
    "        'description': 'This case involves unit conversion and synonym use.'\n",
    "    },\n",
    "    'case7': {\n",
    "        'input': 'The company has an ATO now, so they have been sanctioned by the government and you can work with them.', \n",
    "        'target':  'The company has been sanctioned by the government in response to recent lawbreaking activity.' , \n",
    "        'true_metrics': {'groundedness': 0, 'thoroughness': 0},  # Contextual misuse\n",
    "        'description': 'This case uses \"sanctioned\" in a way that highlights its dual meaning: approved or penalized.'\n",
    "    },\n",
    "    # Add more cases as needed\n",
    "}\n",
    "\n",
    "\n",
    "%env INSPECT_EVAL_MODEL=openai/gpt-4\n",
    "%env INSPECT_MODEL_NAME=openai/gpt-4\n",
    "\n",
    "import asyncio\n",
    "\n",
    "df = compare_metrics(cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b415a9b4-d71f-4673-9f84-36d5dbc2e3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Input Statement</th>\n",
       "      <th>Target Statement</th>\n",
       "      <th>Description</th>\n",
       "      <th>Groundedness (True)</th>\n",
       "      <th>Thoroughness (True)</th>\n",
       "      <th>Groundedness (Model)</th>\n",
       "      <th>Thoroughness (Model)</th>\n",
       "      <th>Context List</th>\n",
       "      <th>Answer List</th>\n",
       "      <th>Facts in Both</th>\n",
       "      <th>Facts Only in Answer</th>\n",
       "      <th>Facts Only in Context</th>\n",
       "      <th>Model Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>case1</td>\n",
       "      <td>The Sun is a medium-sized star. It's about 4.6...</td>\n",
       "      <td>The sun is approximately 4.6 billion years old...</td>\n",
       "      <td>This is a basic use case with pronouns and mil...</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>&lt;facts&gt;\\nThe sun is approximately 4.6 billion ...</td>\n",
       "      <td>&lt;facts&gt;\\nThe Sun is a medium-sized star.\\nThe ...</td>\n",
       "      <td>[The sun is approximately 4.6 billion years ol...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>case2</td>\n",
       "      <td>The Sun, a medium-sized star, is located at th...</td>\n",
       "      <td>The sun is a mid-sized star which has existed ...</td>\n",
       "      <td>This is a basic use case with mild rephrasing.</td>\n",
       "      <td>67</td>\n",
       "      <td>100</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>&lt;facts&gt;\\nThe sun is a mid-sized star.\\nThe sun...</td>\n",
       "      <td>&lt;facts&gt;\\nThe Sun is a medium-sized star.\\nThe ...</td>\n",
       "      <td>[The sun is a mid-sized star., The sun has exi...</td>\n",
       "      <td>[The Sun is located at the center of our Solar...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>case3</td>\n",
       "      <td>Sally is Rachel's cat.</td>\n",
       "      <td>Sally is a cat. Rachel is her owner.</td>\n",
       "      <td>This case involves simple restructuring and cl...</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>&lt;facts&gt;\\nSally is a cat.\\nRachel is the owner ...</td>\n",
       "      <td>&lt;facts&gt;\\nSally is Rachel's cat.\\n&lt;/facts&gt;</td>\n",
       "      <td>[Sally is a cat., Rachel is the owner of Sally.]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>case4</td>\n",
       "      <td>Sally is larger than Stan.</td>\n",
       "      <td>Stan is smaller than Sally.</td>\n",
       "      <td>This case demonstrates a change in comparative...</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>&lt;facts&gt;\\nStan is smaller than Sally.\\n&lt;/facts&gt;</td>\n",
       "      <td>&lt;facts&gt;\\nSally is larger than Stan.\\n&lt;/facts&gt;</td>\n",
       "      <td>[Stan is smaller than Sally, Sally is larger t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>case5</td>\n",
       "      <td>the average temperature today is 20 degrees ce...</td>\n",
       "      <td>the mean temperature today is 68 degrees fahre...</td>\n",
       "      <td>This case involves unit conversion and synonym...</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>&lt;facts&gt;\\nThe mean temperature today is 68 degr...</td>\n",
       "      <td>&lt;facts&gt;\\nThe average temperature today is 20 d...</td>\n",
       "      <td>[The mean/average temperature today is 68 degr...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>case6</td>\n",
       "      <td>the average temperature today is 20 degrees ce...</td>\n",
       "      <td>the average temperature today is 50 degrees ce...</td>\n",
       "      <td>This case involves unit conversion and synonym...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;facts&gt;\\nThe average temperature today is 50 d...</td>\n",
       "      <td>&lt;facts&gt;\\nThe average temperature today is 20 d...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[The average temperature today is 20 degrees c...</td>\n",
       "      <td>[The average temperature today is 50 degrees c...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>case7</td>\n",
       "      <td>The company has an ATO now, so they have been ...</td>\n",
       "      <td>The company has been sanctioned by the governm...</td>\n",
       "      <td>This case uses \"sanctioned\" in a way that high...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>50.0</td>\n",
       "      <td>&lt;facts&gt;\\nThe company has been sanctioned by th...</td>\n",
       "      <td>&lt;facts&gt;\\nThe company has an ATO now.\\nThe comp...</td>\n",
       "      <td>[The company has been sanctioned by the govern...</td>\n",
       "      <td>[The company has an ATO now, You can work with...</td>\n",
       "      <td>[The sanction was in response to recent lawbre...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Case                                    Input Statement  \\\n",
       "0  case1  The Sun is a medium-sized star. It's about 4.6...   \n",
       "1  case2  The Sun, a medium-sized star, is located at th...   \n",
       "2  case3                             Sally is Rachel's cat.   \n",
       "3  case4                         Sally is larger than Stan.   \n",
       "4  case5  the average temperature today is 20 degrees ce...   \n",
       "5  case6  the average temperature today is 20 degrees ce...   \n",
       "6  case7  The company has an ATO now, so they have been ...   \n",
       "\n",
       "                                    Target Statement  \\\n",
       "0  The sun is approximately 4.6 billion years old...   \n",
       "1  The sun is a mid-sized star which has existed ...   \n",
       "2               Sally is a cat. Rachel is her owner.   \n",
       "3                        Stan is smaller than Sally.   \n",
       "4  the mean temperature today is 68 degrees fahre...   \n",
       "5  the average temperature today is 50 degrees ce...   \n",
       "6  The company has been sanctioned by the governm...   \n",
       "\n",
       "                                         Description  Groundedness (True)  \\\n",
       "0  This is a basic use case with pronouns and mil...                  100   \n",
       "1     This is a basic use case with mild rephrasing.                   67   \n",
       "2  This case involves simple restructuring and cl...                  100   \n",
       "3  This case demonstrates a change in comparative...                  100   \n",
       "4  This case involves unit conversion and synonym...                  100   \n",
       "5  This case involves unit conversion and synonym...                    0   \n",
       "6  This case uses \"sanctioned\" in a way that high...                    0   \n",
       "\n",
       "   Thoroughness (True)  Groundedness (Model)  Thoroughness (Model)  \\\n",
       "0                  100            100.000000                 100.0   \n",
       "1                  100             66.666667                 100.0   \n",
       "2                  100            100.000000                 100.0   \n",
       "3                  100            100.000000                 100.0   \n",
       "4                  100            100.000000                 100.0   \n",
       "5                    0              0.000000                   0.0   \n",
       "6                    0             33.333333                  50.0   \n",
       "\n",
       "                                        Context List  \\\n",
       "0  <facts>\\nThe sun is approximately 4.6 billion ...   \n",
       "1  <facts>\\nThe sun is a mid-sized star.\\nThe sun...   \n",
       "2  <facts>\\nSally is a cat.\\nRachel is the owner ...   \n",
       "3     <facts>\\nStan is smaller than Sally.\\n</facts>   \n",
       "4  <facts>\\nThe mean temperature today is 68 degr...   \n",
       "5  <facts>\\nThe average temperature today is 50 d...   \n",
       "6  <facts>\\nThe company has been sanctioned by th...   \n",
       "\n",
       "                                         Answer List  \\\n",
       "0  <facts>\\nThe Sun is a medium-sized star.\\nThe ...   \n",
       "1  <facts>\\nThe Sun is a medium-sized star.\\nThe ...   \n",
       "2          <facts>\\nSally is Rachel's cat.\\n</facts>   \n",
       "3      <facts>\\nSally is larger than Stan.\\n</facts>   \n",
       "4  <facts>\\nThe average temperature today is 20 d...   \n",
       "5  <facts>\\nThe average temperature today is 20 d...   \n",
       "6  <facts>\\nThe company has an ATO now.\\nThe comp...   \n",
       "\n",
       "                                       Facts in Both  \\\n",
       "0  [The sun is approximately 4.6 billion years ol...   \n",
       "1  [The sun is a mid-sized star., The sun has exi...   \n",
       "2   [Sally is a cat., Rachel is the owner of Sally.]   \n",
       "3  [Stan is smaller than Sally, Sally is larger t...   \n",
       "4  [The mean/average temperature today is 68 degr...   \n",
       "5                                                 []   \n",
       "6  [The company has been sanctioned by the govern...   \n",
       "\n",
       "                                Facts Only in Answer  \\\n",
       "0                                                 []   \n",
       "1  [The Sun is located at the center of our Solar...   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "5  [The average temperature today is 20 degrees c...   \n",
       "6  [The company has an ATO now, You can work with...   \n",
       "\n",
       "                               Facts Only in Context Model Error  \n",
       "0                                                 []        None  \n",
       "1                                                 []        None  \n",
       "2                                                 []        None  \n",
       "3                                                 []        None  \n",
       "4                                                 []        None  \n",
       "5  [The average temperature today is 50 degrees c...        None  \n",
       "6  [The sanction was in response to recent lawbre...        None  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f9b087-81ea-4c30-a7ba-9f414cd4349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your samples\n",
    "\n",
    "samples = [\n",
    "    Sample(\n",
    "        input=\"The Sun is a medium-sized star. It's about 4.6 billion years old.\",\n",
    "        target=\"The sun is approximately 4.6 billion years old. It's a mid-sized star.\",\n",
    "        true_metrics={'groundedness': 100, 'thoroughness': 100},\n",
    "        description=\"This is a basic use case with pronouns and mild rephrasing.\",\n",
    "        id=\"case1\"\n",
    "    ),\n",
    "    Sample(\n",
    "        input=\"The Sun, a medium-sized star, is located at the center of our Solar System and is approximately 4.6 billion years old.\",\n",
    "        target=\"The sun is a mid-sized star which has existed for about 4.6 billion years.\",\n",
    "        true_metrics={'groundedness': 67, 'thoroughness': 100},\n",
    "        description=\"This is a basic use case with mild rephrasing.\",\n",
    "        id=\"case2\"\n",
    "    ),\n",
    "    Sample(\n",
    "        input=\"Sally is Rachel's cat.\",\n",
    "        target=\"Sally is a cat. Rachel is her owner.\",\n",
    "        true_metrics={'groundedness': 100, 'thoroughness': 100},  \n",
    "        description=\"This case involves simple restructuring and clarification.\",\n",
    "        id=\"case3\"\n",
    "    ),\n",
    "    Sample(\n",
    "        input=\"Sally is larger than Stan.\",\n",
    "        target=\"Stan is smaller than Sally.\",\n",
    "        true_metrics={'groundedness': 100, 'thoroughness': 100}, \n",
    "        description=\"This case demonstrates a change in comparative perspective.\",\n",
    "        id=\"case4\"\n",
    "    ),\n",
    "    Sample(\n",
    "        input=\"the average temperature today is 20 degrees celsius.\",\n",
    "        target=\"the mean temperature today is 68 degrees fahrenheit.\",\n",
    "        true_metrics={'groundedness': 100, 'thoroughness': 100},  \n",
    "        description=\"This case involves unit conversion and synonym use.\",\n",
    "        id=\"case5\"\n",
    "    ),\n",
    "    Sample(\n",
    "        input=\"the average temperature today is 20 degrees celsius.\",\n",
    "        target=\"the average temperature today is 50 degrees celsius.\",\n",
    "        true_metrics={'groundedness': 0, 'thoroughness': 0},  \n",
    "        description=\"This case involves incorrect unit conversion and synonym use.\",\n",
    "        id=\"case6\"\n",
    "    ),\n",
    "    Sample(\n",
    "        input=\"The company has an ATO now, so they have been sanctioned by the government and you can work with them.\", \n",
    "        target=\"The company has been sanctioned by the government in response to recent lawbreaking activity.\",\n",
    "        true_metrics={'groundedness': 0, 'thoroughness': 0},  \n",
    "        description=\"This case uses 'sanctioned' in a way that highlights its dual meaning: approved or penalized.\",\n",
    "        id=\"case7\"\n",
    "    )\n",
    "    # Add more samples as needed\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2a8551d5-b7f2-4f40-8fa1-3f2d439a2d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"Please answer the question being asked.\"\n",
    "\n",
    "samples = [\n",
    "    Sample(\n",
    "        input=\"How old is the sun?\",\n",
    "        target=\"The sun is approximately 4.6 billion years old. It's a mid-sized star.\",\n",
    "        description=\"Very basic question.\",\n",
    "        id=\"case1\"\n",
    "    ),\n",
    "    Sample(\n",
    "        input=\"What is the capital of France?\",\n",
    "        target=\"The capital of France is Paris.\",\n",
    "        description=\"Basic geography question.\",\n",
    "        id=\"case2\"\n",
    "    ),\n",
    "    Sample(\n",
    "        input=\"Explain the theory of relativity.\",\n",
    "        target=\"The theory of relativity, developed by Albert Einstein, includes the special and general theories. Special relativity introduces a consistent explanation for the speed of light, and general relativity provides a description of gravity as a curvature of spacetime caused by mass.\",\n",
    "        description=\"Complex scientific concept.\",\n",
    "        id=\"case3\"\n",
    "    ),\n",
    "    Sample(\n",
    "        input=\"What is photosynthesis?\",\n",
    "        target=\"Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize nutrients from carbon dioxide and water. It typically involves the green pigment chlorophyll and generates oxygen as a byproduct.\",\n",
    "        description=\"Basic biological process.\",\n",
    "        id=\"case4\"\n",
    "    ),\n",
    "    Sample(\n",
    "        input=\"Who wrote 'Pride and Prejudice'?\",\n",
    "        target=\"'Pride and Prejudice' was written by Jane Austen.\",\n",
    "        description=\"Question about literature.\",\n",
    "        id=\"case5\"\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "96965d12-1461-407f-8fe7-96f08bb9812c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671b5c174b18420cadf41fc46e07de1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">{'_model': &lt;inspect_ai.model._model.ModelName object at 0x0000020DF0275010&gt;, 'sample_id': 'case1', 'epoch': 1, \n",
       "'_input': 'How old is the sun?', 'choices': None, 'messages': [ChatMessageSystem(content='You are an AI assistant \n",
       "helping with fact comparison.', source=None, role='system', tool=None), ChatMessageUser(content='How old is the \n",
       "sun?', source='input', role='user'), ChatMessageAssistant(content=\"The Sun is approximately 4.6 billion years old. \n",
       "This estimation is based on the ages of the oldest meteorites and the Earth's formation, inferred through \n",
       "radiometric age dating and other scientific analyses.\", source='generate', role='assistant', tool_calls=None)], \n",
       "'tools': [], 'tool_choice': None, 'output': ModelOutput(model='gpt-4o-2024-05-13', \n",
       "choices=[ChatCompletionChoice(message=ChatMessageAssistant(content=\"The Sun is approximately 4.6 billion years old.\n",
       "This estimation is based on the ages of the oldest meteorites and the Earth's formation, inferred through \n",
       "radiometric age dating and other scientific analyses.\", source='generate', role='assistant', tool_calls=None), \n",
       "stop_reason='stop', logprobs=None)], usage=ModelUsage(input_tokens=27, output_tokens=40, total_tokens=67), \n",
       "error=None), 'completed': True, 'metadata': {}}\n",
       "</pre>\n"
      ],
      "text/plain": [
       "{'_model': <inspect_ai.model._model.ModelName object at 0x0000020DF0275010>, 'sample_id': 'case1', 'epoch': 1, \n",
       "'_input': 'How old is the sun?', 'choices': None, 'messages': [ChatMessageSystem(content='You are an AI assistant \n",
       "helping with fact comparison.', source=None, role='system', tool=None), ChatMessageUser(content='How old is the \n",
       "sun?', source='input', role='user'), ChatMessageAssistant(content=\"The Sun is approximately 4.6 billion years old. \n",
       "This estimation is based on the ages of the oldest meteorites and the Earth's formation, inferred through \n",
       "radiometric age dating and other scientific analyses.\", source='generate', role='assistant', tool_calls=None)], \n",
       "'tools': [], 'tool_choice': None, 'output': ModelOutput(model='gpt-4o-2024-05-13', \n",
       "choices=[ChatCompletionChoice(message=ChatMessageAssistant(content=\"The Sun is approximately 4.6 billion years old.\n",
       "This estimation is based on the ages of the oldest meteorites and the Earth's formation, inferred through \n",
       "radiometric age dating and other scientific analyses.\", source='generate', role='assistant', tool_calls=None), \n",
       "stop_reason='stop', logprobs=None)], usage=ModelUsage(input_tokens=27, output_tokens=40, total_tokens=67), \n",
       "error=None), 'completed': True, 'metadata': {}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">value={'groundedness': 50.0, 'thoroughness': 20.0} answer=None explanation='{\\'context_list\\': \"&lt;facts&gt;\\\\nThe Sun \n",
       "is approximately 4.6 billion years old.\\\\nThis estimation is based on the ages of the oldest meteorites.\\\\nThis \n",
       "estimation is based on the Earth\\'s formation.\\\\nThe estimation is inferred through radiometric age dating.\\\\nThe \n",
       "estimation is also inferred through other scientific analyses.\\\\n&lt;/facts&gt;\", \\'answer_list\\': \\'&lt;facts&gt;\\\\nThe sun is\n",
       "approximately 4.6 billion years old.\\\\nThe sun is a mid-sized star.\\\\n&lt;/facts&gt;\\', \\'comparison_result\\': \n",
       "ComparisonResult(facts_in_both=[\\'The Sun is approximately 4.6 billion years old.\\'], facts_only_in_answer=[\\'The \n",
       "sun is a mid-sized star.\\'], facts_only_in_context=[\\'This estimation is based on the ages of the oldest \n",
       "meteorites.\\', \"This estimation is based on the Earth\\'s formation.\", \\'The estimation is inferred through \n",
       "radiometric age dating.\\', \\'The estimation is also inferred through other scientific analyses.\\'])}\\nModel Output:\n",
       "The Sun is approximately 4.6 billion years old. This estimation is based on the ages of the oldest meteorites and \n",
       "the Earth\\'s formation, inferred through radiometric age dating and other scientific analyses.' metadata=None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "value={'groundedness': 50.0, 'thoroughness': 20.0} answer=None explanation='{\\'context_list\\': \"<facts>\\\\nThe Sun \n",
       "is approximately 4.6 billion years old.\\\\nThis estimation is based on the ages of the oldest meteorites.\\\\nThis \n",
       "estimation is based on the Earth\\'s formation.\\\\nThe estimation is inferred through radiometric age dating.\\\\nThe \n",
       "estimation is also inferred through other scientific analyses.\\\\n</facts>\", \\'answer_list\\': \\'<facts>\\\\nThe sun is\n",
       "approximately 4.6 billion years old.\\\\nThe sun is a mid-sized star.\\\\n</facts>\\', \\'comparison_result\\': \n",
       "ComparisonResult(facts_in_both=[\\'The Sun is approximately 4.6 billion years old.\\'], facts_only_in_answer=[\\'The \n",
       "sun is a mid-sized star.\\'], facts_only_in_context=[\\'This estimation is based on the ages of the oldest \n",
       "meteorites.\\', \"This estimation is based on the Earth\\'s formation.\", \\'The estimation is inferred through \n",
       "radiometric age dating.\\', \\'The estimation is also inferred through other scientific analyses.\\'])}\\nModel Output:\n",
       "The Sun is approximately 4.6 billion years old. This estimation is based on the ages of the oldest meteorites and \n",
       "the Earth\\'s formation, inferred through radiometric age dating and other scientific analyses.' metadata=None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class FactComparatorScorer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.fact_comparator = FactComparator(model)\n",
    "\n",
    "    async def __call__(self, state: TaskState, target: Sample):\n",
    "        try: \n",
    "            context = state.output.choices[0].message.content\n",
    "        except: \n",
    "            context = state.input\n",
    "        target_text = target.target\n",
    "\n",
    "        result = await self.fact_comparator.process_data(context, target_text)\n",
    "        metrics = self.fact_comparator.calculate_metrics(result[\"comparison_result\"])\n",
    "\n",
    "        scorer_value = {\n",
    "            \"groundedness\": metrics[\"groundedness\"],\n",
    "            \"thoroughness\": metrics[\"thoroughness\"],\n",
    "        }\n",
    "\n",
    "        explanation = str(result) + f\"\\nModel Output: {context}\"\n",
    "\n",
    "        return Score(\n",
    "            value=scorer_value,\n",
    "            explanation=explanation,\n",
    "        )\n",
    "        \n",
    "@metric\n",
    "def thoroughness():\n",
    "  def metric(scores: list[Score]) -> float:\n",
    "    total = 0.0\n",
    "    for item in scores:\n",
    "      metadata = item.metadata\n",
    "      if metadata is not None:\n",
    "          total += float(metadata[\"thoroughness\"])\n",
    "    return total / float(len(scores))\n",
    "  return metric\n",
    "\n",
    "@metric\n",
    "def groundedness():\n",
    "  def metric(scores: list[Score]) -> float:\n",
    "    total = 0.0\n",
    "    for item in scores:\n",
    "        metadata = item.metadata\n",
    "        if metadata is not None:\n",
    "            total += float(metadata[\"groundedness\"])\n",
    "    return total / float(len(scores))\n",
    "  return metric\n",
    "\n",
    "    \n",
    "@scorer(metrics=[groundedness(), thoroughness()])\n",
    "def fact_comparator_scorer(model) -> Scorer:\n",
    "  \n",
    "  async def score(state: TaskState, target: Target) -> Score:\n",
    "\n",
    "    # Create an instance of the scorer\n",
    "    model = InspectChatModel()\n",
    "    fact_comparator_scorer = FactComparatorScorer(model)\n",
    "\n",
    "    # Call the scorer\n",
    "    score = await fact_comparator_scorer(state, target)\n",
    "    print(score)\n",
    "\n",
    "    # Ignore the actual processing and return a dummy value\n",
    "    grounded_score = score.value['groundedness']\n",
    "    thorough_score = score.value['thoroughness']\n",
    "    explanation = score.explanation\n",
    "\n",
    "    answer = state.output.completion\n",
    "\n",
    "    return Score(\n",
    "        value=f\"G:{grounded_score} : T:{thorough_score}\", # make a better string?\n",
    "        answer=answer,\n",
    "        explanation= \"nothing\",\n",
    "        metadata = {\n",
    "           \"thoroughness\": thorough_score,\n",
    "           \"groundedness\": grounded_score,\n",
    "            \"stuff\": explanation\n",
    "        }\n",
    "    )\n",
    "\n",
    "  return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64390128-ba9e-477e-91c0-1d4c0a5d4c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def my_eval():\n",
    "    return Task(\n",
    "        dataset=samples,\n",
    "        plan=[\n",
    "            system_message(SYSTEM_MESSAGE),\n",
    "            generate()\n",
    "        ],\n",
    "        scorer=fact_comparator_scorer(model=get_model()),\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    eval(my_eval(), model=\"openai/gpt-4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Virtual Env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
