{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a851d3d-cb9b-448c-ae3b-21c524601308",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5119e1e4-68fd-4176-9b72-ff30c8f3c38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (0.1.19)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.6.6)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.0.38)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.1.52)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.1.56)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain) (2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (0.1.6)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.46 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-openai) (0.1.52)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.24.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-openai) (1.28.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-openai) (0.5.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (0.1.56)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (2.6.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (8.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (0.26.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain-openai) (3.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.46->langchain-openai) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langchain-openai) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langchain-openai) (2.16.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.24.0->langchain-openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain\n",
    "%pip install langchain-openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35219496-2769-4767-831c-f85ac271b390",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d108b5b-d0ad-4b5f-8b2d-308364e05dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "import pandas as pd \n",
    "\n",
    "class FactComparator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.pronoun_chain = LLMChain(llm=self.model, prompt=self._pronoun_prompt())\n",
    "        self.parse_chain = LLMChain(llm=self.model, prompt=self._parse_prompt())\n",
    "        self.compare_chain = LLMChain(llm=self.model, prompt=self._compare_prompt())\n",
    "        self.parser = PydanticOutputParser(pydantic_object=ComparisonResult)\n",
    "\n",
    "    def process_data(self, context, answer):\n",
    "        context_replace_pronouns = self.pronoun_chain.run(text=context)\n",
    "        answer_replace_pronouns = self.pronoun_chain.run(text=answer)\n",
    "\n",
    "        context_list = self.parse_chain.run(text=context_replace_pronouns)\n",
    "        answer_list = self.parse_chain.run(text=answer_replace_pronouns)\n",
    "\n",
    "        comparison_result = self.parser.parse(self.compare_chain.run(context_list=context_list, answer_list=answer_list))\n",
    "\n",
    "        return {\n",
    "            \"context_replace_pronouns\": context_replace_pronouns,\n",
    "            \"answer_replace_pronouns\": answer_replace_pronouns,\n",
    "            \"context_list\": context_list,\n",
    "            \"answer_list\": answer_list,\n",
    "            \"comparison_result\": comparison_result,\n",
    "        }\n",
    "\n",
    "    def calculate_metrics(self, comparison_result):\n",
    "        facts_in_both_count = len(comparison_result.facts_in_both)\n",
    "        facts_only_in_answer_count = len(comparison_result.facts_only_in_answer)\n",
    "        facts_only_in_context_count = len(comparison_result.facts_only_in_context)\n",
    "\n",
    "        total_answer_facts = facts_in_both_count + facts_only_in_answer_count\n",
    "        total_context_facts = facts_in_both_count + facts_only_in_context_count\n",
    "\n",
    "        groundedness = facts_in_both_count / total_answer_facts * 100 if total_answer_facts > 0 else 0\n",
    "        thoroughness = facts_in_both_count / total_context_facts * 100 if total_context_facts > 0 else 0\n",
    "\n",
    "        return {\n",
    "            \"groundedness\": groundedness,\n",
    "            \"thoroughness\": thoroughness,\n",
    "        }\n",
    "\n",
    "    def process_data_list(self, data_list):\n",
    "        results = []\n",
    "        for data in data_list:\n",
    "            context = data['context']\n",
    "            answer = data['answer']\n",
    "\n",
    "            result = self.process_data(context, answer)\n",
    "            metrics = self.calculate_metrics(result[\"comparison_result\"])\n",
    "\n",
    "            result_data = {\n",
    "                'context': context,\n",
    "                'answer': answer,\n",
    "                'context_replace_pronouns': result[\"context_replace_pronouns\"],\n",
    "                'answer_replace_pronouns': result[\"answer_replace_pronouns\"],\n",
    "                'context_list': result[\"context_list\"],\n",
    "                'answer_list': result[\"answer_list\"],\n",
    "                'facts_in_both': ', '.join(result[\"comparison_result\"].facts_in_both),\n",
    "                'facts_only_in_answer': ', '.join(result[\"comparison_result\"].facts_only_in_answer),\n",
    "                'facts_only_in_context': ', '.join(result[\"comparison_result\"].facts_only_in_context),\n",
    "                'groundedness': metrics['groundedness'],\n",
    "                'thoroughness': metrics['thoroughness']\n",
    "            }\n",
    "            results.append(result_data)\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    @staticmethod\n",
    "    def _pronoun_prompt():\n",
    "        return PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=\"\"\"\n",
    "            Your task is to replace all the pronouns in the following text with the nouns they refer to:\n",
    "\n",
    "            <text>\n",
    "            {text}\n",
    "            </text>\n",
    "\n",
    "            The goal is to make the text more explicit and clear by replacing potentially ambiguous pronouns like \"he\", \"she\", \"it\", \"they\", \"them\", etc. with the specific nouns or names they refer to.\n",
    "\n",
    "            For example:\n",
    "            Original: John went to the store. He bought some milk.\n",
    "            Pronoun replaced: John went to the store. John bought some milk.\n",
    "\n",
    "            Here are the steps to complete this task:\n",
    "\n",
    "            1. Carefully read the provided text and identify all the pronouns \n",
    "            2. For each pronoun, look back in the text to determine which noun or name it is referring to\n",
    "            3. If the pronoun is part of a direct quote, do not replace it\n",
    "            4. Replace each pronoun with the most recent noun or name it refers to\n",
    "            5. If a pronoun does not have a clear referent noun or name, do not replace it\n",
    "            6. Repeat this process until all the pronouns with clear referents have been replaced\n",
    "            \"\"\",\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_prompt():\n",
    "        return PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=\"\"\"\n",
    "            Please parse the following text into a list of individual facts:\n",
    "\n",
    "            <text>\n",
    "            {text}\n",
    "            </text>\n",
    "\n",
    "            Read the text carefully. Your task is to break it down into the key facts it contains. Parse out each individual fact into a separate sentence, even if that means splitting up or rewording the original sentences. The goal is to have a clear, concise list of the core facts contained in the text.\n",
    "\n",
    "            Output the parsed facts in a numbered list, with each fact written as a complete sentence on its own line. Use <facts> tags to demarcate the start and end of the list.\n",
    "            \"\"\",\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _compare_prompt():\n",
    "        return PromptTemplate(\n",
    "            input_variables=[\"context_list\", \"answer_list\"],\n",
    "            template=\"\"\"\n",
    "            You will be comparing facts between a context and an answer to determine which facts are shared and which are unique to each.\n",
    "\n",
    "            Here is the context:\n",
    "\n",
    "            <context>\n",
    "            {context_list}\n",
    "            </context>\n",
    "\n",
    "            And here is the answer: \n",
    "\n",
    "            <answer>\n",
    "            {answer_list}\n",
    "            </answer>\n",
    "\n",
    "            Carefully analyze the facts presented in the context and answer, focusing on the semantic meaning rather than the exact wording.\n",
    "\n",
    "            Then, output a dictionary with the following keys and corresponding lists of facts as values:\n",
    "\n",
    "            1. \"facts_in_both\": A list of facts that are present in both the context and the answer\n",
    "\n",
    "            2. \"facts_only_in_answer\": A list of facts that are only present in the answer \n",
    "\n",
    "            3. \"facts_only_in_context\": A list of facts that are only present in the context\n",
    "\n",
    "            Remember, the facts do not need to be worded identically to be considered the same. Focus on whether the core meaning is shared or unique.\n",
    "\n",
    "            Provide your results in this format:\n",
    "\n",
    "            {{\n",
    "                \"facts_in_both\": [\n",
    "                    \"Fact 1 present in both\",\n",
    "                    \"Fact 2 present in both\"\n",
    "                ],\n",
    "                \"facts_only_in_answer\": [\n",
    "                    \"Fact 1 only in answer\",\n",
    "                    \"Fact 2 only in answer\"  \n",
    "                ],\n",
    "                \"facts_only_in_context\": [\n",
    "                    \"Fact 1 only in context\",\n",
    "                    \"Fact 2 only in context\"\n",
    "                ]\n",
    "            }}\n",
    "            \"\"\",\n",
    "        )\n",
    "\n",
    "\n",
    "class ComparisonResult(BaseModel):\n",
    "    facts_in_both: list[str] = Field(default_factory=list, description=\"List of facts present in both context and answer\")\n",
    "    facts_only_in_answer: list[str] = Field(default_factory=list, description=\"List of facts only present in the answer\")\n",
    "    facts_only_in_context: list[str] = Field(default_factory=list, description=\"List of facts only present in the context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa72dbc-fd5a-462c-8308-9238c5cef0ad",
   "metadata": {},
   "source": [
    "## Run on First Pair of Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2751f4e-3992-496c-aad0-c0604a712257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context with replaced pronouns:\n",
      "\n",
      "            The quick brown fox jumps over the rock because the fox is happy. The fox was born in 2005. The hedgehog was born in 2010, but the hedgehog is even happier than the fox.\n",
      "\n",
      "Answer with replaced pronouns:\n",
      "\n",
      "            The quick brown fox was born in 2005, and the hedgehog in 2010. The quick brown fox is not as happy as the hedgehog.\n",
      "\n",
      "Context list:\n",
      "\n",
      "<facts>\n",
      "1. The quick brown fox jumps over the rock because the fox is happy.\n",
      "2. The fox was born in 2005.\n",
      "3. The hedgehog was born in 2010.\n",
      "4. The hedgehog is even happier than the fox.\n",
      "</facts>\n",
      "\n",
      "Answer list:\n",
      "\n",
      "<facts>\n",
      "1. The quick brown fox was born in 2005.\n",
      "2. The hedgehog was born in 2010.\n",
      "3. The quick brown fox is not as happy as the hedgehog.\n",
      "</facts>\n",
      "\n",
      "Comparison result:\n",
      "facts_in_both=['The fox was born in 2005.', 'The hedgehog was born in 2010.'] facts_only_in_answer=['The quick brown fox is not as happy as the hedgehog.'] facts_only_in_context=['The quick brown fox jumps over the rock because the fox is happy.', 'The hedgehog is even happier than the fox.']\n",
      "\n",
      "Metrics:\n",
      "Groundedness: 66.67%\n",
      "Thoroughness: 50.00%\n"
     ]
    }
   ],
   "source": [
    "model = OpenAI(temperature=0)\n",
    "comparator = FactComparator(model)\n",
    "\n",
    "context = \"The quick brown fox jumps over the rock because he's happy. He was born in 2005. The hedgehog was born in 2010, but she's even happier than him.\"\n",
    "answer = \"The quick brown fox was born in 2005, and the hedgehog in 2010. The quick brown fox is not as happy as the hedgehog\"\n",
    "\n",
    "result = comparator.process_data(context, answer)\n",
    "metrics = comparator.calculate_metrics(result[\"comparison_result\"])\n",
    "\n",
    "print(\"Context with replaced pronouns:\")\n",
    "print(result[\"context_replace_pronouns\"])\n",
    "\n",
    "print(\"\\nAnswer with replaced pronouns:\")\n",
    "print(result[\"answer_replace_pronouns\"])\n",
    "\n",
    "print(\"\\nContext list:\")\n",
    "print(result[\"context_list\"])\n",
    "\n",
    "print(\"\\nAnswer list:\")\n",
    "print(result[\"answer_list\"])\n",
    "\n",
    "print(\"\\nComparison result:\")\n",
    "print(result[\"comparison_result\"])\n",
    "\n",
    "print(\"\\nMetrics:\")\n",
    "print(f\"Groundedness: {metrics['groundedness']:.2f}%\")\n",
    "print(f\"Thoroughness: {metrics['thoroughness']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11437091-c8a1-40fe-b55b-d4f06c0941c3",
   "metadata": {},
   "source": [
    "## Run on another pair of statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad11011d-dd24-4c8f-b4e9-e8573b0c7ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context with replaced pronouns:\n",
      "\n",
      "            To boil pasta, first bring a large pot of salted water to a rolling boil over high heat.\n",
      "\n",
      "Answer with replaced pronouns:\n",
      "\n",
      "To boil pasta, begin by filling a large pot with water, making sure there's enough water to fully submerge the pasta. Bring the water to a rolling boil over high heat, then add salt to enhance the pasta's flavor. Once the water is boiling, carefully add the pasta, stirring gently to prevent the pasta from sticking. Cook the pasta according to the package instructions or until the pasta reaches your desired level of tenderness, usually around 8-12 minutes. To check for doneness, taste a piece of pasta—it should be tender but still slightly firm (al dente).\n",
      "\n",
      "Context list:\n",
      "\n",
      "<facts>\n",
      "1. To boil pasta, you must first bring a large pot of salted water to a rolling boil.\n",
      "2. This should be done over high heat.\n",
      "</facts>\n",
      "\n",
      "Answer list:\n",
      "\n",
      "<facts>\n",
      "1. To boil pasta, begin by filling a large pot with water.\n",
      "2. Make sure there's enough water to fully submerge the pasta.\n",
      "3. Bring the water to a rolling boil over high heat.\n",
      "4. Add salt to enhance the pasta's flavor.\n",
      "5. Once the water is boiling, carefully add the pasta.\n",
      "6. Stir gently to prevent the pasta from sticking.\n",
      "7. Cook the pasta according to the package instructions.\n",
      "8. Or until the pasta reaches your desired level of tenderness.\n",
      "9. Usually around 8-12 minutes.\n",
      "10. To check for doneness, taste a piece of pasta.\n",
      "11. It should be tender but still slightly firm (al dente).\n",
      "</facts>\n",
      "\n",
      "Comparison result:\n",
      "facts_in_both=['To boil pasta, you must first bring a large pot of salted water to a rolling boil.', 'This should be done over high heat.'] facts_only_in_answer=['To boil pasta, begin by filling a large pot with water.', \"Make sure there's enough water to fully submerge the pasta.\", \"Add salt to enhance the pasta's flavor.\", 'Once the water is boiling, carefully add the pasta.', 'Stir gently to prevent the pasta from sticking.', 'Cook the pasta according to the package instructions.', 'Or until the pasta reaches your desired level of tenderness.', 'Usually around 8-12 minutes.', 'To check for doneness, taste a piece of pasta.', 'It should be tender but still slightly firm (al dente).'] facts_only_in_context=[]\n",
      "\n",
      "Metrics:\n",
      "Groundedness: 16.67%\n",
      "Thoroughness: 100.00%\n"
     ]
    }
   ],
   "source": [
    "model = OpenAI(temperature=0)\n",
    "comparator = FactComparator(model)\n",
    "\n",
    "context = \"To boil pasta, first bring a large pot of salted water to a rolling boil over high heat..\"\n",
    "answer = \"To boil pasta, begin by filling a large pot with water, making sure there's enough to fully submerge the pasta. Bring the water to a rolling boil over high heat, then add salt to enhance the pasta's flavor. Once the water is boiling, carefully add the pasta, stirring gently to prevent sticking. Cook the pasta according to the package instructions or until it reaches your desired level of tenderness, usually around 8-12 minutes. To check for doneness, taste a piece of pasta—it should be tender but still slightly firm (al dente).\"\n",
    "\n",
    "result = comparator.process_data(context, answer)\n",
    "metrics = comparator.calculate_metrics(result[\"comparison_result\"])\n",
    "\n",
    "print(\"Context with replaced pronouns:\")\n",
    "print(result[\"context_replace_pronouns\"])\n",
    "\n",
    "print(\"\\nAnswer with replaced pronouns:\")\n",
    "print(result[\"answer_replace_pronouns\"])\n",
    "\n",
    "print(\"\\nContext list:\")\n",
    "print(result[\"context_list\"])\n",
    "\n",
    "print(\"\\nAnswer list:\")\n",
    "print(result[\"answer_list\"])\n",
    "\n",
    "print(\"\\nComparison result:\")\n",
    "print(result[\"comparison_result\"])\n",
    "\n",
    "print(\"\\nMetrics:\")\n",
    "print(f\"Groundedness: {metrics['groundedness']:.2f}%\")\n",
    "print(f\"Thoroughness: {metrics['thoroughness']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaf37e7-66f4-422b-926d-cc1b93109a89",
   "metadata": {},
   "source": [
    "## Run on a list of dictionaries - return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9514e054-f73a-49e8-b893-61981fe6d802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>context_replace_pronouns</th>\n",
       "      <th>answer_replace_pronouns</th>\n",
       "      <th>context_list</th>\n",
       "      <th>answer_list</th>\n",
       "      <th>facts_in_both</th>\n",
       "      <th>facts_only_in_answer</th>\n",
       "      <th>facts_only_in_context</th>\n",
       "      <th>groundedness</th>\n",
       "      <th>thoroughness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The quick brown fox jumps over the rock becaus...</td>\n",
       "      <td>The quick brown fox was born in 2005, and the ...</td>\n",
       "      <td>\\n            The quick brown fox jumps over t...</td>\n",
       "      <td>\\n            The quick brown fox was born in ...</td>\n",
       "      <td>\\n&lt;facts&gt;\\n1. The quick brown fox jumps over t...</td>\n",
       "      <td>\\n&lt;facts&gt;\\n1. The quick brown fox was born in ...</td>\n",
       "      <td>The fox was born in 2005., The hedgehog was bo...</td>\n",
       "      <td>The quick brown fox is not as happy as the hed...</td>\n",
       "      <td>The quick brown fox jumps over the rock becaus...</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The sun is a star at the center of our solar s...</td>\n",
       "      <td>The sun is a star located approximately 93 mil...</td>\n",
       "      <td>\\n            The sun is a star at the center ...</td>\n",
       "      <td>\\n            The sun is a star located approx...</td>\n",
       "      <td>\\n&lt;facts&gt;\\n1. The sun is a star.\\n2. The sun i...</td>\n",
       "      <td>\\n&lt;facts&gt;\\n1. The sun is a star.\\n2. The sun i...</td>\n",
       "      <td>The sun is a star., The sun is a hot ball of g...</td>\n",
       "      <td>The sun is located approximately 93 million mi...</td>\n",
       "      <td>The sun is at the center of our solar system.,...</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Birds are warm-blooded vertebrates that lay eg...</td>\n",
       "      <td>Birds are a diverse group of animals with feat...</td>\n",
       "      <td>\\n            Birds are warm-blooded vertebrat...</td>\n",
       "      <td>\\n            Birds are a diverse group of ani...</td>\n",
       "      <td>\\n&lt;facts&gt;\\n1. Birds are warm-blooded vertebrat...</td>\n",
       "      <td>\\n&lt;facts&gt;\\n1. Birds are a diverse group of ani...</td>\n",
       "      <td>Birds are warm-blooded vertebrates., Birds lay...</td>\n",
       "      <td>Birds are a diverse group of animals with feat...</td>\n",
       "      <td>Some common bird species include sparrows., So...</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>66.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Eiffel Tower is a wrought-iron lattice tow...</td>\n",
       "      <td>The Eiffel Tower, found in Paris, France, is a...</td>\n",
       "      <td>\\n            The Eiffel Tower is a wrought-ir...</td>\n",
       "      <td>\\n            The Eiffel Tower, found in Paris...</td>\n",
       "      <td>\\n&lt;facts&gt;\\n1. The Eiffel Tower is a wrought-ir...</td>\n",
       "      <td>\\n&lt;facts&gt;\\n1. The Eiffel Tower is located in P...</td>\n",
       "      <td>The Eiffel Tower is a wrought-iron lattice tow...</td>\n",
       "      <td>The Eiffel Tower is located in Paris, France.,...</td>\n",
       "      <td></td>\n",
       "      <td>50.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Great Wall of China is a series of fortifi...</td>\n",
       "      <td>The Great Wall of China, a series of walls and...</td>\n",
       "      <td>\\n            The Great Wall of China is a ser...</td>\n",
       "      <td>\\n            The Great Wall of China, a serie...</td>\n",
       "      <td>\\n&lt;facts&gt;\\n1. The Great Wall of China is a ser...</td>\n",
       "      <td>\\n&lt;facts&gt;\\n1. The Great Wall of China is a ser...</td>\n",
       "      <td>The Great Wall of China is a series of walls a...</td>\n",
       "      <td>The Ming dynasty is responsible for the constr...</td>\n",
       "      <td>The most well-known sections were built during...</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  The quick brown fox jumps over the rock becaus...   \n",
       "1  The sun is a star at the center of our solar s...   \n",
       "2  Birds are warm-blooded vertebrates that lay eg...   \n",
       "3  The Eiffel Tower is a wrought-iron lattice tow...   \n",
       "4  The Great Wall of China is a series of fortifi...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  The quick brown fox was born in 2005, and the ...   \n",
       "1  The sun is a star located approximately 93 mil...   \n",
       "2  Birds are a diverse group of animals with feat...   \n",
       "3  The Eiffel Tower, found in Paris, France, is a...   \n",
       "4  The Great Wall of China, a series of walls and...   \n",
       "\n",
       "                            context_replace_pronouns  \\\n",
       "0  \\n            The quick brown fox jumps over t...   \n",
       "1  \\n            The sun is a star at the center ...   \n",
       "2  \\n            Birds are warm-blooded vertebrat...   \n",
       "3  \\n            The Eiffel Tower is a wrought-ir...   \n",
       "4  \\n            The Great Wall of China is a ser...   \n",
       "\n",
       "                             answer_replace_pronouns  \\\n",
       "0  \\n            The quick brown fox was born in ...   \n",
       "1  \\n            The sun is a star located approx...   \n",
       "2  \\n            Birds are a diverse group of ani...   \n",
       "3  \\n            The Eiffel Tower, found in Paris...   \n",
       "4  \\n            The Great Wall of China, a serie...   \n",
       "\n",
       "                                        context_list  \\\n",
       "0  \\n<facts>\\n1. The quick brown fox jumps over t...   \n",
       "1  \\n<facts>\\n1. The sun is a star.\\n2. The sun i...   \n",
       "2  \\n<facts>\\n1. Birds are warm-blooded vertebrat...   \n",
       "3  \\n<facts>\\n1. The Eiffel Tower is a wrought-ir...   \n",
       "4  \\n<facts>\\n1. The Great Wall of China is a ser...   \n",
       "\n",
       "                                         answer_list  \\\n",
       "0  \\n<facts>\\n1. The quick brown fox was born in ...   \n",
       "1  \\n<facts>\\n1. The sun is a star.\\n2. The sun i...   \n",
       "2  \\n<facts>\\n1. Birds are a diverse group of ani...   \n",
       "3  \\n<facts>\\n1. The Eiffel Tower is located in P...   \n",
       "4  \\n<facts>\\n1. The Great Wall of China is a ser...   \n",
       "\n",
       "                                       facts_in_both  \\\n",
       "0  The fox was born in 2005., The hedgehog was bo...   \n",
       "1  The sun is a star., The sun is a hot ball of g...   \n",
       "2  Birds are warm-blooded vertebrates., Birds lay...   \n",
       "3  The Eiffel Tower is a wrought-iron lattice tow...   \n",
       "4  The Great Wall of China is a series of walls a...   \n",
       "\n",
       "                                facts_only_in_answer  \\\n",
       "0  The quick brown fox is not as happy as the hed...   \n",
       "1  The sun is located approximately 93 million mi...   \n",
       "2  Birds are a diverse group of animals with feat...   \n",
       "3  The Eiffel Tower is located in Paris, France.,...   \n",
       "4  The Ming dynasty is responsible for the constr...   \n",
       "\n",
       "                               facts_only_in_context  groundedness  \\\n",
       "0  The quick brown fox jumps over the rock becaus...     66.666667   \n",
       "1  The sun is at the center of our solar system.,...     25.000000   \n",
       "2  Some common bird species include sparrows., So...     60.000000   \n",
       "3                                                        50.000000   \n",
       "4  The most well-known sections were built during...     75.000000   \n",
       "\n",
       "   thoroughness  \n",
       "0     50.000000  \n",
       "1     40.000000  \n",
       "2     66.666667  \n",
       "3    100.000000  \n",
       "4     75.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list = [\n",
    "    {\n",
    "        'context': 'The quick brown fox jumps over the rock because he\\'s happy. He was born in 2005. The hedgehog was born in 2010, but she\\'s even happier than him.',\n",
    "        'answer': 'The quick brown fox was born in 2005, and the hedgehog in 2010. The quick brown fox is not as happy as the hedgehog'\n",
    "    },\n",
    "    {\n",
    "        'context': 'The sun is a star at the center of our solar system. It is about 93 million miles away from Earth. The sun is a hot ball of glowing gases that provides light and warmth to Earth.',\n",
    "        'answer': 'The sun is a star located approximately 93 million miles from Earth. It is the source of light and heat for our planet. The sun is not a solid object, but rather a sphere of hot glowing gases.'\n",
    "    },\n",
    "    {\n",
    "        'context': 'Birds are warm-blooded vertebrates that lay eggs and have feathers, wings, and beaks. There are over 10,000 species of birds worldwide. Some common bird species include sparrows, pigeons, and parrots.',\n",
    "        'answer': 'Birds are a diverse group of animals with feathers and wings. They are warm-blooded egg-laying vertebrates. The number of bird species globally exceeds 10,000. Pigeons, parrots, and sparrows are among the most familiar bird types.'\n",
    "    },\n",
    "    {\n",
    "        'context': 'The Eiffel Tower is a wrought-iron lattice tower located on the Champ de Mars in Paris, France. It was constructed from 1887 to 1889 and stands at a height of 324 meters. The tower is named after Gustave Eiffel, whose company designed and built it.',\n",
    "        'answer': 'The Eiffel Tower, found in Paris, France, is a lattice tower made of wrought iron. Built between 1887 and 1889, it reaches a height of 324 meters. Gustave Eiffel\\'s company was responsible for the tower\\'s design and construction, hence its name.'\n",
    "    },\n",
    "    {\n",
    "        'context': 'The Great Wall of China is a series of fortifications and walls built across the historical northern borders of ancient Chinese states and Imperial China. The most well-known sections were built during the Ming dynasty, which ruled from 1368 to 1644.',\n",
    "        'answer': 'The Great Wall of China, a series of walls and fortifications, was constructed along the northern borders of ancient Chinese states and Imperial China. The Ming dynasty, which lasted from 1368 to 1644, is responsible for the construction of the most famous sections of the wall.'\n",
    "    }\n",
    "]\n",
    "\n",
    "model = OpenAI(temperature=0)\n",
    "comparator = FactComparator(model)\n",
    "\n",
    "df = comparator.process_data_list(data_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f949fc9f-9077-4bf3-acba-7131e65fc72e",
   "metadata": {},
   "source": [
    "## Inspect_AI Grader (not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ed704d9-2595-45eb-b1ea-120de261bc60",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot instantiate typing.Union",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 63\u001b[0m\n\u001b[0;32m     52\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m eval_data:\n\u001b[0;32m     54\u001b[0m     state \u001b[38;5;241m=\u001b[39m TaskState(\n\u001b[0;32m     55\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[0;32m     56\u001b[0m         sample_id\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     57\u001b[0m         epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     59\u001b[0m         choices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     60\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[ChatMessageUser(content\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m])],\n\u001b[0;32m     61\u001b[0m         output\u001b[38;5;241m=\u001b[39mModelOutput(\n\u001b[0;32m     62\u001b[0m             model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(model\u001b[38;5;241m.\u001b[39mmodel_name),\n\u001b[1;32m---> 63\u001b[0m             choices\u001b[38;5;241m=\u001b[39m[ChatCompletionChoice(message\u001b[38;5;241m=\u001b[39m\u001b[43mChatMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)],\n\u001b[0;32m     64\u001b[0m         ),\n\u001b[0;32m     65\u001b[0m         completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     66\u001b[0m     )\n\u001b[0;32m     67\u001b[0m     target \u001b[38;5;241m=\u001b[39m Target(text\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     68\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m fact_comparator_scorer(model)\u001b[38;5;241m.\u001b[39mscore(state, target)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\typing.py:1248\u001b[0m, in \u001b[0;36m_BaseGenericAlias.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inst:\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be instantiated; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1247\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__origin__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1248\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__origin__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1250\u001b[0m     result\u001b[38;5;241m.\u001b[39m__orig_class__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\typing.py:447\u001b[0m, in \u001b[0;36m_SpecialForm.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m--> 447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot instantiate typing.Union"
     ]
    }
   ],
   "source": [
    "from inspect_ai.solver import TaskState\n",
    "from inspect_ai.util import resource\n",
    "from inspect_ai.scorer._metric import Score\n",
    "from inspect_ai.scorer._scorer import Scorer, Target, scorer\n",
    "from inspect_ai.scorer._metrics import accuracy, bootstrap_std\n",
    "from inspect_ai.model import ChatMessageUser, ModelOutput, ChatCompletionChoice, ChatMessage\n",
    "\n",
    "def groundedness():\n",
    "    return lambda value: value[\"groundedness\"]\n",
    "\n",
    "def thoroughness():\n",
    "    return lambda value: value[\"thoroughness\"]\n",
    "\n",
    "@scorer(metrics=[groundedness(), thoroughness()])\n",
    "def fact_comparator_scorer(\n",
    "    model: OpenAI,\n",
    ") -> Scorer:\n",
    "    comparator = FactComparator(model)\n",
    "\n",
    "    async def score(state: TaskState, target: Target) -> Score:\n",
    "        result = comparator.process_data(target.text, state.output.completion)\n",
    "        metrics = comparator.calculate_metrics(result[\"comparison_result\"])\n",
    "\n",
    "        return Score(\n",
    "            value=metrics,\n",
    "            answer=state.output.completion,\n",
    "            explanation=f\"Groundedness: {metrics['groundedness']}, Thoroughness: {metrics['thoroughness']}\",\n",
    "            metadata=dict(\n",
    "                comparison_result=result[\"comparison_result\"].dict(),\n",
    "                context_replace_pronouns=result[\"context_replace_pronouns\"],\n",
    "                answer_replace_pronouns=result[\"answer_replace_pronouns\"],\n",
    "                context_list=result[\"context_list\"],\n",
    "                answer_list=result[\"answer_list\"],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    return score\n",
    "\n",
    "# Initialize your Langchain model\n",
    "model = OpenAI(temperature=0.7)\n",
    "\n",
    "# Define your evaluation data\n",
    "eval_data = [\n",
    "    {\n",
    "        \"context\": \"The capital of France is Paris.\",\n",
    "        \"answer\": \"Paris is the capital city of France.\",\n",
    "    },\n",
    "    # Add more evaluation samples\n",
    "]\n",
    "\n",
    "# Run the evaluation\n",
    "results = []\n",
    "for data in eval_data:\n",
    "    state = TaskState(\n",
    "        model=model.model_name,\n",
    "        sample_id=data.get(\"id\", \"\"),\n",
    "        epoch=0,\n",
    "        input=data[\"context\"],\n",
    "        choices=None,\n",
    "        messages=[ChatMessageUser(content=data[\"context\"])],\n",
    "        output=ModelOutput(\n",
    "            model=str(model.model_name),\n",
    "            choices=[ChatCompletionChoice(message=ChatMessage(content=data[\"answer\"]))],\n",
    "        ),\n",
    "        completed=True,\n",
    "    )\n",
    "    target = Target(text=data[\"context\"])\n",
    "    score = await fact_comparator_scorer(model).score(state, target)\n",
    "    results.append(score)\n",
    "\n",
    "# Process and display the evaluation results\n",
    "for result in results:\n",
    "    print(f\"Groundedness: {result.value['groundedness']}\")\n",
    "    print(f\"Thoroughness: {result.value['thoroughness']}\")\n",
    "    print(f\"Explanation: {result.explanation}\")\n",
    "    print(f\"Metadata: {result.metadata}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45092a7-a722-4d13-979a-8569619ac649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Virtual Env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
