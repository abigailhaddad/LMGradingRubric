{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a851d3d-cb9b-448c-ae3b-21c524601308",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5119e1e4-68fd-4176-9b72-ff30c8f3c38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (0.1.19)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.6.6)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.0.38)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.1.52)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.1.56)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain) (2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.1.6-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.46 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-openai) (0.1.52)\n",
      "Collecting openai<2.0.0,>=1.24.0\n",
      "  Downloading openai-1.28.0-py3-none-any.whl (320 kB)\n",
      "     -------------------------------------- 320.1/320.1 kB 6.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-openai) (0.5.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (0.1.56)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (2.6.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (8.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (0.26.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain-openai) (3.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.46->langchain-openai) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langchain-openai) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langchain-openai) (2.16.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai) (2.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.24.0->langchain-openai) (0.4.6)\n",
      "Installing collected packages: openai, langchain-openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.11.1\n",
      "    Uninstalling openai-1.11.1:\n",
      "      Successfully uninstalled openai-1.11.1\n",
      "Successfully installed langchain-openai-0.1.6 openai-1.28.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain\n",
    "%pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35219496-2769-4767-831c-f85ac271b390",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d108b5b-d0ad-4b5f-8b2d-308364e05dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Define the prompt templates\n",
    "pronoun_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"\"\"\n",
    "Your task is to replace all the pronouns in the following text with the nouns they refer to:\n",
    "\n",
    "<text>\n",
    "{text}\n",
    "</text>\n",
    "\n",
    "The goal is to make the text more explicit and clear by replacing potentially ambiguous pronouns like \"he\", \"she\", \"it\", \"they\", \"them\", etc. with the specific nouns or names they refer to.\n",
    "\n",
    "For example:\n",
    "Original: John went to the store. He bought some milk.\n",
    "Pronoun replaced: John went to the store. John bought some milk.\n",
    "\n",
    "Here are the steps to complete this task:\n",
    "\n",
    "1. Carefully read the provided text and identify all the pronouns \n",
    "2. For each pronoun, look back in the text to determine which noun or name it is referring to\n",
    "3. If the pronoun is part of a direct quote, do not replace it\n",
    "4. Replace each pronoun with the most recent noun or name it refers to\n",
    "5. If a pronoun does not have a clear referent noun or name, do not replace it\n",
    "6. Repeat this process until all the pronouns with clear referents have been replaced\n",
    "\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "parse_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"\"\"\n",
    "\n",
    "    Please parse the following text into a list of individual facts:\n",
    "\n",
    "<text>\n",
    "{text}\n",
    "</text>\n",
    "\n",
    "Read the text carefully. Your task is to break it down into the key facts it contains. Parse out each individual fact into a separate sentence, even if that means splitting up or rewording the original sentences. The goal is to have a clear, concise list of the core facts contained in the text.\n",
    "\n",
    "Output the parsed facts in a numbered list, with each fact written as a complete sentence on its own line. Use <facts> tags to demarcate the start and end of the list.\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "compare_prompt = PromptTemplate(\n",
    "    input_variables=[\"context_list\", \"answer_list\"],\n",
    "    template=\"\"\"\n",
    "\n",
    "You will be comparing facts between a context and an answer to determine which facts are shared and which are unique to each.\n",
    "\n",
    "Here is the context:\n",
    "\n",
    "<context>\n",
    "\n",
    "{context_list}\n",
    "\n",
    "</context>\n",
    "\n",
    "And here is the answer: \n",
    "\n",
    "<answer>\n",
    "\n",
    "{answer_list}\n",
    "\n",
    "</answer>\n",
    "\n",
    "Carefully analyze the facts presented in the context and answer, focusing on the semantic meaning rather than the exact wording.\n",
    "\n",
    "Then, output a dictionary with the following keys and corresponding lists of facts as values:\n",
    "\n",
    "1. \"facts_in_both\": A list of facts that are present in both the context and the answer\n",
    "\n",
    "2. \"facts_only_in_answer\": A list of facts that are only present in the answer \n",
    "\n",
    "3. \"facts_only_in_context\": A list of facts that are only present in the context\n",
    "\n",
    "Remember, the facts do not need to be worded identically to be considered the same. Focus on whether the core meaning is shared or unique.\n",
    "\n",
    "Provide your results in this format:\n",
    "\n",
    "{{\n",
    "    \"facts_in_both\": [\n",
    "        \"Fact 1 present in both\",\n",
    "        \"Fact 2 present in both\"\n",
    "    ],\n",
    "    \"facts_only_in_answer\": [\n",
    "        \"Fact 1 only in answer\",\n",
    "        \"Fact 2 only in answer\"  \n",
    "    ],\n",
    "    \"facts_only_in_context\": [\n",
    "        \"Fact 1 only in context\",\n",
    "        \"Fact 2 only in context\"\n",
    "    ]\n",
    "}}\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class ComparisonResult(BaseModel):\n",
    "    facts_in_both: list[str] = Field(default_factory=list, description=\"List of facts present in both context and answer\")\n",
    "    facts_only_in_answer: list[str] = Field(default_factory=list, description=\"List of facts only present in the answer\")\n",
    "    facts_only_in_context: list[str] = Field(default_factory=list, description=\"List of facts only present in the context\")\n",
    "\n",
    "def process_data(context, answer):\n",
    "    # Replace pronouns in the context and answer\n",
    "    context_replace_pronouns = pronoun_chain.run(text=context)\n",
    "    answer_replace_pronouns = pronoun_chain.run(text=answer)\n",
    "\n",
    "    # Parse the context and answer into lists of strings\n",
    "    context_list = parse_chain.run(text=context_replace_pronouns)\n",
    "    answer_list = parse_chain.run(text=answer_replace_pronouns)\n",
    "\n",
    "    # Compare the context and answer statements\n",
    "    comparison_result = parser.parse(compare_chain.run(context_list=context_list, answer_list=answer_list))\n",
    "\n",
    "    return {\n",
    "        \"context_replace_pronouns\": context_replace_pronouns,\n",
    "        \"answer_replace_pronouns\": answer_replace_pronouns,\n",
    "        \"context_list\": context_list,\n",
    "        \"answer_list\": answer_list,\n",
    "        \"comparison_result\": comparison_result,\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_metrics(comparison_result):\n",
    "    facts_in_both_count = len(comparison_result.facts_in_both)\n",
    "    facts_only_in_answer_count = len(comparison_result.facts_only_in_answer)\n",
    "    facts_only_in_context_count = len(comparison_result.facts_only_in_context)\n",
    "\n",
    "    total_answer_facts = facts_in_both_count + facts_only_in_answer_count\n",
    "    total_context_facts = facts_in_both_count + facts_only_in_context_count\n",
    "\n",
    "    groundedness = facts_in_both_count / total_answer_facts * 100 if total_answer_facts > 0 else 0\n",
    "    thoroughness = facts_in_both_count / total_context_facts * 100 if total_context_facts > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"groundedness\": groundedness,\n",
    "        \"thoroughness\": thoroughness,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2751f4e-3992-496c-aad0-c0604a712257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context with replaced pronouns:\n",
      "\n",
      "The quick brown fox jumps over the rock because the fox is happy. The fox was born in 2005. The hedgehog was born in 2010, but the hedgehog is even happier than the fox.\n",
      "\n",
      "Answer with replaced pronouns:\n",
      "\n",
      "The quick brown fox was born in 2005, and the hedgehog in 2010. The quick brown fox is not as happy as the hedgehog.\n",
      "\n",
      "Context list:\n",
      "\n",
      "<facts>\n",
      "1. The quick brown fox jumps over the rock.\n",
      "2. The fox is happy.\n",
      "3. The fox was born in 2005.\n",
      "4. The hedgehog was born in 2010.\n",
      "5. The hedgehog is even happier than the fox.\n",
      "</facts>\n",
      "\n",
      "Answer list:\n",
      "\n",
      "<facts>\n",
      "1. The quick brown fox was born in 2005.\n",
      "2. The hedgehog was born in 2010.\n",
      "3. The quick brown fox is not as happy as the hedgehog.\n",
      "</facts>\n",
      "\n",
      "Comparison result:\n",
      "facts_in_both=['The quick brown fox was born in 2005.', 'The hedgehog was born in 2010.'] facts_only_in_answer=['The quick brown fox is not as happy as the hedgehog.'] facts_only_in_context=['The quick brown fox jumps over the rock.', 'The fox is happy.', 'The fox was born in 2005.', 'The hedgehog is even happier than the fox.']\n",
      "\n",
      "Metrics:\n",
      "Groundedness: 66.67%\n",
      "Thoroughness: 33.33%\n"
     ]
    }
   ],
   "source": [
    "# Set up the language model and chains\n",
    "model = OpenAI(temperature=0)\n",
    "pronoun_chain = LLMChain(llm=model, prompt=pronoun_prompt)\n",
    "parse_chain = LLMChain(llm=model, prompt=parse_prompt)\n",
    "parser = PydanticOutputParser(pydantic_object=ComparisonResult)\n",
    "compare_prompt_with_instructions = PromptTemplate(\n",
    "    template=compare_prompt.template + \"\\n{format_instructions}\",\n",
    "    input_variables=[\"context_list\", \"answer_list\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "compare_chain = LLMChain(llm=model, prompt=compare_prompt_with_instructions)\n",
    "\n",
    "# Example usage\n",
    "context = \"The quick brown fox jumps over the rock because he's happy. He was born in 2005. The hedgehog was born in 2010, but she's even happier than him.\"\n",
    "answer = \"The quick brown fox was born in 2005, and the hedgehog in 2010. The quick brown fox is not as happy as the hedgehog\"\n",
    "\n",
    "result = process_data(context, answer)\n",
    "\n",
    "metrics = calculate_metrics(result[\"comparison_result\"])\n",
    "\n",
    "print(\"Context with replaced pronouns:\")\n",
    "print(result[\"context_replace_pronouns\"])\n",
    "\n",
    "print(\"\\nAnswer with replaced pronouns:\")\n",
    "print(result[\"answer_replace_pronouns\"])\n",
    "\n",
    "print(\"\\nContext list:\")\n",
    "print(result[\"context_list\"])\n",
    "\n",
    "print(\"\\nAnswer list:\")\n",
    "print(result[\"answer_list\"])\n",
    "\n",
    "print(\"\\nComparison result:\")\n",
    "print(result[\"comparison_result\"])\n",
    "\n",
    "print(\"\\nMetrics:\")\n",
    "print(f\"Groundedness: {metrics['groundedness']:.2f}%\")\n",
    "print(f\"Thoroughness: {metrics['thoroughness']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11437091-c8a1-40fe-b55b-d4f06c0941c3",
   "metadata": {},
   "source": [
    "## Run on one pair of statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad11011d-dd24-4c8f-b4e9-e8573b0c7ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context with replaced pronouns:\n",
      "\n",
      "To boil pasta, first bring a large pot of salted water to a rolling boil over high heat.\n",
      "\n",
      "Answer with replaced pronouns:\n",
      "\n",
      "To boil pasta, begin by filling a large pot with water, making sure there's enough water to fully submerge the pasta. Bring the water to a rolling boil over high heat, then add salt to enhance the pasta's flavor. Once the water is boiling, carefully add the pasta, stirring gently to prevent the pasta from sticking. Cook the pasta according to the package instructions or until the pasta reaches your desired level of tenderness, usually around 8-12 minutes. To check for doneness, taste a piece of pasta—it should be tender but still slightly firm (al dente).\n",
      "\n",
      "Context list:\n",
      "\n",
      "<facts>\n",
      "1. To boil pasta, you need to bring a large pot of salted water to a rolling boil.\n",
      "2. The water should be brought to a rolling boil over high heat.\n",
      "3. The pot should be large.\n",
      "4. The water should be salted.\n",
      "5. The heat should be high.\n",
      "</facts>\n",
      "\n",
      "Answer list:\n",
      "\n",
      "<facts>\n",
      "1. To boil pasta, begin by filling a large pot with water.\n",
      "2. Make sure there's enough water to fully submerge the pasta.\n",
      "3. Bring the water to a rolling boil over high heat.\n",
      "4. Add salt to enhance the pasta's flavor.\n",
      "5. Once the water is boiling, carefully add the pasta.\n",
      "6. Stir gently to prevent the pasta from sticking.\n",
      "7. Cook the pasta according to the package instructions.\n",
      "8. Or until the pasta reaches your desired level of tenderness.\n",
      "9. Usually around 8-12 minutes.\n",
      "10. To check for doneness, taste a piece of pasta.\n",
      "11. It should be tender but still slightly firm (al dente).\n",
      "</facts>\n",
      "\n",
      "Comparison result:\n",
      "facts_in_both=['To boil pasta, you need to bring a large pot of salted water to a rolling boil.', 'The water should be brought to a rolling boil over high heat.', 'The pot should be large.', 'The water should be salted.', 'The heat should be high.'] facts_only_in_answer=['To boil pasta, begin by filling a large pot with water.', \"Make sure there's enough water to fully submerge the pasta.\", \"Add salt to enhance the pasta's flavor.\", 'Once the water is boiling, carefully add the pasta.', 'Stir gently to prevent the pasta from sticking.', 'Cook the pasta according to the package instructions.', 'Or until the pasta reaches your desired level of tenderness.', 'Usually around 8-12 minutes.', 'To check for doneness, taste a piece of pasta.', 'It should be tender but still slightly firm (al dente).'] facts_only_in_context=[]\n",
      "\n",
      "Metrics:\n",
      "Groundedness: 33.33%\n",
      "Thoroughness: 100.00%\n"
     ]
    }
   ],
   "source": [
    "context = \"To boil pasta, first bring a large pot of salted water to a rolling boil over high heat..\"\n",
    "answer = \"To boil pasta, begin by filling a large pot with water, making sure there's enough to fully submerge the pasta. Bring the water to a rolling boil over high heat, then add salt to enhance the pasta's flavor. Once the water is boiling, carefully add the pasta, stirring gently to prevent sticking. Cook the pasta according to the package instructions or until it reaches your desired level of tenderness, usually around 8-12 minutes. To check for doneness, taste a piece of pasta—it should be tender but still slightly firm (al dente).\"\n",
    "\n",
    "result = process_data(context, answer)\n",
    "\n",
    "metrics = calculate_metrics(result[\"comparison_result\"])\n",
    "\n",
    "print(\"Context with replaced pronouns:\")\n",
    "print(result[\"context_replace_pronouns\"])\n",
    "\n",
    "print(\"\\nAnswer with replaced pronouns:\")\n",
    "print(result[\"answer_replace_pronouns\"])\n",
    "\n",
    "print(\"\\nContext list:\")\n",
    "print(result[\"context_list\"])\n",
    "\n",
    "print(\"\\nAnswer list:\")\n",
    "print(result[\"answer_list\"])\n",
    "\n",
    "print(\"\\nComparison result:\")\n",
    "print(result[\"comparison_result\"])\n",
    "\n",
    "print(\"\\nMetrics:\")\n",
    "print(f\"Groundedness: {metrics['groundedness']:.2f}%\")\n",
    "print(f\"Thoroughness: {metrics['thoroughness']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaf37e7-66f4-422b-926d-cc1b93109a89",
   "metadata": {},
   "source": [
    "## Run on a list of dictionaries - return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9514e054-f73a-49e8-b893-61981fe6d802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 3]\n",
      "[4, 1, 2]\n",
      "[8, 1, 3]\n",
      "[5, 8, 1]\n",
      "[4, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "data_list = [\n",
    "    {\n",
    "        'context': 'The quick brown fox jumps over the rock because he\\'s happy. He was born in 2005. The hedgehog was born in 2010, but she\\'s even happier than him.',\n",
    "        'answer': 'The quick brown fox was born in 2005, and the hedgehog in 2010. The quick brown fox is not as happy as the hedgehog'\n",
    "    },\n",
    "    {\n",
    "        'context': 'The sun is a star at the center of our solar system. It is about 93 million miles away from Earth. The sun is a hot ball of glowing gases that provides light and warmth to Earth.',\n",
    "        'answer': 'The sun is a star located approximately 93 million miles from Earth. It is the source of light and heat for our planet. The sun is not a solid object, but rather a sphere of hot glowing gases.'\n",
    "    },\n",
    "    {\n",
    "        'context': 'Birds are warm-blooded vertebrates that lay eggs and have feathers, wings, and beaks. There are over 10,000 species of birds worldwide. Some common bird species include sparrows, pigeons, and parrots.',\n",
    "        'answer': 'Birds are a diverse group of animals with feathers and wings. They are warm-blooded egg-laying vertebrates. The number of bird species globally exceeds 10,000. Pigeons, parrots, and sparrows are among the most familiar bird types.'\n",
    "    },\n",
    "    {\n",
    "        'context': 'The Eiffel Tower is a wrought-iron lattice tower located on the Champ de Mars in Paris, France. It was constructed from 1887 to 1889 and stands at a height of 324 meters. The tower is named after Gustave Eiffel, whose company designed and built it.',\n",
    "        'answer': 'The Eiffel Tower, found in Paris, France, is a lattice tower made of wrought iron. Built between 1887 and 1889, it reaches a height of 324 meters. Gustave Eiffel\\'s company was responsible for the tower\\'s design and construction, hence its name.'\n",
    "    },\n",
    "    {\n",
    "        'context': 'The Great Wall of China is a series of fortifications and walls built across the historical northern borders of ancient Chinese states and Imperial China. The most well-known sections were built during the Ming dynasty, which ruled from 1368 to 1644.',\n",
    "        'answer': 'The Great Wall of China, a series of walls and fortifications, was constructed along the northern borders of ancient Chinese states and Imperial China. The Ming dynasty, which lasted from 1368 to 1644, is responsible for the construction of the most famous sections of the wall.'\n",
    "    }\n",
    "]\n",
    "\n",
    "result_df = process_data(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5a634a7-bb57-41c0-9370-8052bbfa35cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>context_replace_pronouns</th>\n",
       "      <th>answer_replace_pronouns</th>\n",
       "      <th>context_list</th>\n",
       "      <th>answer_list</th>\n",
       "      <th>classifications</th>\n",
       "      <th>groundedness</th>\n",
       "      <th>thoroughness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The quick brown fox jumps over the rock becaus...</td>\n",
       "      <td>The quick brown fox was born in 2005, and the ...</td>\n",
       "      <td>The quick brown fox jumps over the rock becaus...</td>\n",
       "      <td>The quick brown fox was born in 2005, and the ...</td>\n",
       "      <td>[The quick brown fox jumps over the rock., The...</td>\n",
       "      <td>[The quick brown fox was born in 2005., The he...</td>\n",
       "      <td>[[The quick brown fox was born in 2005., The h...</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The sun is a star at the center of our solar s...</td>\n",
       "      <td>The sun is a star located approximately 93 mil...</td>\n",
       "      <td>The sun is a star at the center of our solar s...</td>\n",
       "      <td>The sun is a star located approximately 93 mil...</td>\n",
       "      <td>[The sun is a star at the center of our solar ...</td>\n",
       "      <td>[The sun is a star., The sun is located approx...</td>\n",
       "      <td>[[The sun is a star., The sun is located appro...</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>66.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Birds are warm-blooded vertebrates that lay eg...</td>\n",
       "      <td>Birds are a diverse group of animals with feat...</td>\n",
       "      <td>Birds are warm-blooded vertebrates that lay eg...</td>\n",
       "      <td>Birds are a diverse group of animals with feat...</td>\n",
       "      <td>[Birds are warm-blooded vertebrates., Birds la...</td>\n",
       "      <td>[Birds are a diverse group of animals with fea...</td>\n",
       "      <td>[[Birds are warm-blooded vertebrates., Birds l...</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>72.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Eiffel Tower is a wrought-iron lattice tow...</td>\n",
       "      <td>The Eiffel Tower, found in Paris, France, is a...</td>\n",
       "      <td>The Eiffel Tower is a wrought-iron lattice tow...</td>\n",
       "      <td>The Eiffel Tower, found in Paris, France, is a...</td>\n",
       "      <td>[The Eiffel Tower is a wrought-iron lattice to...</td>\n",
       "      <td>[The Eiffel Tower is found in Paris, France., ...</td>\n",
       "      <td>[[The Eiffel Tower is located on the Champ de ...</td>\n",
       "      <td>38.461538</td>\n",
       "      <td>83.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Great Wall of China is a series of fortifi...</td>\n",
       "      <td>The Great Wall of China, a series of walls and...</td>\n",
       "      <td>The Great Wall of China is a series of fortifi...</td>\n",
       "      <td>The Great Wall of China, a series of walls and...</td>\n",
       "      <td>[The Great Wall of China is a series of fortif...</td>\n",
       "      <td>[The Great Wall of China is a series of walls ...</td>\n",
       "      <td>[[The Great Wall of China is a series of walls...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  The quick brown fox jumps over the rock becaus...   \n",
       "1  The sun is a star at the center of our solar s...   \n",
       "2  Birds are warm-blooded vertebrates that lay eg...   \n",
       "3  The Eiffel Tower is a wrought-iron lattice tow...   \n",
       "4  The Great Wall of China is a series of fortifi...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  The quick brown fox was born in 2005, and the ...   \n",
       "1  The sun is a star located approximately 93 mil...   \n",
       "2  Birds are a diverse group of animals with feat...   \n",
       "3  The Eiffel Tower, found in Paris, France, is a...   \n",
       "4  The Great Wall of China, a series of walls and...   \n",
       "\n",
       "                            context_replace_pronouns  \\\n",
       "0  The quick brown fox jumps over the rock becaus...   \n",
       "1  The sun is a star at the center of our solar s...   \n",
       "2  Birds are warm-blooded vertebrates that lay eg...   \n",
       "3  The Eiffel Tower is a wrought-iron lattice tow...   \n",
       "4  The Great Wall of China is a series of fortifi...   \n",
       "\n",
       "                             answer_replace_pronouns  \\\n",
       "0  The quick brown fox was born in 2005, and the ...   \n",
       "1  The sun is a star located approximately 93 mil...   \n",
       "2  Birds are a diverse group of animals with feat...   \n",
       "3  The Eiffel Tower, found in Paris, France, is a...   \n",
       "4  The Great Wall of China, a series of walls and...   \n",
       "\n",
       "                                        context_list  \\\n",
       "0  [The quick brown fox jumps over the rock., The...   \n",
       "1  [The sun is a star at the center of our solar ...   \n",
       "2  [Birds are warm-blooded vertebrates., Birds la...   \n",
       "3  [The Eiffel Tower is a wrought-iron lattice to...   \n",
       "4  [The Great Wall of China is a series of fortif...   \n",
       "\n",
       "                                         answer_list  \\\n",
       "0  [The quick brown fox was born in 2005., The he...   \n",
       "1  [The sun is a star., The sun is located approx...   \n",
       "2  [Birds are a diverse group of animals with fea...   \n",
       "3  [The Eiffel Tower is found in Paris, France., ...   \n",
       "4  [The Great Wall of China is a series of walls ...   \n",
       "\n",
       "                                     classifications  groundedness  \\\n",
       "0  [[The quick brown fox was born in 2005., The h...     66.666667   \n",
       "1  [[The sun is a star., The sun is located appro...     80.000000   \n",
       "2  [[Birds are warm-blooded vertebrates., Birds l...     88.888889   \n",
       "3  [[The Eiffel Tower is located on the Champ de ...     38.461538   \n",
       "4  [[The Great Wall of China is a series of walls...    100.000000   \n",
       "\n",
       "   thoroughness  \n",
       "0     40.000000  \n",
       "1     66.666667  \n",
       "2     72.727273  \n",
       "3     83.333333  \n",
       "4    100.000000  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Virtual Env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
