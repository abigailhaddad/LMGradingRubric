{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a851d3d-cb9b-448c-ae3b-21c524601308",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5119e1e4-68fd-4176-9b72-ff30c8f3c38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (0.1.19)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.6.6)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.0.38)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.1.52)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (0.1.56)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain) (2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.2-py3-none-any.whl (139 kB)\n",
      "     -------------------------------------- 139.4/139.4 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipywidgets) (8.21.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipywidgets) (5.14.1)\n",
      "Collecting widgetsnbextension~=4.0.10\n",
      "  Downloading widgetsnbextension-4.0.10-py3-none-any.whl (2.3 MB)\n",
      "     ---------------------------------------- 2.3/2.3 MB 9.1 MB/s eta 0:00:00\n",
      "Collecting jupyterlab-widgets~=3.0.10\n",
      "  Downloading jupyterlab_widgets-3.0.10-py3-none-any.whl (215 kB)\n",
      "     ------------------------------------- 215.0/215.0 kB 12.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: decorator in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.2 jupyterlab-widgets-3.0.10 widgetsnbextension-4.0.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (0.1.6)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.46 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-openai) (0.1.52)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.24.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-openai) (1.28.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-openai) (0.5.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (0.1.56)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (2.6.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (8.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (0.26.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain-openai) (3.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.46->langchain-openai) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langchain-openai) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langchain-openai) (2.16.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\abiga\\onedrive\\documents\\pythonscripts\\llmgradingrubric\\env\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.24.0->langchain-openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain\n",
    "%pip install ipywidgets\n",
    "%pip install langchain-openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35219496-2769-4767-831c-f85ac271b390",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbb8fd73-0951-4687-9e4f-3c2433666dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Dict, Protocol, cast, runtime_checkable\n",
    "\n",
    "from langchain_core.callbacks import (\n",
    "    AsyncCallbackManagerForLLMRun,\n",
    "    CallbackManagerForLLMRun,\n",
    ")\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "from langchain_core.messages import ToolCall as LCToolCall\n",
    "from langchain_core.outputs import (\n",
    "    ChatGeneration,\n",
    "    ChatResult,\n",
    ")\n",
    "from pydantic.v1 import Field\n",
    "from typing_extensions import override\n",
    "\n",
    "from inspect_ai.model import (\n",
    "    ChatMessage,\n",
    "    ChatMessageAssistant,\n",
    "    ChatMessageSystem,\n",
    "    ChatMessageTool,\n",
    "    ChatMessageUser,\n",
    "    Content,\n",
    "    ContentImage,\n",
    "    ContentText,\n",
    "    GenerateConfig,\n",
    "    ModelName,\n",
    "    ModelOutput,\n",
    "    ToolCall,\n",
    "    ToolChoice,\n",
    "    ToolInfo,\n",
    "    ToolParam,\n",
    "    get_model,\n",
    ")\n",
    "from inspect_ai.solver import Generate, Solver, TaskState\n",
    "\n",
    "@runtime_checkable\n",
    "class LangChainAgent(Protocol):\n",
    "    async def __call__(\n",
    "        self, llm: BaseChatModel, input: dict[str, Any]\n",
    "    ) -> str | list[str | dict[str, Any]]:\n",
    "        ...\n",
    "\n",
    "\n",
    "def langchain_solver(agent: LangChainAgent) -> Solver:\n",
    "    async def solve(state: TaskState, generate: Generate) -> TaskState:\n",
    "        # create the inspect model api bridge\n",
    "        llm = InspectChatModel()\n",
    "\n",
    "        # call the agent\n",
    "        await agent(\n",
    "            llm=llm,\n",
    "            input=dict(\n",
    "                input=state.user_prompt.text,\n",
    "                chat_history=as_langchain_chat_history(state.messages[1:]),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # collect output from llm interface\n",
    "        state.messages = llm.messages\n",
    "        state.output = llm.output\n",
    "\n",
    "        # return state\n",
    "        return state\n",
    "\n",
    "    return solve\n",
    "\n",
    "\n",
    "class InspectChatModel(BaseChatModel):\n",
    "    # track messages and model output so we can update\n",
    "    # the inspect task state when we are complete\n",
    "    messages: list[ChatMessage] = Field(default=[], exclude=True)\n",
    "    output: ModelOutput = Field(default=ModelOutput(), exclude=True)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return f\"Inspect ({ModelName(get_model()).api})\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"model_name\": str(ModelName(get_model()).name),\n",
    "        }\n",
    "\n",
    "    @override\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: list[BaseMessage],\n",
    "        stop: list[str] | None = None,\n",
    "        run_manager: CallbackManagerForLLMRun | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        # inspect uses async exclusively\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @override\n",
    "    async def _agenerate(\n",
    "        self,\n",
    "        messages: list[BaseMessage],\n",
    "        stop: list[str] | None = None,\n",
    "        run_manager: AsyncCallbackManagerForLLMRun | None = None,\n",
    "        **kwargs: dict[str, Any],\n",
    "    ) -> ChatResult:\n",
    "        # extract tools from kwargs\n",
    "        tools: list[ToolInfo] = []\n",
    "        tool_choice: ToolChoice | None = None\n",
    "        lc_tools = cast(list[dict[str, Any]] | None, kwargs.get(\"tools\", None))\n",
    "        if lc_tools:\n",
    "            tools = [\n",
    "                ToolInfo(\n",
    "                    name=tool[\"function\"][\"name\"],\n",
    "                    description=tool[\"function\"][\"description\"],\n",
    "                    params=as_inspect_tool_params(tool[\"function\"][\"parameters\"]),\n",
    "                )\n",
    "                for tool in lc_tools\n",
    "            ]\n",
    "            tool_choice = \"auto\"\n",
    "\n",
    "        # generate\n",
    "        input = [as_inspect_message(message) for message in messages]\n",
    "        result = await get_model().generate(\n",
    "            input=input,\n",
    "            tools=tools,\n",
    "            tool_choice=tool_choice,\n",
    "            config=GenerateConfig(stop_seqs=stop),\n",
    "        )\n",
    "\n",
    "        # track last messages / model output\n",
    "        self.messages = input\n",
    "        self.messages.append(result.choices[0].message)\n",
    "        self.output = result\n",
    "\n",
    "        # extract choices\n",
    "        generations = [\n",
    "            ChatGeneration(message=as_langchain_message(choice.message))\n",
    "            for choice in result.choices\n",
    "        ]\n",
    "\n",
    "        # return\n",
    "        return ChatResult(generations=generations)\n",
    "\n",
    "\n",
    "def as_inspect_message(message: BaseMessage) -> ChatMessage:\n",
    "    if isinstance(message, SystemMessage):\n",
    "        return ChatMessageSystem(content=as_inspect_content(message.content))\n",
    "    elif isinstance(message, HumanMessage):\n",
    "        return ChatMessageUser(content=as_inspect_content(message.content))\n",
    "    elif isinstance(message, AIMessage):\n",
    "        return ChatMessageAssistant(\n",
    "            content=as_inspect_content(message.content),\n",
    "            tool_calls=(\n",
    "                [\n",
    "                    ToolCall(\n",
    "                        type=\"function\",\n",
    "                        function=call[\"name\"],\n",
    "                        id=call[\"id\"] or call[\"name\"],\n",
    "                        arguments=call[\"args\"],\n",
    "                    )\n",
    "                    for call in message.tool_calls\n",
    "                ]\n",
    "                if message.tool_calls and len(message.tool_calls) > 0\n",
    "                else None\n",
    "            ),\n",
    "        )\n",
    "    elif isinstance(message, ToolMessage):\n",
    "        return ChatMessageTool(\n",
    "            content=as_inspect_content(message.content),\n",
    "            tool_call_id=message.tool_call_id,\n",
    "        )\n",
    "    elif isinstance(message, FunctionMessage):\n",
    "        return ChatMessageTool(\n",
    "            content=as_inspect_content(message.content), tool_call_id=message.name\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected message type: {type(message)}\")\n",
    "\n",
    "\n",
    "def as_langchain_message(message: ChatMessage) -> BaseMessage:\n",
    "    if isinstance(message, ChatMessageSystem):\n",
    "        return SystemMessage(content=as_langchain_content(message.content))\n",
    "    elif isinstance(message, ChatMessageUser):\n",
    "        return HumanMessage(content=as_langchain_content(message.content))\n",
    "    elif isinstance(message, ChatMessageAssistant):\n",
    "        additional_kwargs: dict[str, Any] = {}\n",
    "        if message.tool_calls and len(message.tool_calls) > 0:\n",
    "            additional_kwargs[\"tool_calls\"] = [\n",
    "                dict(\n",
    "                    id=call.id, name=call.function, arguments=json.dumps(call.arguments)\n",
    "                )\n",
    "                for call in message.tool_calls\n",
    "            ]\n",
    "\n",
    "        return AIMessage(\n",
    "            content=as_langchain_content(message.content),\n",
    "            tool_calls=(\n",
    "                [\n",
    "                    LCToolCall(id=call.id, name=call.function, args=call.arguments)\n",
    "                    for call in message.tool_calls\n",
    "                ]\n",
    "                if message.tool_calls\n",
    "                else []\n",
    "            ),\n",
    "            additional_kwargs=additional_kwargs,\n",
    "        )\n",
    "    elif isinstance(message, ChatMessageTool):\n",
    "        return ToolMessage(\n",
    "            content=as_langchain_content(message.content),\n",
    "            tool_call_id=message.tool_call_id or \"\",\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected message type: {type(message)}\")\n",
    "\n",
    "\n",
    "def as_langchain_chat_history(messages: list[ChatMessage]) -> list[dict[str, Any]]:\n",
    "    return [dict(role=message.role, content=message.text) for message in messages]\n",
    "\n",
    "\n",
    "def as_inspect_content(\n",
    "    content: str | list[str | dict[str, Any]],\n",
    ") -> str | list[Content]:\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "    else:\n",
    "        return [\n",
    "            (\n",
    "                ContentText(text=c)\n",
    "                if isinstance(c, str)\n",
    "                else (\n",
    "                    ContentText(text=c[\"text\"])\n",
    "                    if c[\"type\"] == \"text\"\n",
    "                    else ContentImage(image=c[\"image\"])\n",
    "                )\n",
    "            )\n",
    "            for c in content\n",
    "        ]\n",
    "\n",
    "\n",
    "def as_inspect_tool_params(parameters: dict[str, Any]) -> list[ToolParam]:\n",
    "    params: list[ToolParam] = []\n",
    "    for key, param in parameters[\"properties\"].items():\n",
    "        params.append(\n",
    "            ToolParam(\n",
    "                name=key,\n",
    "                type=param[\"type\"],\n",
    "                description=param.get(\"description\", param.get(\"title\")),\n",
    "                optional=key not in parameters[\"required\"],\n",
    "            )\n",
    "        )\n",
    "    return params\n",
    "\n",
    "\n",
    "def as_langchain_content(\n",
    "    content: str | list[Content],\n",
    ") -> str | list[str | dict[str, Any]]:\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "    else:\n",
    "        return [c if isinstance(c, str) else c.model_dump() for c in content]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d108b5b-d0ad-4b5f-8b2d-308364e05dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "import pandas as pd \n",
    "\n",
    "class FactComparator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.parser = PydanticOutputParser(pydantic_object=ComparisonResult)\n",
    "\n",
    "    async def process_data(self, context, answer):\n",
    "        print(type(self.model))\n",
    "        context_replace_pronouns = (await self.model._agenerate([HumanMessage(content=context)])).generations[0].text\n",
    "        answer_replace_pronouns = (await self.model._agenerate([HumanMessage(content=answer)])).generations[0].text\n",
    "\n",
    "        context_list = (await self.model._agenerate([HumanMessage(content=self._parse_prompt().format(text=context_replace_pronouns))])).generations[0].text\n",
    "        answer_list = (await self.model._agenerate([HumanMessage(content=self._parse_prompt().format(text=answer_replace_pronouns))])).generations[0].text\n",
    "\n",
    "        comparison_result = self.parser.parse((await self.model._agenerate([HumanMessage(content=self._compare_prompt().format(context_list=context_list, answer_list=answer_list))])).generations[0].text)\n",
    "\n",
    "        return {\n",
    "            \"context_replace_pronouns\": context_replace_pronouns,\n",
    "            \"answer_replace_pronouns\": answer_replace_pronouns,\n",
    "            \"context_list\": context_list,\n",
    "            \"answer_list\": answer_list,\n",
    "            \"comparison_result\": comparison_result,\n",
    "        }\n",
    "\n",
    "    def calculate_metrics(self, comparison_result):\n",
    "        facts_in_both_count = len(comparison_result.facts_in_both)\n",
    "        facts_only_in_answer_count = len(comparison_result.facts_only_in_answer)\n",
    "        facts_only_in_context_count = len(comparison_result.facts_only_in_context)\n",
    "\n",
    "        total_answer_facts = facts_in_both_count + facts_only_in_answer_count\n",
    "        total_context_facts = facts_in_both_count + facts_only_in_context_count\n",
    "\n",
    "        groundedness = facts_in_both_count / total_answer_facts * 100 if total_answer_facts > 0 else 0\n",
    "        thoroughness = facts_in_both_count / total_context_facts * 100 if total_context_facts > 0 else 0\n",
    "\n",
    "        return {\n",
    "            \"groundedness\": groundedness,\n",
    "            \"thoroughness\": thoroughness,\n",
    "        }\n",
    "\n",
    "    def process_data_list(self, data_list):\n",
    "        results = []\n",
    "        for data in data_list:\n",
    "            context = data['context']\n",
    "            answer = data['answer']\n",
    "    \n",
    "            result = asyncio.run(comparator.process_data(context, answer))\n",
    "            metrics = comparator.calculate_metrics(result[\"comparison_result\"])\n",
    "\n",
    "            result_data = {\n",
    "                'context': context,\n",
    "                'answer': answer,\n",
    "                'context_replace_pronouns': result[\"context_replace_pronouns\"],\n",
    "                'answer_replace_pronouns': result[\"answer_replace_pronouns\"],\n",
    "                'context_list': result[\"context_list\"],\n",
    "                'answer_list': result[\"answer_list\"],\n",
    "                'facts_in_both': ', '.join(result[\"comparison_result\"].facts_in_both),\n",
    "                'facts_only_in_answer': ', '.join(result[\"comparison_result\"].facts_only_in_answer),\n",
    "                'facts_only_in_context': ', '.join(result[\"comparison_result\"].facts_only_in_context),\n",
    "                'groundedness': metrics['groundedness'],\n",
    "                'thoroughness': metrics['thoroughness']\n",
    "            }\n",
    "            results.append(result_data)\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    @staticmethod\n",
    "    def _pronoun_prompt():\n",
    "        return PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=\"\"\"\n",
    "            Your task is to replace all the pronouns in the following text with the nouns they refer to:\n",
    "\n",
    "            <text>\n",
    "            {text}\n",
    "            </text>\n",
    "\n",
    "            The goal is to make the text more explicit and clear by replacing potentially ambiguous pronouns like \"he\", \"she\", \"it\", \"they\", \"them\", etc. with the specific nouns or names they refer to.\n",
    "\n",
    "            For example:\n",
    "            Original: John went to the store. He bought some milk.\n",
    "            Pronoun replaced: John went to the store. John bought some milk.\n",
    "\n",
    "            Here are the steps to complete this task:\n",
    "\n",
    "            1. Carefully read the provided text and identify all the pronouns \n",
    "            2. For each pronoun, look back in the text to determine which noun or name it is referring to\n",
    "            3. If the pronoun is part of a direct quote, do not replace it\n",
    "            4. Replace each pronoun with the most recent noun or name it refers to\n",
    "            5. If a pronoun does not have a clear referent noun or name, do not replace it\n",
    "            6. Repeat this process until all the pronouns with clear referents have been replaced\n",
    "            \"\"\",\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_prompt():\n",
    "        return PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=\"\"\"\n",
    "            Please parse the following text into a list of individual facts:\n",
    "\n",
    "            <text>\n",
    "            {text}\n",
    "            </text>\n",
    "\n",
    "            Read the text carefully. Your task is to break it down into the key facts it contains. Parse out each individual fact into a separate sentence, even if that means splitting up or rewording the original sentences. The goal is to have a clear, concise list of the core facts contained in the text.\n",
    "\n",
    "            Output the parsed facts in a numbered list, with each fact written as a complete sentence on its own line. Use <facts> tags to demarcate the start and end of the list.\n",
    "            \"\"\",\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _compare_prompt():\n",
    "        return PromptTemplate(\n",
    "            input_variables=[\"context_list\", \"answer_list\"],\n",
    "            template=\"\"\"\n",
    "            You will be comparing facts between a context and an answer to determine which facts are shared and which are unique to each.\n",
    "\n",
    "            Here is the context:\n",
    "\n",
    "            <context>\n",
    "            {context_list}\n",
    "            </context>\n",
    "\n",
    "            And here is the answer: \n",
    "\n",
    "            <answer>\n",
    "            {answer_list}\n",
    "            </answer>\n",
    "\n",
    "            Carefully analyze the facts presented in the context and answer, focusing on the semantic meaning rather than the exact wording.\n",
    "\n",
    "            Then, output a dictionary with the following keys and corresponding lists of facts as values:\n",
    "\n",
    "            1. \"facts_in_both\": A list of facts that are present in both the context and the answer\n",
    "\n",
    "            2. \"facts_only_in_answer\": A list of facts that are only present in the answer \n",
    "\n",
    "            3. \"facts_only_in_context\": A list of facts that are only present in the context\n",
    "\n",
    "            Remember, the facts do not need to be worded identically to be considered the same. Focus on whether the core meaning is shared or unique.\n",
    "\n",
    "            Provide your results in this format:\n",
    "\n",
    "            {{\n",
    "                \"facts_in_both\": [\n",
    "                    \"Fact 1 present in both\",\n",
    "                    \"Fact 2 present in both\"\n",
    "                ],\n",
    "                \"facts_only_in_answer\": [\n",
    "                    \"Fact 1 only in answer\",\n",
    "                    \"Fact 2 only in answer\"  \n",
    "                ],\n",
    "                \"facts_only_in_context\": [\n",
    "                    \"Fact 1 only in context\",\n",
    "                    \"Fact 2 only in context\"\n",
    "                ]\n",
    "            }}\n",
    "            \"\"\",\n",
    "        )\n",
    "\n",
    "\n",
    "class ComparisonResult(BaseModel):\n",
    "    facts_in_both: list[str] = Field(default_factory=list, description=\"List of facts present in both context and answer\")\n",
    "    facts_only_in_answer: list[str] = Field(default_factory=list, description=\"List of facts only present in the answer\")\n",
    "    facts_only_in_context: list[str] = Field(default_factory=list, description=\"List of facts only present in the context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa72dbc-fd5a-462c-8308-9238c5cef0ad",
   "metadata": {},
   "source": [
    "## Run on First Pair of Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2751f4e-3992-496c-aad0-c0604a712257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: INSPECT_EVAL_MODEL=openai/gpt-4\n",
      "env: INSPECT_MODEL_NAME=openai/gpt-4\n",
      "<class '__main__.InspectChatModel'>\n",
      "Context with replaced pronouns:\n",
      "The fox, born in 2005, is a content and lively creature with a coat as brown as well-aged oak. His joy manifests itself in leaps of exuberance. These jumps are not dictated by necessity or an instinct for survival but are joyful bounds, spawned from his ebullient spirit. His favourite platform for these athletic displays is an old worn-out rock, almost as if the leaping is his unique way of expressing his happiness.\n",
      "\n",
      "In 2010, another littleness of joy came into the world - a bright-eyed hedgehog. Treading along the garden, she radiates an undeniable joy that surpasses even the fox's. Despite her spiky exterior, the light in her eyes and the small curl of her mouth when she munches on her favorite treats speak a clear language: here lives one happy hedgehog. Some say it is the innocent bliss of youth, however, no one can deny that her happiness outshines even that of the nimble fox.\n",
      "Context with replaced pronouns:\n",
      "The fox, born in 2005, is a content and lively creature with a coat as brown as well-aged oak. His joy manifests itself in leaps of exuberance. These jumps are not dictated by necessity or an instinct for survival but are joyful bounds, spawned from his ebullient spirit. His favourite platform for these athletic displays is an old worn-out rock, almost as if the leaping is his unique way of expressing his happiness.\n",
      "\n",
      "In 2010, another littleness of joy came into the world - a bright-eyed hedgehog. Treading along the garden, she radiates an undeniable joy that surpasses even the fox's. Despite her spiky exterior, the light in her eyes and the small curl of her mouth when she munches on her favorite treats speak a clear language: here lives one happy hedgehog. Some say it is the innocent bliss of youth, however, no one can deny that her happiness outshines even that of the nimble fox.\n",
      "\n",
      "Answer with replaced pronouns:\n",
      "The quick brown fox was born in 2005 making him 5 years older than the hedgehog who was born in 2010. Despite the age difference, happiness seems to not follow the same path. The hedgehog, being younger, appears to be more content and enjoys the simple pleasures of life more. Conversely, the quick brown fox, despite his speed and agility, lacks the same level of happiness. The reasons behind this disparity remain unknown. Perhaps the quick brown fox has had to face more trials and tribulations in his life, or maybe the hedgehog simply has a more optimistic outlook. Either way, the contrast in their dispositions provides an intriguing study into the emotional lives of these two creatures.\n",
      "\n",
      "Context list:\n",
      "<facts>\n",
      "1. The fox was born in 2005.\n",
      "2. The fox is a content and lively creature.\n",
      "3. The fox has a brown coat that is compared to well-aged oak.\n",
      "4. The fox exhibits joy through leaps of exuberance.\n",
      "5. The fox's jumps are not out of necessity or survival instinct, but come from happiness.\n",
      "6. The fox often jumps from an old worn-out rock.\n",
      "7. The fox uses his jumps as a way to express his happiness.\n",
      "8. A hedgehog was born in 2010.\n",
      "9. The hedgehog moves around the garden.\n",
      "10. The hedgehog expresses joy that is greater than the fox's, according to the text.\n",
      "11. The hedgehog has a spiky exterior.\n",
      "12. The hedgehog displays signs of happiness when she eats her favorite treats.\n",
      "13. Some attribute the hedgehog's evident happiness to the bliss of youth.\n",
      "14. The text suggests that the hedgehog's happiness surpasses that of the fox.\n",
      "</facts>\n",
      "\n",
      "Answer list:\n",
      "<facts>\n",
      "1. The quick brown fox was born in 2005.\n",
      "2. The hedgehog was born in 2010.\n",
      "3. The quick brown fox is five years older than the hedgehog.\n",
      "4. The hedgehog appears to be more content than the quick brown fox.\n",
      "5. The hedgehog enjoys the simple pleasures of life more than the quick brown fox does.\n",
      "6. Despite his speed and agility, the quick brown fox lacks the same level of happiness as the hedgehog.\n",
      "7. There is an unknown reason behind this disparity in happiness levels.\n",
      "8. Possible reasons for this disparity could be that the quick brown fox has faced more trials and tribulations in his life.\n",
      "9. Another theory is that the hedgehog simply has a more optimistic outlook in life.\n",
      "10. The contrast in their dispositions provides an interesting study into the emotional lives of these two creatures.\n",
      "</facts>\n",
      "\n",
      "Comparison result:\n",
      "facts_in_both=['The fox was born in 2005.', 'The hedgehog was born in 2010.', \"The hedgehog's happiness surpasses that of the fox.\"] facts_only_in_answer=['The quick brown fox is five years older than the hedgehog.', 'The hedgehog seems to be more content than the quick brown fox.', \"The hedgehog appears to find greater joy in life's simple pleasures than the quick brown fox.\", \"Despite his agility, the quick brown fox doesn't seem to be as happy as the hedgehog.\", 'The reason for the disparity in happiness levels between the two animals is unknown.', 'One potential reason behind the discrepancy could be that the quick brown fox has experienced more hardships in his life.', 'Another theory posits that the hedgehog simply maintains a more optimistic outlook on life.', 'The differing characters of the two animals provide an intriguing insight into their emotional states.'] facts_only_in_context=['The fox is a spirited and joyful creature.', \"The fox's coat is compared to the color of a well-aged oak.\", \"The fox's jumps are an expression of joy, not out of survival or necessity.\", \"The fox's favorite jumping spot is an old, worn-out rock.\", 'The hedgehog moves around the garden.', \"The hedgehog's joy becomes apparent when eating favorite snacks.\", \"Some attribute the hedgehog's joy to the bliss of youth.\", 'The hedgehog has a prickly appearance.']\n",
      "\n",
      "Metrics:\n",
      "Groundedness: 27.27%\n",
      "Thoroughness: 27.27%\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "%env INSPECT_EVAL_MODEL=openai/gpt-4\n",
    "%env INSPECT_MODEL_NAME=openai/gpt-4\n",
    "\n",
    "# Create an instance of InspectChatModel with the specified model\n",
    "inspect_model = InspectChatModel()\n",
    "\n",
    "# Create an instance of FactComparator with the InspectChatModel\n",
    "comparator = FactComparator(inspect_model)\n",
    "\n",
    "context = \"The quick brown fox jumps over the rock because he's happy. He was born in 2005. The hedgehog was born in 2010, but she's even happier than him.\"\n",
    "answer = \"The quick brown fox was born in 2005, and the hedgehog in 2010. The quick brown fox is not as happy as the hedgehog\"\n",
    "\n",
    "# Run the asynchronous process_data method\n",
    "result = asyncio.run(comparator.process_data(context, answer))\n",
    "\n",
    "metrics = comparator.calculate_metrics(result[\"comparison_result\"])\n",
    "\n",
    "print(\"Context with replaced pronouns:\")\n",
    "print(result[\"context_replace_pronouns\"])\n",
    "\n",
    "print(\"Context with replaced pronouns:\")\n",
    "print(result[\"context_replace_pronouns\"])\n",
    "\n",
    "print(\"\\nAnswer with replaced pronouns:\")\n",
    "print(result[\"answer_replace_pronouns\"])\n",
    "\n",
    "print(\"\\nContext list:\")\n",
    "print(result[\"context_list\"])\n",
    "\n",
    "print(\"\\nAnswer list:\")\n",
    "print(result[\"answer_list\"])\n",
    "\n",
    "print(\"\\nComparison result:\")\n",
    "print(result[\"comparison_result\"])\n",
    "\n",
    "print(\"\\nMetrics:\")\n",
    "print(f\"Groundedness: {metrics['groundedness']:.2f}%\")\n",
    "print(f\"Thoroughness: {metrics['thoroughness']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11437091-c8a1-40fe-b55b-d4f06c0941c3",
   "metadata": {},
   "source": [
    "## Run on another pair of statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad11011d-dd24-4c8f-b4e9-e8573b0c7ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: INSPECT_EVAL_MODEL=openai/gpt-4\n",
      "env: INSPECT_MODEL_NAME=openai/gpt-4\n",
      "<class '__main__.InspectChatModel'>\n",
      "Context with replaced pronouns:\n",
      "Once the water is boiling, add the pasta. Stir it occasionally to prevent it from sticking to the bottom of the pot. \n",
      "\n",
      "Cooking times will vary depending on the type of pasta you are using. For example, spaghetti usually takes around 8-10 minutes to cook, while smaller pasta types like macaroni may only take 5-7 minutes. \n",
      "\n",
      "Check the pasta's packaging for specific cooking instructions and times. Generally, you'll want to cook the pasta until it is al dente, or firm to the bite.\n",
      "\n",
      "Once the pasta is cooked, drain it immediately in a colander. Avoid rinsing the pasta as this will remove the starch that helps sauce adhere to it.\n",
      "\n",
      "Now your pasta is ready to be combined with your favorite sauce or used in any recipe! Enjoy.\n",
      "\n",
      "Answer with replaced pronouns:\n",
      "When your pasta is ready, carefully drain the water from the pot using a colander. Be careful not to burn yourself with the hot water or steam. After draining, it may be helpful to briefly rinse your pasta under hot tap water to remove any excess starch. However, if you're using the pasta for a sauce-based dish, it may be better to skip the rinse to help the sauce adhere better. Finally, toss the pasta with a bit of olive oil to prevent sticking and it's ready to be served.\n",
      "\n",
      "Context list:\n",
      "<facts>\n",
      "1. Once the water is boiling, pasta should be added.\n",
      "2. The pasta needs to be stirred occasionally to prevent it from sticking to the bottom of the pot.\n",
      "3. Cooking times for pasta will vary depending on the type of pasta being cooked.\n",
      "4. As an example, spaghetti usually takes around 8-10 minutes to cook.\n",
      "5. Conversely, smaller types of pasta like macaroni may only require 5-7 minutes of cooking time.\n",
      "6. Detailed cooking instructions and times can be found on the pasta's packaging.\n",
      "7. Pasta should generally be cooked until it reaches an 'al dente' state, which means it should be firm to the bite.\n",
      "8. Once the pasta is cooked, it should be drained immediately in a colander.\n",
      "9. Rinsing the pasta should be avoided because it removes the starch that helps sauce adhere to it.\n",
      "10. Once these steps are complete, the pasta is ready to be combined with sauce or used in a recipe.\n",
      "</facts>\n",
      "\n",
      "Answer list:\n",
      "<facts>\n",
      "1. When pasta is ready, it should be drained from the pot using a colander.\n",
      "2. Caution should be taken when draining the pasta to avoid burns from the hot water or steam.\n",
      "3. After draining, rinsing the pasta under hot tap water can help remove excess starch.\n",
      "4. For sauce-based dishes, it may be better to skip rinsing the pasta so that the sauce adheres better.\n",
      "5. The pasta is tossed with a bit of olive oil to prevent sticking after draining and rinsing.\n",
      "6. The pasta is ready to be served when it has been tossed with olive oil.\n",
      "</facts>\n",
      "\n",
      "Comparison result:\n",
      "facts_in_both=['Once the pasta is cooked, it should be drained immediately in a colander.', 'Rinsing the pasta should be avoided because it removes the starch that helps sauce adhere to it.', 'Once these steps are complete, the pasta is ready to be combined with sauce or used in a recipe.'] facts_only_in_answer=['Caution should be taken when draining the pasta to avoid burns from the hot water or steam.', 'After draining, rinsing the pasta under hot tap water can help remove excess starch.', 'The pasta is tossed with a bit of olive oil to prevent sticking after draining and rinsing.', 'The pasta is ready to be served when it has been tossed with olive oil.'] facts_only_in_context=['Once the water is boiling, pasta should be added.', 'The pasta needs to be stirred occasionally to prevent it from sticking to the bottom of the pot.', 'Cooking times for pasta will vary depending on the type of pasta being cooked.', 'As an example, spaghetti usually takes around 8-10 minutes to cook.', 'Conversely, smaller types of pasta like macaroni may only require 5-7 minutes of cooking time.', \"Detailed cooking instructions and times can be found on the pasta's packaging.\", \"Pasta should generally be cooked until it reaches an 'al dente' state, which means it should be firm to the bite.\"]\n",
      "\n",
      "Metrics:\n",
      "Groundedness: 42.86%\n",
      "Thoroughness: 30.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abiga\\AppData\\Local\\Temp\\ipykernel_45788\\3272148234.py:14: RuntimeWarning: coroutine 'FactComparator.process_data' was never awaited\n",
      "  result = asyncio.run(comparator.process_data(context, answer))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "%env INSPECT_EVAL_MODEL=openai/gpt-4\n",
    "%env INSPECT_MODEL_NAME=openai/gpt-4\n",
    "\n",
    "# Create an instance of InspectChatModel with the specified model\n",
    "inspect_model = InspectChatModel()\n",
    "\n",
    "# Create an instance of FactComparator with the InspectChatModel\n",
    "comparator = FactComparator(inspect_model)\n",
    "\n",
    "context = \"To boil pasta, first bring a large pot of salted water to a rolling boil over high heat..\"\n",
    "answer = \"To boil pasta, begin by filling a large pot with water, making sure there's enough to fully submerge the pasta. Bring the water to a rolling boil over high heat, then add salt to enhance the pasta's flavor. Once the water is boiling, carefully add the pasta, stirring gently to prevent sticking. Cook the pasta according to the package instructions or until it reaches your desired level of tenderness, usually around 8-12 minutes. To check for doneness, taste a piece of pasta—it should be tender but still slightly firm (al dente).\"\n",
    "\n",
    "# Run the asynchronous process_data method\n",
    "result = asyncio.run(comparator.process_data(context, answer))\n",
    "\n",
    "metrics = comparator.calculate_metrics(result[\"comparison_result\"])\n",
    "print(\"Context with replaced pronouns:\")\n",
    "print(result[\"context_replace_pronouns\"])\n",
    "\n",
    "print(\"\\nAnswer with replaced pronouns:\")\n",
    "print(result[\"answer_replace_pronouns\"])\n",
    "\n",
    "print(\"\\nContext list:\")\n",
    "print(result[\"context_list\"])\n",
    "\n",
    "print(\"\\nAnswer list:\")\n",
    "print(result[\"answer_list\"])\n",
    "\n",
    "print(\"\\nComparison result:\")\n",
    "print(result[\"comparison_result\"])\n",
    "\n",
    "print(\"\\nMetrics:\")\n",
    "print(f\"Groundedness: {metrics['groundedness']:.2f}%\")\n",
    "print(f\"Thoroughness: {metrics['thoroughness']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaf37e7-66f4-422b-926d-cc1b93109a89",
   "metadata": {},
   "source": [
    "## Run on a list of dictionaries - return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9514e054-f73a-49e8-b893-61981fe6d802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.InspectChatModel'>\n",
      "<class '__main__.InspectChatModel'>\n",
      "<class '__main__.InspectChatModel'>\n",
      "<class '__main__.InspectChatModel'>\n",
      "<class '__main__.InspectChatModel'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>context_replace_pronouns</th>\n",
       "      <th>answer_replace_pronouns</th>\n",
       "      <th>context_list</th>\n",
       "      <th>answer_list</th>\n",
       "      <th>facts_in_both</th>\n",
       "      <th>facts_only_in_answer</th>\n",
       "      <th>facts_only_in_context</th>\n",
       "      <th>groundedness</th>\n",
       "      <th>thoroughness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The quick brown fox jumps over the rock becaus...</td>\n",
       "      <td>The quick brown fox was born in 2005, and the ...</td>\n",
       "      <td>The fox and the hedgehog, despite their differ...</td>\n",
       "      <td>because he is older and has had to face more c...</td>\n",
       "      <td>&lt;facts&gt;\\n1. The story features a fox and a hed...</td>\n",
       "      <td>&lt;facts&gt;\\n1. The brown fox is older. \\n2. The f...</td>\n",
       "      <td>The fox is 16 years old., The hedgehog is 11 y...</td>\n",
       "      <td>The brown fox is older., The fox has faced mor...</td>\n",
       "      <td>The story features a fox and a hedgehog., Desp...</td>\n",
       "      <td>18.181818</td>\n",
       "      <td>13.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The sun is a star at the center of our solar s...</td>\n",
       "      <td>The sun is a star located approximately 93 mil...</td>\n",
       "      <td>The sun's diameter is about 109 times that of ...</td>\n",
       "      <td>The core temperature of the sun is estimated t...</td>\n",
       "      <td>&lt;facts&gt;\\n1. The sun's diameter is approximatel...</td>\n",
       "      <td>&lt;facts&gt;\\n1. The sun's core temperature is esti...</td>\n",
       "      <td>The sun's core is the site of nuclear fusion a...</td>\n",
       "      <td>The sun's core temperature is estimated to be ...</td>\n",
       "      <td>The sun's diameter is approximately 109 times ...</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>35.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Birds are warm-blooded vertebrates that lay eg...</td>\n",
       "      <td>Birds are a diverse group of animals with feat...</td>\n",
       "      <td>Birds play a vital role in the ecosystem, as t...</td>\n",
       "      <td>Birds have unique capabilities; their ability ...</td>\n",
       "      <td>&lt;facts&gt;\\n1. Birds play a vital role in the eco...</td>\n",
       "      <td>&lt;facts&gt;\\n1. Birds have the unique capability t...</td>\n",
       "      <td>Birds play a vital role in the ecosystem by co...</td>\n",
       "      <td>Birds have the unique capability to fly., Ostr...</td>\n",
       "      <td>Birds serve as food for other animals., Birds ...</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>37.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Eiffel Tower is a wrought-iron lattice tow...</td>\n",
       "      <td>The Eiffel Tower, found in Paris, France, is a...</td>\n",
       "      <td>The Eiffel Tower is renowned worldwide as a sy...</td>\n",
       "      <td>The Eiffel Tower was initially criticized by s...</td>\n",
       "      <td>&lt;facts&gt;\\n1. The Eiffel Tower is known worldwid...</td>\n",
       "      <td>&lt;facts&gt;\\n1. The Eiffel Tower was initially cri...</td>\n",
       "      <td>The Eiffel Tower is known worldwide as a symbo...</td>\n",
       "      <td>The Eiffel Tower was initially criticized by s...</td>\n",
       "      <td>The Tower has three levels accessible by eleva...</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>43.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Great Wall of China is a series of fortifi...</td>\n",
       "      <td>The Great Wall of China, a series of walls and...</td>\n",
       "      <td>The construction of the Great Wall began as ea...</td>\n",
       "      <td>The Great Wall was initially built to protect ...</td>\n",
       "      <td>&lt;facts&gt;\\n1. The construction of the Great Wall...</td>\n",
       "      <td>&lt;facts&gt;\\n1. The Great Wall was initially built...</td>\n",
       "      <td>The construction of the Great Wall of China be...</td>\n",
       "      <td>Numerous myths surround the Great Wall, includ...</td>\n",
       "      <td>These sections were mainly made from bricks an...</td>\n",
       "      <td>73.333333</td>\n",
       "      <td>73.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  The quick brown fox jumps over the rock becaus...   \n",
       "1  The sun is a star at the center of our solar s...   \n",
       "2  Birds are warm-blooded vertebrates that lay eg...   \n",
       "3  The Eiffel Tower is a wrought-iron lattice tow...   \n",
       "4  The Great Wall of China is a series of fortifi...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  The quick brown fox was born in 2005, and the ...   \n",
       "1  The sun is a star located approximately 93 mil...   \n",
       "2  Birds are a diverse group of animals with feat...   \n",
       "3  The Eiffel Tower, found in Paris, France, is a...   \n",
       "4  The Great Wall of China, a series of walls and...   \n",
       "\n",
       "                            context_replace_pronouns  \\\n",
       "0  The fox and the hedgehog, despite their differ...   \n",
       "1  The sun's diameter is about 109 times that of ...   \n",
       "2  Birds play a vital role in the ecosystem, as t...   \n",
       "3  The Eiffel Tower is renowned worldwide as a sy...   \n",
       "4  The construction of the Great Wall began as ea...   \n",
       "\n",
       "                             answer_replace_pronouns  \\\n",
       "0  because he is older and has had to face more c...   \n",
       "1  The core temperature of the sun is estimated t...   \n",
       "2  Birds have unique capabilities; their ability ...   \n",
       "3  The Eiffel Tower was initially criticized by s...   \n",
       "4  The Great Wall was initially built to protect ...   \n",
       "\n",
       "                                        context_list  \\\n",
       "0  <facts>\\n1. The story features a fox and a hed...   \n",
       "1  <facts>\\n1. The sun's diameter is approximatel...   \n",
       "2  <facts>\\n1. Birds play a vital role in the eco...   \n",
       "3  <facts>\\n1. The Eiffel Tower is known worldwid...   \n",
       "4  <facts>\\n1. The construction of the Great Wall...   \n",
       "\n",
       "                                         answer_list  \\\n",
       "0  <facts>\\n1. The brown fox is older. \\n2. The f...   \n",
       "1  <facts>\\n1. The sun's core temperature is esti...   \n",
       "2  <facts>\\n1. Birds have the unique capability t...   \n",
       "3  <facts>\\n1. The Eiffel Tower was initially cri...   \n",
       "4  <facts>\\n1. The Great Wall was initially built...   \n",
       "\n",
       "                                       facts_in_both  \\\n",
       "0  The fox is 16 years old., The hedgehog is 11 y...   \n",
       "1  The sun's core is the site of nuclear fusion a...   \n",
       "2  Birds play a vital role in the ecosystem by co...   \n",
       "3  The Eiffel Tower is known worldwide as a symbo...   \n",
       "4  The construction of the Great Wall of China be...   \n",
       "\n",
       "                                facts_only_in_answer  \\\n",
       "0  The brown fox is older., The fox has faced mor...   \n",
       "1  The sun's core temperature is estimated to be ...   \n",
       "2  Birds have the unique capability to fly., Ostr...   \n",
       "3  The Eiffel Tower was initially criticized by s...   \n",
       "4  Numerous myths surround the Great Wall, includ...   \n",
       "\n",
       "                               facts_only_in_context  groundedness  \\\n",
       "0  The story features a fox and a hedgehog., Desp...     18.181818   \n",
       "1  The sun's diameter is approximately 109 times ...     35.714286   \n",
       "2  Birds serve as food for other animals., Birds ...     40.000000   \n",
       "3  The Tower has three levels accessible by eleva...     35.000000   \n",
       "4  These sections were mainly made from bricks an...     73.333333   \n",
       "\n",
       "   thoroughness  \n",
       "0     13.333333  \n",
       "1     35.714286  \n",
       "2     37.500000  \n",
       "3     43.750000  \n",
       "4     73.333333  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list = [\n",
    "    {\n",
    "        'context': 'The quick brown fox jumps over the rock because he\\'s happy. He was born in 2005. The hedgehog was born in 2010, but she\\'s even happier than him.',\n",
    "        'answer': 'The quick brown fox was born in 2005, and the hedgehog in 2010. The quick brown fox is not as happy as the hedgehog'\n",
    "    },\n",
    "    {\n",
    "        'context': 'The sun is a star at the center of our solar system. It is about 93 million miles away from Earth. The sun is a hot ball of glowing gases that provides light and warmth to Earth.',\n",
    "        'answer': 'The sun is a star located approximately 93 million miles from Earth. It is the source of light and heat for our planet. The sun is not a solid object, but rather a sphere of hot glowing gases.'\n",
    "    },\n",
    "    {\n",
    "        'context': 'Birds are warm-blooded vertebrates that lay eggs and have feathers, wings, and beaks. There are over 10,000 species of birds worldwide. Some common bird species include sparrows, pigeons, and parrots.',\n",
    "        'answer': 'Birds are a diverse group of animals with feathers and wings. They are warm-blooded egg-laying vertebrates. The number of bird species globally exceeds 10,000. Pigeons, parrots, and sparrows are among the most familiar bird types.'\n",
    "    },\n",
    "    {\n",
    "        'context': 'The Eiffel Tower is a wrought-iron lattice tower located on the Champ de Mars in Paris, France. It was constructed from 1887 to 1889 and stands at a height of 324 meters. The tower is named after Gustave Eiffel, whose company designed and built it.',\n",
    "        'answer': 'The Eiffel Tower, found in Paris, France, is a lattice tower made of wrought iron. Built between 1887 and 1889, it reaches a height of 324 meters. Gustave Eiffel\\'s company was responsible for the tower\\'s design and construction, hence its name.'\n",
    "    },\n",
    "    {\n",
    "        'context': 'The Great Wall of China is a series of fortifications and walls built across the historical northern borders of ancient Chinese states and Imperial China. The most well-known sections were built during the Ming dynasty, which ruled from 1368 to 1644.',\n",
    "        'answer': 'The Great Wall of China, a series of walls and fortifications, was constructed along the northern borders of ancient Chinese states and Imperial China. The Ming dynasty, which lasted from 1368 to 1644, is responsible for the construction of the most famous sections of the wall.'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create an instance of InspectChatModel with the specified model\n",
    "inspect_model = InspectChatModel()\n",
    "\n",
    "# Create an instance of FactComparator with the InspectChatModel\n",
    "comparator = FactComparator(inspect_model)\n",
    "\n",
    "df = comparator.process_data_list(data_list)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Virtual Env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
